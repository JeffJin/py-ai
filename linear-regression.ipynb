{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T02:45:29.432669Z",
     "start_time": "2025-10-02T02:45:29.230801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: w=[ 0.5 -0.9], loss=99.66666666666666, gradiant=[ 50. 190.]\n",
      "Loss at iteration 1: w=[0.99466667 0.90133333], loss=90.16000000000003, gradiant=[ -49.46666667 -180.13333333]\n",
      "Loss at iteration 2: w=[ 0.54343111 -0.81649778], loss=81.56503703703709, gradiant=[ 45.12355556 171.78311111]\n",
      "Loss at iteration 3: w=[0.99137967 0.81172504], loss=73.79428068103381, gradiant=[ -44.7948563  -162.82228148]\n",
      "Loss at iteration 4: w=[ 0.58421367 -0.74141931], loss=66.76863217879882, gradiant=[ 40.71660057 155.31443421]\n",
      "Loss at iteration 5: w=[0.98991448 0.73031318], loss=60.41658499066581, gradiant=[ -40.57008149 -147.17324833]\n",
      "Loss at iteration 6: w=[ 0.62257404 -0.67394852], loss=54.673496038688434, gradiant=[ 36.73404411 140.42616996]\n",
      "Loss at iteration 7: w=[0.99006862 0.65631842], loss=49.480926905726484, gradiant=[ -36.74945811 -133.02669416]\n",
      "Loss at iteration 8: w=[ 0.65871738 -0.61334783], loss=44.786048271021116, gradiant=[ 33.13512385 126.9666255 ]\n",
      "Loss at iteration 9: w=[0.99165956 0.5890361 ], loss=40.541101512356, gradiant=[ -33.2942179  -120.23839318]\n",
      "Loss at iteration 10: w=[ 0.69282966 -0.55895059], loss=36.70291198753449, gradiant=[ 29.88299062 114.79866845]\n",
      "Loss at iteration 11: w=[0.99452279 0.52782905], loss=33.23244903461196, gradiant=[ -30.16931338 -108.67796357]\n",
      "Loss at iteration 12: w=[ 0.72507946 -0.51015424], loss=30.09442820647361, gradiant=[ 26.94433276 103.7983293 ]\n",
      "Loss at iteration 13: w=[0.99851007 0.47212114], loss=27.256951685794878, gradiant=[-27.34306075 -98.22753864]\n",
      "Loss at iteration 14: w=[ 0.75561971 -0.46641424], loss=24.69118321555063, gradiant=[24.28903656 93.85353792]\n",
      "Loss at iteration 15: w=[1.00348788 0.42139142], loss=22.37105423201737, gradiant=[-24.78681701 -88.78056555]\n",
      "Loss at iteration 16: w=[ 0.78458908 -0.4272384 ], loss=20.27299820522423, gradiant=[21.88987927 84.86298136]\n",
      "Loss at iteration 17: w=[1.00933596 0.3751688 ], loss=18.375710479294547, gradiant=[-22.47468809 -80.2407193 ]\n",
      "Loss at iteration 18: w=[ 0.81211345 -0.39218192], loss=16.659931165011905, gradiant=[19.72225163 76.73507136]\n",
      "Loss at iteration 19: w=[1.0159461  0.33302729], loss=15.108248871888918, gradiant=[-20.38326499 -72.52092098]\n",
      "Loss at iteration 20: w=[ 0.83830703 -0.36084281], loss=13.704923279410801, gradiant=[17.76390698 69.38701083]\n",
      "Loss at iteration 21: w=[1.02322088 0.29458169], loss=12.43572473913192, gradiant=[-18.49138527 -65.54245028]\n",
      "Loss at iteration 22: w=[ 0.86327354 -0.33285781], loss=11.287789272879403, gradiant=[15.99473447 62.74395006]\n",
      "Loss at iteration 23: w=[1.03107271 0.25948362], loss=10.24948748923307, gradiant=[-16.77991738 -59.23414316]\n",
      "Loss at iteration 24: w=[ 0.88710717 -0.30789862], loss=9.310306082303931, gradiant=[14.39655406 56.73822385]\n",
      "Loss at iteration 25: w=[1.03942283 0.22741805], loss=8.460740705070478, gradiant=[-15.23156578 -53.53166653]\n",
      "Loss at iteration 26: w=[ 0.90989352 -0.28566857], loss=7.692199125459916, gradiant=[12.95293114 51.3086619 ]\n",
      "Loss at iteration 27: w=[1.04820046 0.19810005], loss=6.99691367816162, gradiant=[-13.83069468 -48.37686258]\n",
      "Loss at iteration 28: w=[ 0.93171037 -0.2658996 ], loss=6.367862119901, gradiant=[11.64900903 46.39996539]\n",
      "Loss at iteration 29: w=[1.05734206 0.17127196], loss=5.798696081548638, gradiant=[-12.56316873 -43.71715602]\n",
      "Loss at iteration 30: w=[ 0.95262848 -0.24834948], loss=5.283676387865439, gradiant=[10.47135749 41.96214338]\n",
      "Loss at iteration 31: w=[1.06679057 0.14670071], loss=4.81761458567847, gradiant=[-11.416209   -39.50501822]\n",
      "Loss at iteration 32: w=[ 0.97271222 -0.23279933], loss=4.39582008455747, gradiant=[ 9.40783579 37.95000332]\n",
      "Loss at iteration 33: w=[1.07649484 0.1241755 ], loss=4.014052371263865, gradiant=[-10.37826284 -35.69748281]\n",
      "Loss at iteration 34: w=[ 0.99202016 -0.2190514 ], loss=3.668477810955059, gradiant=[ 8.44746893 34.32269048]\n",
      "Loss at iteration 35: w=[1.08640902 0.10350567], loss=3.3556305948744556, gradiant=[ -9.43888609 -32.25570774]\n",
      "Loss at iteration 36: w=[ 1.01060566 -0.20692704], loss=3.072377436517767, gradiant=[ 7.58033573 31.04327158]\n",
      "Loss at iteration 37: w=[1.09649203 0.08451875], loss=2.8158856564702157, gradiant=[ -8.58863678 -29.14457941]\n",
      "Loss at iteration 38: w=[ 1.02851735 -0.19626483], loss=2.583594330645885, gradiant=[ 6.79746769 28.07835835]\n",
      "Loss at iteration 39: w=[1.10670714 0.06705872], loss=2.373188207882156, gradiant=[ -7.81897885 -26.33235479]\n",
      "Loss at iteration 40: w=[ 1.04579956 -0.18691896], loss=2.1825741310670432, gradiant=[ 6.09075752 25.3977673 ]\n",
      "Loss at iteration 41: w=[1.11702152 0.05098442], loss=2.009859721492716, gradiant=[ -7.12219529 -23.79033785]\n",
      "Loss at iteration 42: w=[ 1.06249275 -0.1787577 ], loss=1.8533341091949211, gradiant=[ 5.45287647 22.97421207]\n",
      "Loss at iteration 43: w=[1.12740585 0.03616817], loss=1.7114505128903468, gradiant=[ -6.49130944 -21.49258712]\n",
      "Loss at iteration 44: w=[ 1.07863385 -0.17166208], loss=1.5828104919746881, gradiant=[ 4.87719961 20.78302537]\n",
      "Loss at iteration 45: w=[1.13783399 0.02249443], loss=1.4661497100855378, gradiant=[ -5.92001402 -19.41565128]\n",
      "Loss at iteration 46: w=[ 1.09425661 -0.16552464], loss=1.3603250651397367, gradiant=[ 4.35773823 18.80190753]\n",
      "Loss at iteration 47: w=[1.14828268 0.00985866], loss=1.2643030546816878, gradiant=[ -5.40260678 -17.53833012]\n",
      "Loss at iteration 48: w=[ 1.10939189 -0.16024834], loss=1.1771492579691645, gradiant=[ 3.88907882 17.01069933]\n",
      "Loss at iteration 49: w=[ 1.15873121 -0.00183375], loss=1.0980188276046583, gradiant=[ -4.93393255 -15.84145848]\n",
      "Loss at iteration 50: w=[ 1.12406794 -0.15574552], loss=1.0261478938093895, gradiant=[ 3.4663278  15.39117646]\n",
      "Loss at iteration 51: w=[ 1.16916124 -0.01266841], loss=0.9608457937385617, gradiant=[ -4.50933064 -14.30771089]\n",
      "Loss at iteration 52: w=[ 1.13831063 -0.15193704], loss=0.9014880466450205, gradiant=[ 3.08506162 13.92686384]\n",
      "Loss at iteration 53: w=[ 1.1795565  -0.02272279], loss=0.8475100033000071, gradiant=[ -4.12458741 -12.92142501]\n",
      "Loss at iteration 54: w=[ 1.15214368 -0.14875147], loss=0.7984011049515394, gradiant=[ 2.74128166 12.60286771]\n",
      "Loss at iteration 55: w=[ 1.18990262 -0.03206705], loss=0.7536996933131832, gradiant=[ -3.77589339 -11.66844193]\n",
      "Loss at iteration 56: w=[ 1.16558888 -0.14612429], loss=0.7129883186919368, gradiant=[ 2.43137342 11.40572384]\n",
      "Loss at iteration 57: w=[ 1.20018693 -0.04076467], loss=0.6758894984408179, gradiant=[ -3.45980446 -10.53596194]\n",
      "Loss at iteration 58: w=[ 1.17866623 -0.14399727], loss=0.6420618825113241, gradiant=[ 2.15206965 10.3232603 ]\n",
      "Loss at iteration 59: w=[ 1.2103983  -0.04887313], loss=0.6111967870299544, gradiant=[-3.17320678 -9.51241401]\n",
      "Loss at iteration 60: w=[ 1.19139413 -0.14231787], loss=0.5830150605737603, gradiant=[1.90041699 9.34447345]\n",
      "Loss at iteration 61: w=[ 1.22052698 -0.05644449], loss=0.5572642512106359, gradiant=[-2.91328514 -8.58733783]\n",
      "Loss at iteration 62: w=[ 1.20378952 -0.14103865], loss=0.5337160454353338, gradiant=[1.67374586 8.45941579]\n",
      "Loss at iteration 63: w=[ 1.23056446 -0.06352588], loss=0.5121639529032456, gradiant=[-2.67749425 -7.75127726]\n",
      "Loss at iteration 64: w=[ 1.21586803 -0.14011682], loss=0.49242121336905653, gradiant=[1.46964322 7.65909456]\n",
      "Loss at iteration 65: w=[ 1.24050336 -0.07015998], loss=0.4743189045019729, gradiant=[-2.46353285 -6.9956839 ]\n",
      "Loss at iteration 66: w=[ 1.22764408 -0.13951378], loss=0.4577042312964821, gradiant=[1.28592791 6.93538012]\n",
      "Loss at iteration 67: w=[ 1.25033728 -0.07638548], loss=0.4424389796483195, gradiant=[-2.26932032 -6.31282995]\n",
      "Loss at iteration 68: w=[ 1.239131   -0.13919471], loss=0.42839811833839214, gradiant=[1.12062843 6.28092311]\n",
      "Loss at iteration 69: w=[ 1.26006075 -0.08223742], loss=0.4154685351799112, gradiant=[-2.09297546 -5.69572946]\n",
      "Loss at iteration 70: w=[ 1.25034113 -0.13912822], loss=0.40354789445127626, gradiant=[0.97196277 5.68907955]\n",
      "Loss at iteration 71: w=[ 1.2696691  -0.08774754], loss=0.392543603973332, gradiant=[-1.9327974  -5.13806711]\n",
      "Loss at iteration 72: w=[ 1.2612859  -0.13928598], loss=0.38237188130702143, gradiant=[0.83832023 5.1538431 ]\n",
      "Loss at iteration 73: w=[ 1.27915838 -0.09294464], loss=0.3729569095576304, gradiant=[-1.78724824 -4.63413385]\n",
      "Loss at iteration 74: w=[ 1.27197593 -0.13964248], loss=0.36423007418500586, gradiant=[0.71824499 4.6697839 ]\n",
      "Loss at iteration 75: w=[ 1.28852531 -0.09785479], loss=0.35612927304467706, gradiant=[-1.65493748 -4.17876869]\n",
      "Loss at iteration 76: w=[ 1.28242109 -0.14017472], loss=0.3485982926311078, gradiant=[0.6104212  4.23199323]\n",
      "Loss at iteration 77: w=[ 1.29776717 -0.10250166], loss=0.34158624416897077, gradiant=[-1.53460781 -3.7673061 ]\n",
      "Loss at iteration 78: w=[ 1.29263058 -0.140862  ], loss=0.3350470538082443, gradiant=[0.51365959 3.83603347]\n",
      "Loss at iteration 79: w=[ 1.3068818  -0.10690671], loss=0.32893900173029134, gradiant=[-1.42512237 -3.39552845]\n",
      "Loss at iteration 80: w=[ 1.30261295 -0.14168564], loss=0.3232243054705191, gradiant=[0.42688527 3.47789288]\n",
      "Loss at iteration 81: w=[ 1.31586748 -0.11108941], loss=0.3178687432138184, gradiant=[-1.3254532  -3.05962299]\n",
      "Loss at iteration 82: w=[ 1.31237621 -0.14262886], loss=0.31284131322631703, gradiant=[0.34912677 3.15394475]\n",
      "Loss at iteration 83: w=[ 1.32472292 -0.11506743], loss=0.30811392595523224, gradiant=[-1.23467076 -2.75614305]\n",
      "Loss at iteration 84: w=[ 1.32192786 -0.14367653], loss=0.30366112566150383, gradiant=[0.2795061  2.86091041]\n",
      "Loss at iteration 85: w=[ 1.3334472 -0.1188568], loss=0.2994598387508299, gradiant=[-1.15193448 -2.48197284]\n",
      "Loss at iteration 86: w=[ 1.3312749  -0.14481506], loss=0.29548914624078865, gradiant=[0.21722979 2.59582584]\n",
      "Loss at iteration 87: w=[ 1.34203975 -0.1224721 ], loss=0.29173007804766904, gradiant=[-1.07648429 -2.23429569]\n",
      "Loss at iteration 88: w=[ 1.34042394 -0.14603222], loss=0.288165426998971, gradiant=[0.16158075 2.35601148]\n",
      "Loss at iteration 89: w=[ 1.35050027 -0.12592656], loss=0.2847795806785295, gradiant=[-1.00763281 -2.01056538]\n",
      "Loss at iteration 90: w=[ 1.34938116 -0.14731701], loss=0.28155836939292056, gradiant=[0.11191093 2.13904491]\n",
      "Loss at iteration 91: w=[ 1.35882874 -0.12923221], loss=0.27848892871206293, gradiant=[-0.94475846 -1.80848015]\n",
      "Loss at iteration 92: w=[ 1.3581524  -0.14865957], loss=0.2755595751854304, gradiant=[0.06763473 1.94273617]\n",
      "Loss at iteration 93: w=[ 1.36702539 -0.13239998], loss=0.27275969396952393, gradiant=[-0.88729907 -1.62595923]\n",
      "Loss at iteration 94: w=[ 1.36674316 -0.15005104], loss=0.2700796372236077, gradiant=[0.02822296 1.76510545]\n",
      "Loss at iteration 95: w=[ 1.37509062 -0.13543982], loss=0.26751063224043214, gradiant=[-0.83474624 -1.46112164]\n",
      "Loss at iteration 96: w=[ 1.37515865 -0.15148345], loss=0.2650446983778193, gradiant=[-0.00680257  1.60436292]\n",
      "Loss at iteration 97: w=[ 1.38302505 -0.13836078], loss=0.26267457194667115, gradiant=[-0.78664016 -1.31226701]\n",
      "Loss at iteration 98: w=[ 1.38340379 -0.15294968], loss=0.26039363829199014, gradiant=[-0.03787392  1.45889049]\n",
      "Loss at iteration 99: w=[ 1.39082944 -0.1411711 ], loss=0.2581958703767828, gradiant=[-0.74256495 -1.17785823]\n",
      "Loss at iteration 100: w=[ 1.39148325 -0.15444336], loss=0.256075773244956, gradiant=[-0.06538152  1.32722535]\n",
      "Loss at iteration 101: w=[ 1.3985047 -0.1438783], loss=0.25402833379918943, gradiant=[-0.70214444 -1.05650578]\n",
      "Loss at iteration 102: w=[ 1.39940148 -0.15595875], loss=0.2520489753839088, gradiant=[-0.08967807  1.20804504]\n",
      "Loss at iteration 103: w=[ 1.40605186 -0.14648921], loss=0.2501335167124127, gradiant=[-0.6650384 -0.9469536]\n",
      "Loss at iteration 104: w=[ 1.40716268 -0.15749075], loss=0.24827813472145116, gradiant=[-0.11108227  1.10015397]\n",
      "Loss at iteration 105: w=[ 1.41347207 -0.14901009], loss=0.2464793309765471, gradiant=[-0.63093902 -0.84806622]\n",
      "Loss at iteration 106: w=[ 1.41477089 -0.1590348 ], loss=0.24473390128750108, gradiant=[-0.12988199  1.00247129]\n",
      "Loss at iteration 107: w=[ 1.42076657 -0.15144663], loss=0.24303890822621002, gradiant=[-0.59956788 -0.75881722]\n",
      "Loss at iteration 108: w=[ 1.42222994 -0.16058683], loss=0.24139165626847414, gradiant=[-0.14633733  0.91401982]\n",
      "Loss at iteration 109: w=[ 1.42793668 -0.15380404], loss=0.2397896693081775, gradiant=[-0.57067308 -0.67827875]\n",
      "Loss at iteration 110: w=[ 1.42954351 -0.1621432 ], loss=0.23823067031637696, gradiant=[-0.16068319  0.83391614]\n",
      "Loss at iteration 111: w=[ 1.43498377 -0.15608708], loss=0.2367125629396587, gradiant=[-0.54402673 -0.60561209]\n",
      "Loss at iteration 112: w=[ 1.43671509 -0.1637007 ], loss=0.23523341485186303, gradiant=[-0.17313177  0.76136159]\n",
      "Loss at iteration 113: w=[ 1.44190932 -0.15830011], loss=0.23379144269110902, gradiant=[-0.51942263 -0.54005904]\n",
      "Loss at iteration 114: w=[ 1.44374807 -0.16525645], loss=0.2323849984301886, gradiant=[-0.18387473  0.69563409]\n",
      "Loss at iteration 115: w=[ 1.44871481 -0.16044711], loss=0.2310125570429687, gradiant=[-0.49667419 -0.48093425]\n",
      "Loss at iteration 116: w=[ 1.45064566 -0.16680791], loss=0.2296727053426284, gradiant=[-0.19308514  0.63608086]\n",
      "Loss at iteration 117: w=[ 1.45540178 -0.16253173], loss=0.22836413187947024, gradiant=[-0.47561257 -0.42761821]\n",
      "Loss at iteration 118: w=[ 1.45741098 -0.16835285], loss=0.2270856177968143, gradiant=[-0.20091928  0.58211171]\n",
      "Loss at iteration 119: w=[ 1.46197183 -0.16455734], loss=0.22583602855322685, gradiant=[-0.45608495 -0.37955089]\n",
      "Loss at iteration 120: w=[ 1.46404701 -0.16988927], loss=0.22461430642813177, gradiant=[-0.20751829  0.53319307]\n",
      "Loss at iteration 121: w=[ 1.46842654 -0.16652701], loss=0.22341946373581859, gradiant=[-0.43795303 -0.33622608]\n",
      "Loss at iteration 122: w=[ 1.47055664 -0.17141544], loss=0.22225057668004522, gradiant=[-0.21300958  0.48884253]\n",
      "Loss at iteration 123: w=[ 1.47476755 -0.16844357], loss=0.22110677978795074, gradiant=[-0.4210916  -0.29718621]\n",
      "Loss at iteration 124: w=[ 1.47694263 -0.17292981], loss=0.21998726086786027, gradiant=[-0.21750815  0.44862394]\n",
      "Loss at iteration 125: w=[ 1.48099651 -0.17030964], loss=0.21889125644088725, gradiant=[-0.40538733 -0.26201766]\n",
      "Loss at iteration 126: w=[ 1.48320769 -0.17443107], loss=0.21781804760104043, gradiant=[-0.22111784  0.41214298]\n",
      "Loss at iteration 127: w=[ 1.48711506 -0.1721276 ], loss=0.21676695626288928, gradiant=[-0.39073759 -0.23034656]\n",
      "Loss at iteration 128: w=[ 1.48935438 -0.17591803], loss=0.21573734175976594, gradiant=[-0.22393233  0.37904314]\n",
      "Loss at iteration 129: w=[ 1.49312488 -0.17389968], loss=0.214728597759032, gradiant=[-0.37704947 -0.20183498]\n",
      "Loss at iteration 130: w=[ 1.49538524 -0.1773897 ], loss=0.21374014946415243, gradiant=[-0.22603619  0.34900209]\n",
      "Loss at iteration 131: w=[ 1.49902763 -0.17562793], loss=0.21277145107621454, gradiant=[-0.36423881 -0.17617743]\n",
      "Loss at iteration 132: w=[ 1.50130269 -0.17884521], loss=0.21182198349015569, gradiant=[-0.22750569  0.32172838]\n",
      "Loss at iteration 133: w=[ 1.50482498 -0.17731423], loss=0.2108912522033372, gradiant=[-0.35222937 -0.15309784]\n",
      "Loss at iteration 134: w=[ 1.50710908 -0.18028382], loss=0.2099787854162391, gradiant=[-0.22840965  0.29695852]\n",
      "Loss at iteration 135: w=[ 1.5105186  -0.17896035], loss=0.20908413230699613, gradiant=[-0.34095211 -0.13234663]\n",
      "Loss at iteration 136: w=[ 1.5128067 -0.1817049], loss=0.20820686146324136, gradiant=[-0.22881012  0.27445426]\n",
      "Loss at iteration 137: w=[ 1.51611014 -0.18056791], loss=0.20734655945631125, gradiant=[-0.33034445 -0.11369823]\n",
      "Loss at iteration 138: w=[ 1.51839777 -0.18310791], loss=0.2065028295442968, gradiant=[-0.22876308  0.25400018]\n",
      "Loss at iteration 139: w=[ 1.52160127 -0.18213843], loss=0.20567529049171968, gradiant=[-0.32034967 -0.09694878]\n",
      "Loss at iteration 140: w=[ 1.52388446 -0.18449244], loss=0.2048635754947825, gradiant=[-0.22831898  0.23540151]\n",
      "Loss at iteration 141: w=[ 1.52699362 -0.1836733 ], loss=0.20406733120220283, gradiant=[-0.31091639 -0.081914  ]\n",
      "Loss at iteration 142: w=[ 1.52926886 -0.18585812], loss=0.20328621682259185, gradiant=[-0.22752328  0.21848212]\n",
      "Loss at iteration 143: w=[ 1.53228884 -0.18517385], loss=0.20251990331021066, gradiant=[-0.30199799 -0.06842734]\n",
      "Loss at iteration 144: w=[ 1.53455301 -0.18720468], loss=0.20176807262171087, gradiant=[-0.22641696  0.20308278]\n",
      "Loss at iteration 145: w=[ 1.53748853 -0.18664129], loss=0.20103041703717983, gradiant=[-0.2935522  -0.05633829]\n",
      "Loss at iteration 146: w=[ 1.5397389  -0.18853189], loss=0.20030663853944758, gradiant=[-0.22503693  0.18905947]\n",
      "Loss at iteration 147: w=[ 1.5425943  -0.18807678], loss=0.19959644824618722, gradiant=[-0.2855407  -0.04551079]\n",
      "Loss at iteration 148: w=[ 1.54482847 -0.1898396 ], loss=0.19889956588986704, gradiant=[-0.22341642  0.17628199]\n",
      "Loss at iteration 149: w=[ 1.54760776 -0.18948138], loss=0.19821571934108329, gradiant=[-0.27792869 -0.03582191]\n",
      "Loss at iteration 150: w=[ 1.54982361 -0.19112771], loss=0.19754464417122977, gradiant=[-0.22158534  0.16463259]\n",
      "Loss at iteration 151: w=[ 1.55253046 -0.1908561 ], loss=0.19688608325084422, gradiant=[-0.27068459 -0.02716053]\n",
      "Loss at iteration 152: w=[ 1.55472616 -0.19239615], loss=0.1962397863803254, gradiant=[-0.21957062  0.1540048 ]\n",
      "Loss at iteration 153: w=[ 1.55736396 -0.19220189], loss=0.19560550995002335, gradiant=[-0.26377974 -0.01942623]\n",
      "Loss at iteration 154: w=[ 1.55953792 -0.19364491], loss=0.19498301662699774, gradiant=[-0.21739648  0.14430232]\n",
      "Loss at iteration 155: w=[ 1.5621098  -0.19351963], loss=0.1943720750659917, gradiant=[-0.2571881  -0.01252828]\n",
      "Loss at iteration 156: w=[ 1.56426065 -0.19487401], loss=0.1937724596424062, gradiant=[-0.21508467  0.13543806]\n",
      "Loss at iteration 157: w=[ 1.56676951 -0.19481016], loss=0.1931839502052654, gradiant=[-0.250886   -0.00638468]\n",
      "Loss at iteration 158: w=[ 1.56889606 -0.1960835 ], loss=0.1926063318483598, gradiant=[-0.21265476  0.12733329]\n",
      "Loss at iteration 159: w=[ 1.57134458 -0.19607428], loss=0.19203939469792286, gradiant=[-0.24485196 -0.00092132]\n",
      "Loss at iteration 160: w=[ 1.57344582 -0.19727345], loss=0.19148293371535444, gradiant=[-0.21012431  0.11991677]\n",
      "Loss at iteration 161: w=[ 1.57583649 -0.19731274], loss=0.19093674851364234, gradiant=[-0.2390664   0.00392875]\n",
      "Loss at iteration 162: w=[ 1.57791158 -0.19844398], loss=0.19040064318626632, gradiant=[-0.20750909  0.11312407]\n",
      "Loss at iteration 163: w=[ 1.58024669 -0.19852624], loss=0.18987442614747757, gradiant=[-0.23351154  0.00822602]\n",
      "Loss at iteration 164: w=[ 1.58229492 -0.19959521], loss=0.18935790998295682, gradiant=[-0.20482325  0.10689694]\n",
      "Loss at iteration 165: w=[ 1.58457664 -0.19971546], loss=0.18885091130994208, gradiant=[-0.22817118  0.01202515]\n",
      "Loss at iteration 166: w=[ 1.58659743 -0.20072729], loss=0.1883532506460111, gradiant=[-0.20207945  0.10118268]\n",
      "Loss at iteration 167: w=[ 1.58882774 -0.20088104], loss=0.1878647522857707, gradiant=[-0.22303057  0.01537554]\n",
      "Loss at iteration 168: w=[ 1.59082063 -0.20184038], loss=0.18738524418478286, gradiant=[-0.19928905  0.0959336 ]\n",
      "Loss at iteration 169: w=[ 1.59300139 -0.2020236 ], loss=0.18691455785011502, gradiant=[-0.21807627  0.01832178]\n",
      "Loss at iteration 170: w=[ 1.59496601 -0.20293466], loss=0.18645252823696393, gradiant=[-0.19646219  0.09110659]\n",
      "Loss at iteration 171: w=[ 1.59709897 -0.2031437 ], loss=0.1859989936508491, gradiant=[-0.21329602  0.0209042 ]\n",
      "Loss at iteration 172: w=[ 1.59903505 -0.20401033], loss=0.18555379565492186, gradiant=[-0.19360796  0.08666262]\n",
      "Loss at iteration 173: w=[ 1.60112184 -0.20424192], loss=0.18511677898197804, gradiant=[-0.20867865  0.02315919]\n",
      "Loss at iteration 174: w=[ 1.60302918 -0.20506759], loss=0.18468779145079817, gradiant=[-0.19073445  0.08256641]\n",
      "Loss at iteration 175: w=[ 1.60507132 -0.20531878], loss=0.1842666838864767, gradiant=[-0.20421394  0.02511964]\n",
      "Loss at iteration 176: w=[ 1.60694981 -0.20610664], loss=0.18385331004442984, gradiant=[-0.18784888  0.07878601]\n",
      "Loss at iteration 177: w=[ 1.60894874 -0.20637479], loss=0.18344752653780308, gradiant=[-0.19989257  0.02681526]\n",
      "Loss at iteration 178: w=[ 1.61079831 -0.20712772], loss=0.18304919276802312, gradiant=[-0.1849577   0.07529254]\n",
      "Loss at iteration 179: w=[ 1.61275537 -0.20741045], loss=0.1826581708582612, gradiant=[-0.19570599  0.02827287]\n",
      "Loss at iteration 180: w=[ 1.61457604 -0.20813105], loss=0.18227432558959916, gradiant=[-0.18206661  0.07205983]\n",
      "Loss at iteration 181: w=[ 1.6164925  -0.20842621], loss=0.18189752433970507, gradiant=[-0.1916464   0.02951669]\n",
      "Loss at iteration 182: w=[ 1.61828431 -0.20911686], loss=0.1815276370238431, gradiant=[-0.1791807   0.06906423]\n",
      "Loss at iteration 183: w=[ 1.62016138 -0.20942254], loss=0.18116453603806154, gradiant=[-0.18770666  0.0305686 ]\n",
      "Loss at iteration 184: w=[ 1.62192442 -0.21008539], loss=0.18080809620440946, gradiant=[-0.17630445  0.06628432]\n",
      "Loss at iteration 185: w=[ 1.62376322 -0.21039987], loss=0.18045819471805483, gradiant=[-0.18388019  0.03144836]\n",
      "Loss at iteration 186: w=[ 1.62549764 -0.21103688], loss=0.18011471109617833, gradiant=[-0.17344183  0.06370071]\n",
      "Loss at iteration 187: w=[ 1.62729925 -0.21135861], loss=0.17977752712853692, gradiant=[-0.18016099  0.03217378]\n",
      "Loss at iteration 188: w=[ 1.62900521 -0.21197157], loss=0.1794465268295942, gradiant=[-0.17059635  0.06129585]\n",
      "Loss at iteration 189: w=[ 1.63077065 -0.21229918], loss=0.1791215963921246, gradiant=[-0.17654351  0.03276096]\n",
      "Loss at iteration 190: w=[ 1.63244836 -0.21288972], loss=0.17880262414220965, gradiant=[-0.16777109  0.05905385]\n",
      "Loss at iteration 191: w=[ 1.63417859 -0.21322196], loss=0.17848950049554646, gradiant=[-0.17302268  0.03322444]\n",
      "Loss at iteration 192: w=[ 1.63582827 -0.21379157], loss=0.1781821179149985, gradiant=[-0.16496873  0.05696033]\n",
      "Loss at iteration 193: w=[ 1.63752421 -0.21412734], loss=0.17788037086932396, gradiant=[-0.1695938   0.03357731]\n",
      "Loss at iteration 194: w=[ 1.63914613 -0.21467736], loss=0.17758415579302134, gradiant=[-0.16219163  0.05500226]\n",
      "Loss at iteration 195: w=[ 1.64080865 -0.21501568], loss=0.17729337104723586, gradiant=[-0.16625256  0.03383141]\n",
      "Loss at iteration 196: w=[ 1.64240307 -0.21554736], loss=0.17700791688167872, gradiant=[-0.15944182  0.05316783]\n",
      "Loss at iteration 197: w=[ 1.64403302 -0.21588733], loss=0.17672769539751054, gradiant=[-0.16299498  0.0339974 ]\n",
      "Loss at iteration 198: w=[ 1.64560023 -0.21640179], loss=0.1764526105111445, gradiant=[-0.15672108  0.05144635]\n",
      "Loss at iteration 199: w=[ 1.64719841 -0.21674264], loss=0.17618256791893264, gradiant=[-0.15981736  0.03408492]\n",
      "Loss at iteration 200: w=[ 1.64873872 -0.21724092], loss=0.1759174750626955, gradiant=[-0.15403092  0.04982815]\n",
      "Loss at iteration 201: w=[ 1.65030588 -0.21758195], loss=0.17565724109606007, gradiant=[-0.15671629  0.03410263]\n",
      "Loss at iteration 202: w=[ 1.65181961 -0.218065  ], loss=0.1754017768515786, gradiant=[-0.15137262  0.04830443]\n",
      "Loss at iteration 203: w=[ 1.65335649 -0.21840558], loss=0.17515099480859267, gradiant=[-0.15368861  0.03405837]\n",
      "Loss at iteration 204: w=[ 1.65484396 -0.21887425], loss=0.17490480906181832, gradiant=[-0.14874728  0.04686726]\n",
      "Loss at iteration 205: w=[ 1.65635128 -0.21921384], loss=0.17466313529062738, gradiant=[-0.1507314   0.03395919]\n",
      "Loss at iteration 206: w=[ 1.65781284 -0.21966894], loss=0.17442589072899645, gradiant=[-0.14615581  0.04550941]\n",
      "Loss at iteration 207: w=[ 1.65929126 -0.22000705], loss=0.17419299413610695, gradiant=[-0.14784191  0.03381144]\n",
      "Loss at iteration 208: w=[ 1.66072725 -0.2204493 ], loss=0.17396436576756885, gradiant=[-0.14359894  0.04422434]\n",
      "Loss at iteration 209: w=[ 1.66217742 -0.2207855 ], loss=0.17373992734725255, gradiant=[-0.1450176   0.03362086]\n",
      "Loss at iteration 210: w=[ 1.66358819 -0.22121557], loss=0.17351960203970868, gradiant=[-0.14107729  0.04300613]\n",
      "Loss at iteration 211: w=[ 1.66501076 -0.22154949], loss=0.17330331442315816, gradiant=[-0.14225612  0.03339259]\n",
      "Loss at iteration 212: w=[ 1.66639667 -0.22196798], loss=0.17309099046303567, gradiant=[-0.13859132  0.04184939]\n",
      "Loss at iteration 213: w=[ 1.66779222 -0.2222993 ], loss=0.17288255748607073, gradiant=[-0.13955524  0.03313128]\n",
      "Loss at iteration 214: w=[ 1.66915363 -0.22270679], loss=0.17267794415489238, gradiant=[-0.1361414   0.04074924]\n",
      "Loss at iteration 215: w=[ 1.67052276 -0.2230352 ], loss=0.1724770804431422, gradiant=[-0.1369129  0.0328411]\n",
      "Loss at iteration 216: w=[ 1.67186004 -0.22343221], loss=0.17227989761108176, gradiant=[-0.13372776  0.03970124]\n",
      "Loss at iteration 217: w=[ 1.67320331 -0.22375747], loss=0.1720863281816844, gradiant=[-0.13432714  0.03252583]\n",
      "Loss at iteration 218: w=[ 1.67451682 -0.22414449], loss=0.1718963059171951, gradiant=[-0.13135058  0.03870136]\n",
      "Loss at iteration 219: w=[ 1.67583478 -0.22446637], loss=0.1717097657961494, gradiant=[-0.13179616  0.03218883]\n",
      "Loss at iteration 220: w=[ 1.67712488 -0.22484383], loss=0.17152664399084128, gradiant=[-0.12900993  0.03774596]\n",
      "Loss at iteration 221: w=[ 1.67841806 -0.22516216], loss=0.1713468778452242, gradiant=[-0.12931824  0.03183315]\n",
      "Loss at iteration 222: w=[ 1.67968512 -0.22553048], loss=0.17117040585324228, gradiant=[-0.12670582  0.03683169]\n",
      "Loss at iteration 223: w=[ 1.68095404 -0.2258451 ], loss=0.17099716763757494, gradiant=[-0.12689176  0.03146151]\n",
      "Loss at iteration 224: w=[ 1.68219842 -0.22620465], loss=0.17082710392878978, gradiant=[-0.12443818  0.03595555]\n",
      "Loss at iteration 225: w=[ 1.68344357 -0.22651542], loss=0.1706601565448932, gradiant=[-0.12451521  0.03107636]\n",
      "Loss at iteration 226: w=[ 1.68466564 -0.22686656], loss=0.17049626837127024, gradiant=[-0.12220691  0.03511478]\n",
      "Loss at iteration 227: w=[ 1.68588751 -0.22717336], loss=0.17033538334100332, gradiant=[-0.12218713  0.0306799 ]\n",
      "Loss at iteration 228: w=[ 1.68708763 -0.22751643], loss=0.17017744641556645, gradiant=[-0.12001182  0.03430688]\n",
      "Loss at iteration 229: w=[ 1.68828669 -0.22781917], loss=0.17002240356588033, gradiant=[-0.11990618  0.03027411]\n",
      "Loss at iteration 230: w=[ 1.68946522 -0.22815447], loss=0.16987020175372564, gradiant=[-0.11785272  0.03352957]\n",
      "Loss at iteration 231: w=[ 1.69064193 -0.22845308], loss=0.1697207889135071, gradiant=[-0.11767106  0.02986074]\n",
      "Loss at iteration 232: w=[ 1.69179922 -0.22878088], loss=0.16957411393435579, gradiant=[-0.11572934  0.03278078]\n",
      "Loss at iteration 233: w=[ 1.69295403 -0.2290753 ], loss=0.16943012664257043, gradiant=[-0.11548055  0.02944137]\n",
      "Loss at iteration 234: w=[ 1.69409044 -0.22939588], loss=0.16928877778438362, gradiant=[-0.1136414  0.0320586]\n",
      "Loss at iteration 235: w=[ 1.69522378 -0.22968606], loss=0.16915001900905172, gradiant=[-0.11333346  0.02901743]\n",
      "Loss at iteration 236: w=[ 1.69633966 -0.22999967], loss=0.1690138028522593, gradiant=[-0.11158859  0.03136133]\n",
      "Loss at iteration 237: w=[ 1.69745195 -0.23028557], loss=0.1688800827198307, gradiant=[-0.1112287   0.02859016]\n",
      "Loss at iteration 238: w=[ 1.69854766 -0.23059245], loss=0.1687488128717475, gradiant=[-0.10957056  0.03068737]\n",
      "Loss at iteration 239: w=[ 1.69963931 -0.23087405], loss=0.16861994840645955, gradiant=[-0.1091652  0.0281607]\n",
      "Loss at iteration 240: w=[ 1.70071518 -0.23117441], loss=0.16849344524548962, gradiant=[-0.10758695  0.03003529]\n",
      "Loss at iteration 241: w=[ 1.7017866  -0.23145171], loss=0.16836926011832035, gradiant=[-0.10714195  0.02773004]\n",
      "Loss at iteration 242: w=[ 1.70284297 -0.23174574], loss=0.1682473505475645, gradiant=[-0.10563736  0.02940378]\n",
      "Loss at iteration 243: w=[ 1.70389455 -0.23201874], loss=0.16812767483440627, gradiant=[-0.10515797  0.02729908]\n",
      "Loss at iteration 244: w=[ 1.70493176 -0.23230665], loss=0.16801019204431372, gradiant=[-0.10372141  0.02879163]\n",
      "Loss at iteration 245: w=[ 1.70596389 -0.23257534], loss=0.1678948619930142, gradiant=[-0.10321234  0.02686861]\n",
      "Loss at iteration 246: w=[ 1.70698227 -0.23285732], loss=0.16778164523272893, gradiant=[-0.10183867  0.02819774]\n",
      "Loss at iteration 247: w=[ 1.70799532 -0.23312171], loss=0.16767050303866204, gradiant=[-0.10130416  0.02643934]\n",
      "Loss at iteration 248: w=[ 1.7089952  -0.23339792], loss=0.16756139739573744, gradiant=[-0.09998872  0.0276211 ]\n",
      "Loss at iteration 249: w=[ 1.70998953 -0.23365804], loss=0.16745429098558132, gradiant=[-0.09943257  0.02601187]\n",
      "Loss at iteration 250: w=[ 1.71097124 -0.23392865], loss=0.16734914717374388, gradiant=[-0.09817112  0.02706078]\n",
      "Loss at iteration 251: w=[ 1.71194721 -0.23418451], loss=0.16724592999715682, gradiant=[-0.09759675  0.02558677]\n",
      "Loss at iteration 252: w=[ 1.71291106 -0.23444967], loss=0.16714460415182097, gradiant=[-0.09638543  0.02651593]\n",
      "Loss at iteration 253: w=[ 1.71386902 -0.23470132], loss=0.16704513498072104, gradiant=[-0.09579589  0.02516451]\n",
      "Loss at iteration 254: w=[ 1.71481533 -0.23496118], loss=0.16694748846196228, gradiant=[-0.0946312   0.02598576]\n",
      "Loss at iteration 255: w=[ 1.71575563 -0.23520863], loss=0.166851631197126, gradiant=[-0.09402923  0.02474552]\n",
      "Loss at iteration 256: w=[ 1.7166847  -0.23546333], loss=0.16675753039983823, gradiant=[-0.09290796  0.02546957]\n",
      "Loss at iteration 257: w=[ 1.71760767 -0.23570663], loss=0.16666515388454878, gradiant=[-0.09229603  0.02433016]\n",
      "Loss at iteration 258: w=[ 1.71851982 -0.2359563 ], loss=0.16657447005551695, gradiant=[-0.09121527  0.02496667]\n",
      "Loss at iteration 259: w=[ 1.71942577 -0.23619548], loss=0.16648544789599923, gradiant=[-0.09059558  0.02391876]\n",
      "Loss at iteration 260: w=[ 1.7203213  -0.23644025], loss=0.16639805695763515, gradiant=[-0.08955266  0.02447646]\n",
      "Loss at iteration 261: w=[ 1.72121057 -0.23667536], loss=0.16631226735002858, gradiant=[-0.08892717  0.0235116 ]\n",
      "Loss at iteration 262: w=[ 1.72208977 -0.23691535], loss=0.16622804973052052, gradiant=[-0.08791967  0.02399838]\n",
      "Loss at iteration 263: w=[ 1.72296267 -0.23714644], loss=0.16614537529414908, gradiant=[-0.08729013  0.0231089 ]\n",
      "Loss at iteration 264: w=[ 1.72382583 -0.23738176], loss=0.1660642157637935, gradiant=[-0.08631585  0.0235319 ]\n",
      "Loss at iteration 265: w=[ 1.72468267 -0.23760886], loss=0.16598454338050073, gradiant=[-0.08568382  0.02271089]\n",
      "Loss at iteration 266: w=[ 1.72553007 -0.23783963], loss=0.1659063308939866, gradiant=[-0.08474072  0.02307653]\n",
      "Loss at iteration 267: w=[ 1.72637115 -0.23806281], loss=0.16582955155331458, gradiant=[-0.08410759  0.02231773]\n",
      "Loss at iteration 268: w=[ 1.72720309 -0.23828912], loss=0.16575417909774304, gradiant=[-0.08319382  0.02263182]\n",
      "Loss at iteration 269: w=[ 1.7280287  -0.23850842], loss=0.16568018774774118, gradiant=[-0.08256084  0.02192957]\n",
      "Loss at iteration 270: w=[ 1.72884544 -0.23873039], loss=0.16560755219617054, gradiant=[-0.08167471  0.02219737]\n",
      "Loss at iteration 271: w=[ 1.72965587 -0.23894586], loss=0.16553624759962698, gradiant=[-0.08104297  0.02154652]\n",
      "Loss at iteration 272: w=[ 1.7304577  -0.23916359], loss=0.16546624956994235, gradiant=[-0.08018293  0.02177277]\n",
      "Loss at iteration 273: w=[ 1.73125324 -0.23937527], loss=0.16539753416584158, gradiant=[-0.0795534   0.02116869]\n",
      "Loss at iteration 274: w=[ 1.73204042 -0.23958885], loss=0.16533007788475296, gradiant=[-0.07871802  0.02135766]\n",
      "Loss at iteration 275: w=[ 1.73282133 -0.23979681], loss=0.16526385765476842, gradiant=[-0.07809156  0.02079615]\n",
      "Loss at iteration 276: w=[ 1.73359413 -0.24000633], loss=0.16519885082675143, gradiant=[-0.07727953  0.02095171]\n",
      "Loss at iteration 277: w=[ 1.7343607  -0.24021062], loss=0.16513503516658923, gradiant=[-0.0766569   0.02042897]\n",
      "Loss at iteration 278: w=[ 1.73511937 -0.24041616], loss=0.1650723888475874, gradiant=[-0.07586703  0.0205546 ]\n",
      "Loss at iteration 279: w=[ 1.73587186 -0.24061684], loss=0.1650108904430044, gradiant=[-0.07524889  0.02006718]\n",
      "Loss at iteration 280: w=[ 1.73661666 -0.2408185 ], loss=0.16495051891872103, gradiant=[-0.07448007  0.02016604]\n",
      "Loss at iteration 281: w=[ 1.73735533 -0.24101561], loss=0.16489125362604712, gradiant=[-0.073867    0.01971082]\n",
      "Loss at iteration 282: w=[ 1.73808651 -0.24121346], loss=0.16483307429465743, gradiant=[-0.07311822  0.01978574]\n",
      "Loss at iteration 283: w=[ 1.73881162 -0.24140706], loss=0.16477596102565878, gradiant=[-0.07251073  0.0193599 ]\n",
      "Loss at iteration 284: w=[ 1.73952943 -0.2416012 ], loss=0.16471989428478362, gradiant=[-0.07178104  0.01941344]\n",
      "Loss at iteration 285: w=[ 1.74024122 -0.24179134], loss=0.16466485489570798, gradiant=[-0.07117957  0.01901442]\n",
      "Loss at iteration 286: w=[ 1.7409459  -0.24198183], loss=0.16461082403349253, gradiant=[-0.07046811  0.0190489 ]\n",
      "Loss at iteration 287: w=[ 1.74164463 -0.24216857], loss=0.16455778321814366, gradiant=[-0.06987304  0.01867438]\n",
      "Loss at iteration 288: w=[ 1.74233642 -0.24235549], loss=0.16450571430829292, gradiant=[-0.06917902  0.01869188]\n",
      "Loss at iteration 289: w=[ 1.74302233 -0.24253889], loss=0.16445459949499058, gradiant=[-0.06859067  0.01833978]\n",
      "Loss at iteration 290: w=[ 1.74370146 -0.24272231], loss=0.16440442129561617, gradiant=[-0.06791333  0.01834218]\n",
      "Loss at iteration 291: w=[ 1.74437478 -0.24290242], loss=0.16435516254789712, gradiant=[-0.06733199  0.01801057]\n",
      "Loss at iteration 292: w=[ 1.74504149 -0.24308241], loss=0.16430680640403933, gradiant=[-0.06667066  0.01799957]\n",
      "Loss at iteration 293: w=[ 1.74570246 -0.24325928], loss=0.16425933632496476, gradiant=[-0.06609656  0.01768674]\n",
      "Loss at iteration 294: w=[ 1.74635696 -0.24343592], loss=0.1642127360746541, gradiant=[-0.06545059  0.01766386]\n",
      "Loss at iteration 295: w=[ 1.7470058 -0.2436096], loss=0.164166989714593, gradiant=[-0.06488392  0.01736825]\n",
      "Loss at iteration 296: w=[ 1.74764833 -0.24378295], loss=0.16412208159832226, gradiant=[-0.06425272  0.01733487]\n",
      "Loss at iteration 297: w=[ 1.74828526 -0.2439535 ], loss=0.16407799636608425, gradiant=[-0.06369364  0.01705506]\n",
      "Loss at iteration 298: w=[ 1.74891603 -0.24412362], loss=0.16403471893957022, gradiant=[-0.06307666  0.01701242]\n",
      "Loss at iteration 299: w=[ 1.74954128 -0.2442911 ], loss=0.16399223451676342, gradiant=[-0.06252531  0.01674712]\n",
      "Loss at iteration 300: w=[ 1.7501605  -0.24445806], loss=0.16395052856687548, gradiant=[-0.06192203  0.01669634]\n",
      "Loss at iteration 301: w=[ 1.75077429 -0.2446225 ], loss=0.1639095868253771, gradiant=[-0.0613785   0.01644439]\n",
      "Loss at iteration 302: w=[ 1.75138217 -0.24478637], loss=0.1638693952891187, gradiant=[-0.06078844  0.01638647]\n",
      "Loss at iteration 303: w=[ 1.7519847  -0.24494784], loss=0.16382994021154124, gradiant=[-0.06025281  0.0161468 ]\n",
      "Loss at iteration 304: w=[ 1.75258146 -0.24510866], loss=0.16379120809797437, gradiant=[-0.05967553  0.01608266]\n",
      "Loss at iteration 305: w=[ 1.75317294 -0.24526721], loss=0.16375318570102057, gradiant=[-0.05914785  0.01585431]\n",
      "Loss at iteration 306: w=[ 1.75375876 -0.24542505], loss=0.16371586001602456, gradiant=[-0.05858291  0.01578475]\n",
      "Loss at iteration 307: w=[ 1.7543394  -0.24558072], loss=0.16367921827662504, gradiant=[-0.05806322  0.01556686]\n",
      "Loss at iteration 308: w=[ 1.7549145  -0.24573565], loss=0.16364324795038832, gradiant=[-0.05751024  0.01549261]\n",
      "Loss at iteration 309: w=[ 1.75548448 -0.24588849], loss=0.1636079367345219, gradiant=[-0.05699854  0.01528437]\n",
      "Loss at iteration 310: w=[ 1.75604906 -0.24604055], loss=0.16357327255166665, gradiant=[-0.05645714  0.0152061 ]\n",
      "Loss at iteration 311: w=[ 1.75660859 -0.24619062], loss=0.16353924354576566, gradiant=[-0.05595345  0.0150068 ]\n",
      "Loss at iteration 312: w=[ 1.75716282 -0.24633987], loss=0.1635058380780093, gradiant=[-0.05542327  0.0149251 ]\n",
      "Loss at iteration 313: w=[ 1.7577121  -0.24648721], loss=0.16347304472285504, gradiant=[-0.05492756  0.01473408]\n",
      "Loss at iteration 314: w=[ 1.75825618 -0.24663371], loss=0.16344085226411806, gradiant=[-0.05440829  0.01464947]\n",
      "Loss at iteration 315: w=[ 1.75879539 -0.24677837], loss=0.163409249691137, gradiant=[-0.05392054  0.01446614]\n",
      "Loss at iteration 316: w=[ 1.75932951 -0.24692216], loss=0.16337822619500608, gradiant=[-0.05341185  0.0143791 ]\n",
      "Loss at iteration 317: w=[ 1.75985883 -0.24706419], loss=0.16334777116487983, gradiant=[-0.05293202  0.01420291]\n",
      "Loss at iteration 318: w=[ 1.76038316 -0.24720533], loss=0.16331787418434252, gradiant=[-0.05243362  0.01411386]\n",
      "Loss at iteration 319: w=[ 1.76090278 -0.24734477], loss=0.1632885250278463, gradiant=[-0.05196166  0.01394433]\n",
      "Loss at iteration 320: w=[ 1.76141751 -0.24748331], loss=0.16325971365721337, gradiant=[-0.05147326  0.01385366]\n",
      "Loss at iteration 321: w=[ 1.7619276  -0.24762021], loss=0.1632314302182034, gradiant=[-0.05100912  0.01369032]\n",
      "Loss at iteration 322: w=[ 1.76243291 -0.24775619], loss=0.16320366503714165, gradiant=[-0.05053047  0.01359838]\n",
      "Loss at iteration 323: w=[ 1.76293365 -0.2478906 ], loss=0.163176408617612, gradiant=[-0.05007408  0.01344083]\n",
      "Loss at iteration 324: w=[ 1.7634297  -0.24802408], loss=0.16314965163720746, gradiant=[-0.04960491  0.01334791]\n",
      "Loss at iteration 325: w=[ 1.76392126 -0.24815604], loss=0.1631233849443427, gradiant=[-0.0491562   0.01319577]\n",
      "Loss at iteration 326: w=[ 1.76440822 -0.24828706], loss=0.16309759955512343, gradiant=[-0.04869628  0.01310215]\n",
      "Loss at iteration 327: w=[ 1.76489077 -0.24841661], loss=0.1630722866502739, gradiant=[-0.04825518  0.01295509]\n",
      "Loss at iteration 328: w=[ 1.76536882 -0.24854522], loss=0.16304743757212053, gradiant=[-0.04780426  0.01286101]\n",
      "Loss at iteration 329: w=[ 1.76584252 -0.24867241], loss=0.16302304382163, gradiant=[-0.04737069  0.01271871]\n",
      "Loss at iteration 330: w=[ 1.76631181 -0.24879865], loss=0.16299909705550364, gradiant=[-0.04692857  0.0126244 ]\n",
      "Loss at iteration 331: w=[ 1.76677683 -0.24892352], loss=0.16297558908332288, gradiant=[-0.04650244  0.01248657]\n",
      "Loss at iteration 332: w=[ 1.76723752 -0.24904744], loss=0.16295251186474785, gradiant=[-0.0460689  0.0123922]\n",
      "Loss at iteration 333: w=[ 1.76769402 -0.24917003], loss=0.16292985750676842, gradiant=[-0.04565012  0.01225859]\n",
      "Loss at iteration 334: w=[ 1.76814627 -0.24929167], loss=0.1629076182610028, gradiant=[-0.04522495  0.01216435]\n",
      "Loss at iteration 335: w=[ 1.76859441 -0.24941202], loss=0.16288578652104946, gradiant=[-0.04481344  0.01203471]\n",
      "Loss at iteration 336: w=[ 1.76903837 -0.24953142], loss=0.1628643548198837, gradiant=[-0.04439645  0.01194075]\n",
      "Loss at iteration 337: w=[ 1.76947829 -0.24964957], loss=0.16284331582730519, gradiant=[-0.04399211  0.01181486]\n",
      "Loss at iteration 338: w=[ 1.76991412 -0.24976679], loss=0.1628226623474296, gradiant=[-0.04358312  0.01172131]\n",
      "Loss at iteration 339: w=[ 1.77034598 -0.24988278], loss=0.1628023873162282, gradiant=[-0.04318585  0.01159898]\n",
      "Loss at iteration 340: w=[ 1.77077383 -0.24999784], loss=0.16278248379911184, gradiant=[-0.04278467  0.01150595]\n",
      "Loss at iteration 341: w=[ 1.77119777 -0.25011171], loss=0.1627629449885589, gradiant=[-0.04239438  0.01138699]\n",
      "Loss at iteration 342: w=[ 1.77161778 -0.25022465], loss=0.1627437642017862, gradiant=[-0.04200083  0.0112946 ]\n",
      "Loss at iteration 343: w=[ 1.77203396 -0.25033644], loss=0.16272493487846512, gradiant=[-0.04161742  0.01117883]\n",
      "Loss at iteration 344: w=[ 1.77244627 -0.25044731], loss=0.1627064505784759, gradiant=[-0.04123135  0.01108717]\n",
      "Loss at iteration 345: w=[ 1.77285482 -0.25055706], loss=0.16268830497970596, gradiant=[-0.04085471  0.01097445]\n",
      "Loss at iteration 346: w=[ 1.77325958 -0.25066589], loss=0.16267049187588756, gradiant=[-0.04047595  0.01088358]\n",
      "Loss at iteration 347: w=[ 1.77366064 -0.25077363], loss=0.16265300517447462, gradiant=[-0.04010599  0.01077376]\n",
      "Loss at iteration 348: w=[ 1.77405798 -0.25088047], loss=0.16263583889455968, gradiant=[-0.03973438  0.01068377]\n",
      "Loss at iteration 349: w=[ 1.77445169 -0.25098623], loss=0.16261898716482726, gradiant=[-0.039371    0.01057671]\n",
      "Loss at iteration 350: w=[ 1.77484175 -0.25109111], loss=0.16260244422154635, gradiant=[-0.0390064   0.01048765]\n",
      "Loss at iteration 351: w=[ 1.77522825 -0.25119494], loss=0.16258620440659924, gradiant=[-0.03864949  0.01038324]\n",
      "Loss at iteration 352: w=[ 1.77561117 -0.25129789], loss=0.16257026216554596, gradiant=[-0.03829174  0.01029517]\n",
      "Loss at iteration 353: w=[ 1.77599058 -0.25139983], loss=0.1625546120457242, gradiant=[-0.03794121  0.01019328]\n",
      "Loss at iteration 354: w=[ 1.77636648 -0.25150089], loss=0.16253924869438519, gradiant=[-0.03759017  0.01010623]\n",
      "Loss at iteration 355: w=[ 1.77673894 -0.25160096], loss=0.16252416685686216, gradiant=[-0.03724591  0.01000677]\n",
      "Loss at iteration 356: w=[ 1.77710795 -0.25170017], loss=0.16250936137477345, gradiant=[-0.03690145  0.00992079]\n",
      "Loss at iteration 357: w=[ 1.77747359 -0.2517984 ], loss=0.1624948271842585, gradiant=[-0.03656336  0.00982365]\n",
      "Loss at iteration 358: w=[ 1.77783584 -0.25189579], loss=0.1624805593142449, gradiant=[-0.03622534  0.00973877]\n",
      "Loss at iteration 359: w=[ 1.77819477 -0.25199223], loss=0.1624665528847493, gradiant=[-0.03589333  0.00964387]\n",
      "Loss at iteration 360: w=[ 1.77855039 -0.25208783], loss=0.16245280310520777, gradiant=[-0.03556161  0.00956011]\n",
      "Loss at iteration 361: w=[ 1.77890275 -0.2521825 ], loss=0.1624393052728373, gradiant=[-0.03523557  0.00946736]\n",
      "Loss at iteration 362: w=[ 1.77925185 -0.25227635], loss=0.1624260547710271, gradiant=[-0.03491004  0.00938475]\n",
      "Loss at iteration 363: w=[ 1.77959774 -0.25236929], loss=0.16241304706776039, gradiant=[-0.03458988  0.00929406]\n",
      "Loss at iteration 364: w=[ 1.77994045 -0.25246142], loss=0.16240027771406357, gradiant=[-0.0342704   0.00921261]\n",
      "Loss at iteration 365: w=[ 1.78028001 -0.25255266], loss=0.1623877423424846, gradiant=[-0.03395602  0.00912392]\n",
      "Loss at iteration 366: w=[ 1.78061643 -0.25264309], loss=0.16237543666559978, gradiant=[-0.03364248  0.00904364]\n",
      "Loss at iteration 367: w=[ 1.78094977 -0.25273266], loss=0.16236335647454686, gradiant=[-0.03333378  0.00895689]\n",
      "Loss at iteration 368: w=[ 1.78128003 -0.25282144], loss=0.16235149763758544, gradiant=[-0.03302607  0.00887779]\n",
      "Loss at iteration 369: w=[ 1.78160726 -0.25290937], loss=0.16233985609868432, gradiant=[-0.03272294  0.0087929 ]\n",
      "Loss at iteration 370: w=[ 1.78193147 -0.25299652], loss=0.16232842787613344, gradiant=[-0.03242094  0.00871499]\n",
      "Loss at iteration 371: w=[ 1.7822527  -0.25308284], loss=0.1623172090611832, gradiant=[-0.0321233  0.0086319]\n",
      "Loss at iteration 372: w=[ 1.78257097 -0.25316839], loss=0.16230619581670602, gradiant=[-0.0318269   0.00855518]\n",
      "Loss at iteration 373: w=[ 1.78288632 -0.25325313], loss=0.16229538437588562, gradiant=[-0.03153466  0.00847384]\n",
      "Loss at iteration 374: w=[ 1.78319876 -0.25333711], loss=0.16228477104092706, gradiant=[-0.03124374  0.00839832]\n",
      "Loss at iteration 375: w=[ 1.78350833 -0.2534203 ], loss=0.1622743521817932, gradiant=[-0.0309568   0.00831867]\n",
      "Loss at iteration 376: w=[ 1.78381504 -0.25350274], loss=0.16226412423496273, gradiant=[-0.03067126  0.00824433]\n",
      "Loss at iteration 377: w=[ 1.78411893 -0.2535844 ], loss=0.16225408370221117, gradiant=[-0.03038953  0.00816633]\n",
      "Loss at iteration 378: w=[ 1.78442003 -0.25366534], loss=0.1622442271494146, gradiant=[-0.03010927  0.00809318]\n",
      "Loss at iteration 379: w=[ 1.78471835 -0.2537455 ], loss=0.16223455120537503, gradiant=[-0.02983266  0.00801677]\n",
      "Loss at iteration 380: w=[ 1.78501393 -0.25382495], loss=0.16222505256066724, gradiant=[-0.02955758  0.00794481]\n",
      "Loss at iteration 381: w=[ 1.78530679 -0.25390365], loss=0.1622157279665073, gradiant=[-0.02928599  0.00786995]\n",
      "Loss at iteration 382: w=[ 1.78559695 -0.25398164], loss=0.1622065742336408, gradiant=[-0.02901599  0.00779916]\n",
      "Loss at iteration 383: w=[ 1.78588444 -0.2540589 ], loss=0.16219758823125266, gradiant=[-0.02874935  0.00772581]\n",
      "Loss at iteration 384: w=[ 1.78616928 -0.25413546], loss=0.16218876688589567, gradiant=[-0.02848433  0.00765619]\n",
      "Loss at iteration 385: w=[ 1.78645151 -0.25421131], loss=0.16218010718044001, gradiant=[-0.02822254  0.0075843 ]\n",
      "Loss at iteration 386: w=[ 1.78673113 -0.25428646], loss=0.1621716061530403, gradiant=[-0.02796241  0.00751584]\n",
      "Loss at iteration 387: w=[ 1.78700819 -0.25436092], loss=0.1621632608961237, gradiant=[-0.02770538  0.00744538]\n",
      "Loss at iteration 388: w=[ 1.78728269 -0.2544347 ], loss=0.16215506855539494, gradiant=[-0.02745004  0.00737807]\n",
      "Loss at iteration 389: w=[ 1.78755467 -0.25450779], loss=0.16214702632886002, gradiant=[-0.0271977  0.007309 ]\n",
      "Loss at iteration 390: w=[ 1.78782414 -0.25458022], loss=0.16213913146586753, gradiant=[-0.02694707  0.00724283]\n",
      "Loss at iteration 391: w=[ 1.78809113 -0.25465197], loss=0.1621313812661687, gradiant=[-0.02669932  0.00717512]\n",
      "Loss at iteration 392: w=[ 1.78835566 -0.25472307], loss=0.16212377307899323, gradiant=[-0.02645331  0.00711007]\n",
      "Loss at iteration 393: w=[ 1.78861776 -0.25479351], loss=0.16211630430214258, gradiant=[-0.02621008  0.00704368]\n",
      "Loss at iteration 394: w=[ 1.78887745 -0.2548633 ], loss=0.16210897238110075, gradiant=[-0.0259686   0.00697975]\n",
      "Loss at iteration 395: w=[ 1.78913475 -0.25493245], loss=0.16210177480815927, gradiant=[-0.02572981  0.00691465]\n",
      "Loss at iteration 396: w=[ 1.78938967 -0.25500097], loss=0.16209470912156063, gradiant=[-0.02549277  0.00685182]\n",
      "Loss at iteration 397: w=[ 1.78964226 -0.25506885], loss=0.16208777290465598, gradiant=[-0.02525833  0.00678798]\n",
      "Loss at iteration 398: w=[ 1.78989251 -0.25513611], loss=0.1620809637850783, gradiant=[-0.02502565  0.00672624]\n",
      "Loss at iteration 399: w=[ 1.79014047 -0.25520275], loss=0.1620742794339311, gradiant=[-0.0247955   0.00666363]\n",
      "Loss at iteration 400: w=[ 1.79038614 -0.25526878], loss=0.16206771756499208, gradiant=[-0.0245671   0.00660296]\n",
      "Loss at iteration 401: w=[ 1.79062955 -0.25533419], loss=0.16206127593393116, gradiant=[-0.02434114  0.00654156]\n",
      "Loss at iteration 402: w=[ 1.79087072 -0.25539901], loss=0.16205495233754225, gradiant=[-0.02411694  0.00648195]\n",
      "Loss at iteration 403: w=[ 1.79110967 -0.25546323], loss=0.16204874461299087, gradiant=[-0.02389512  0.00642171]\n",
      "Loss at iteration 404: w=[ 1.79134642 -0.25552686], loss=0.16204265063707346, gradiant=[-0.02367504  0.00636315]\n",
      "Loss at iteration 405: w=[ 1.791581  -0.2555899], loss=0.16203666832549107, gradiant=[-0.02345726  0.00630407]\n",
      "Loss at iteration 406: w=[ 1.79181341 -0.25565237], loss=0.16203079563213746, gradiant=[-0.02324123  0.00624653]\n",
      "Loss at iteration 407: w=[ 1.79204368 -0.25571425], loss=0.16202503054839837, gradiant=[-0.02302743  0.00618857]\n",
      "Loss at iteration 408: w=[ 1.79227184 -0.25577557], loss=0.16201937110246473, gradiant=[-0.02281537  0.00613206]\n",
      "Loss at iteration 409: w=[ 1.79249789 -0.25583632], loss=0.16201381535865866, gradiant=[-0.02260548  0.00607519]\n",
      "Loss at iteration 410: w=[ 1.79272186 -0.25589652], loss=0.16200836141677083, gradiant=[-0.02239731  0.00601968]\n",
      "Loss at iteration 411: w=[ 1.79294378 -0.25595616], loss=0.16200300741141122, gradiant=[-0.02219126  0.00596389]\n",
      "Loss at iteration 412: w=[ 1.79316365 -0.25601525], loss=0.16199775151136997, gradiant=[-0.02198691  0.00590936]\n",
      "Loss at iteration 413: w=[ 1.79338149 -0.2560738 ], loss=0.1619925919189928, gradiant=[-0.02178463  0.00585462]\n",
      "Loss at iteration 414: w=[ 1.79359733 -0.25613181], loss=0.16198752686956422, gradiant=[-0.02158403  0.00580106]\n",
      "Loss at iteration 415: w=[ 1.79381119 -0.25618928], loss=0.1619825546307055, gradiant=[-0.02138545  0.00574736]\n",
      "Loss at iteration 416: w=[ 1.79402307 -0.25624623], loss=0.16197767350178116, gradiant=[-0.02118853  0.00569475]\n",
      "Loss at iteration 417: w=[ 1.79423301 -0.25630265], loss=0.1619728818133181, gradiant=[-0.02099359  0.00564206]\n",
      "Loss at iteration 418: w=[ 1.79444101 -0.25635856], loss=0.16196817792643375, gradiant=[-0.02080029  0.00559039]\n",
      "Loss at iteration 419: w=[ 1.7946471  -0.25641394], loss=0.16196356023227632, gradiant=[-0.0206089   0.00553868]\n",
      "Loss at iteration 420: w=[ 1.79485129 -0.25646882], loss=0.16195902715147395, gradiant=[-0.02041915  0.00548795]\n",
      "Loss at iteration 421: w=[ 1.7950536  -0.25652319], loss=0.1619545771335948, gradiant=[-0.02023127  0.0054372 ]\n",
      "Loss at iteration 422: w=[ 1.79525405 -0.25657707], loss=0.16195020865661702, gradiant=[-0.020045    0.00538738]\n",
      "Loss at iteration 423: w=[ 1.79545266 -0.25663044], loss=0.1619459202264072, gradiant=[-0.01986055  0.00533758]\n",
      "Loss at iteration 424: w=[ 1.79564944 -0.25668333], loss=0.16194171037621075, gradiant=[-0.0196777   0.00528865]\n",
      "Loss at iteration 425: w=[ 1.7958444  -0.25673573], loss=0.16193757766614897, gradiant=[-0.01949663  0.00523979]\n",
      "Loss at iteration 426: w=[ 1.79603757 -0.25678765], loss=0.16193352068272754, gradiant=[-0.01931713  0.00519174]\n",
      "Loss at iteration 427: w=[ 1.79622897 -0.25683908], loss=0.1619295380383522, gradiant=[-0.01913938  0.00514378]\n",
      "Loss at iteration 428: w=[ 1.7964186  -0.25689005], loss=0.1619256283708547, gradiant=[-0.01896317  0.0050966 ]\n",
      "Loss at iteration 429: w=[ 1.79660649 -0.25694054], loss=0.16192179034302684, gradiant=[-0.01878867  0.00504953]\n",
      "Loss at iteration 430: w=[ 1.79679264 -0.25699058], loss=0.1619180226421632, gradiant=[-0.0186157  0.0050032]\n",
      "Loss at iteration 431: w=[ 1.79697709 -0.25704015], loss=0.16191432397961172, gradiant=[-0.01844439  0.00495701]\n",
      "Loss at iteration 432: w=[ 1.79715983 -0.25708926], loss=0.16191069309033362, gradiant=[-0.01827459  0.00491152]\n",
      "Loss at iteration 433: w=[ 1.7973409  -0.25713792], loss=0.16190712873246976, gradiant=[-0.01810642  0.00486619]\n",
      "Loss at iteration 434: w=[ 1.79752029 -0.25718614], loss=0.1619036296869173, gradiant=[-0.01793973  0.00482152]\n",
      "Loss at iteration 435: w=[ 1.79769804 -0.25723391], loss=0.16190019475691106, gradiant=[-0.01777464  0.00477703]\n",
      "Loss at iteration 436: w=[ 1.79787415 -0.25728124], loss=0.16189682276761558, gradiant=[-0.01761101  0.00473317]\n",
      "Loss at iteration 437: w=[ 1.79804864 -0.25732814], loss=0.16189351256572257, gradiant=[-0.01744894  0.0046895 ]\n",
      "Loss at iteration 438: w=[ 1.79822152 -0.2573746 ], loss=0.16189026301905687, gradiant=[-0.01728831  0.00464643]\n",
      "Loss at iteration 439: w=[ 1.79839282 -0.25742064], loss=0.1618870730161886, gradiant=[-0.01712921  0.00460357]\n",
      "Loss at iteration 440: w=[ 1.79856253 -0.25746625], loss=0.16188394146605373, gradiant=[-0.01697153  0.00456129]\n",
      "Loss at iteration 441: w=[ 1.79873068 -0.25751144], loss=0.16188086729758047, gradiant=[-0.01681534  0.00451922]\n",
      "Loss at iteration 442: w=[ 1.79889729 -0.25755622], loss=0.161877849459323, gradiant=[-0.01666055  0.00447771]\n",
      "Loss at iteration 443: w=[ 1.79906236 -0.25760058], loss=0.16187488691910207, gradiant=[-0.01650722  0.00443642]\n",
      "Loss at iteration 444: w=[ 1.79922592 -0.25764454], loss=0.16187197866365172, gradiant=[-0.01635527  0.00439565]\n",
      "Loss at iteration 445: w=[ 1.79938796 -0.25768809], loss=0.16186912369827275, gradiant=[-0.01620475  0.00435513]\n",
      "Loss at iteration 446: w=[ 1.79954852 -0.25773124], loss=0.16186632104649248, gradiant=[-0.01605558  0.00431511]\n",
      "Loss at iteration 447: w=[ 1.7997076  -0.25777399], loss=0.16186356974973104, gradiant=[-0.01590781  0.00427533]\n",
      "Loss at iteration 448: w=[ 1.79986521 -0.25781635], loss=0.16186086886697312, gradiant=[-0.01576138  0.00423604]\n",
      "Loss at iteration 449: w=[ 1.80002137 -0.25785832], loss=0.16185821747444656, gradiant=[-0.01561632  0.00419699]\n",
      "Loss at iteration 450: w=[ 1.8001761  -0.25789991], loss=0.161855614665306, gradiant=[-0.01547257  0.00415841]\n",
      "Loss at iteration 451: w=[ 1.8003294  -0.25794111], loss=0.16185305954932258, gradiant=[-0.01533017  0.00412009]\n",
      "Loss at iteration 452: w=[ 1.80048129 -0.25798193], loss=0.16185055125258044, gradiant=[-0.01518906  0.00408221]\n",
      "Loss at iteration 453: w=[ 1.80063178 -0.25802238], loss=0.1618480889171763, gradiant=[-0.01504927  0.00404459]\n",
      "Loss at iteration 454: w=[ 1.80078089 -0.25806245], loss=0.16184567170092762, gradiant=[-0.01491074  0.00400741]\n",
      "Loss at iteration 455: w=[ 1.80092863 -0.25810216], loss=0.1618432987770833, gradiant=[-0.01477351  0.00397048]\n",
      "Loss at iteration 456: w=[ 1.801075  -0.2581415], loss=0.1618409693340415, gradiant=[-0.01463752  0.00393398]\n",
      "Loss at iteration 457: w=[ 1.80122003 -0.25818047], loss=0.16183868257507203, gradiant=[-0.0145028   0.00389773]\n",
      "Loss at iteration 458: w=[ 1.80136372 -0.25821909], loss=0.16183643771804374, gradiant=[-0.01436931  0.00386189]\n",
      "Loss at iteration 459: w=[ 1.80150609 -0.25825736], loss=0.16183423399515684, gradiant=[-0.01423706  0.00382631]\n",
      "Loss at iteration 460: w=[ 1.80164715 -0.25829527], loss=0.1618320706526808, gradiant=[-0.01410601  0.00379113]\n",
      "Loss at iteration 461: w=[ 1.80178692 -0.25833283], loss=0.16182994695069589, gradiant=[-0.01397618  0.0037562 ]\n",
      "Loss at iteration 462: w=[ 1.80192539 -0.25837005], loss=0.16182786216284112, gradiant=[-0.01384753  0.00372166]\n",
      "Loss at iteration 463: w=[ 1.80206259 -0.25840692], loss=0.16182581557606426, gradiant=[-0.01372008  0.00368737]\n",
      "Loss at iteration 464: w=[ 1.80219853 -0.25844345], loss=0.16182380649037953, gradiant=[-0.01359379  0.00365346]\n",
      "Loss at iteration 465: w=[ 1.80233322 -0.25847965], loss=0.16182183421862717, gradiant=[-0.01346868  0.00361981]\n",
      "Loss at iteration 466: w=[ 1.80246666 -0.25851552], loss=0.16181989808623887, gradiant=[-0.01334471  0.00358652]\n",
      "Loss at iteration 467: w=[ 1.80259888 -0.25855105], loss=0.16181799743100672, gradiant=[-0.01322188  0.00355348]\n",
      "Loss at iteration 468: w=[ 1.80272988 -0.25858626], loss=0.16181613160285735, gradiant=[-0.01310018  0.0035208 ]\n",
      "Loss at iteration 469: w=[ 1.80285968 -0.25862114], loss=0.16181429996362875, gradiant=[-0.01297961  0.00348837]\n",
      "Loss at iteration 470: w=[ 1.80298828 -0.25865571], loss=0.1618125018868524, gradiant=[-0.01286014  0.00345628]\n",
      "Loss at iteration 471: w=[ 1.8031157  -0.25868995], loss=0.16181073675753951, gradiant=[-0.01274178  0.00342445]\n",
      "Loss at iteration 472: w=[ 1.80324194 -0.25872388], loss=0.16180900397196962, gradiant=[-0.01262449  0.00339295]\n",
      "Loss at iteration 473: w=[ 1.80336703 -0.2587575 ], loss=0.16180730293748505, gradiant=[-0.0125083  0.0033617]\n",
      "Loss at iteration 474: w=[ 1.80349096 -0.25879081], loss=0.1618056330722875, gradiant=[-0.01239316  0.00333078]\n",
      "Loss at iteration 475: w=[ 1.80361375 -0.25882381], loss=0.16180399380523963, gradiant=[-0.0122791  0.0033001]\n",
      "Loss at iteration 476: w=[ 1.80373541 -0.2588565 ], loss=0.16180238457566934, gradiant=[-0.01216608  0.00326974]\n",
      "Loss at iteration 477: w=[ 1.80385595 -0.2588889 ], loss=0.16180080483317807, gradiant=[-0.0120541   0.00323963]\n",
      "Loss at iteration 478: w=[ 1.80397538 -0.258921  ], loss=0.16179925403745304, gradiant=[-0.01194315  0.00320983]\n",
      "Loss at iteration 479: w=[ 1.80409372 -0.2589528 ], loss=0.16179773165808128, gradiant=[-0.01183322  0.00318027]\n",
      "Loss at iteration 480: w=[ 1.80421096 -0.25898431], loss=0.16179623717436964, gradiant=[-0.01172431  0.00315101]\n",
      "Loss at iteration 481: w=[ 1.80432712 -0.25901553], loss=0.16179477007516563, gradiant=[-0.0116164  0.003122 ]\n",
      "Loss at iteration 482: w=[ 1.80444222 -0.25904646], loss=0.1617933298586832, gradiant=[-0.01150947  0.00309327]\n",
      "Loss at iteration 483: w=[ 1.80455625 -0.25907711], loss=0.1617919160323305, gradiant=[-0.01140354  0.00306479]\n",
      "Loss at iteration 484: w=[ 1.80466924 -0.25910748], loss=0.1617905281125424, gradiant=[-0.01129858  0.00303659]\n",
      "Loss at iteration 485: w=[ 1.80478118 -0.25913756], loss=0.161789165624614, gradiant=[-0.01119458  0.00300863]\n",
      "Loss at iteration 486: w=[ 1.8048921  -0.25916737], loss=0.1617878281025387, gradiant=[-0.01109154  0.00298095]\n",
      "Loss at iteration 487: w=[ 1.80500199 -0.25919691], loss=0.1617865150888494, gradiant=[-0.01098946  0.00295351]\n",
      "Loss at iteration 488: w=[ 1.80511088 -0.25922617], loss=0.16178522613446106, gradiant=[-0.01088831  0.00292633]\n",
      "Loss at iteration 489: w=[ 1.80521876 -0.25925517], loss=0.16178396079851765, gradiant=[-0.01078809  0.00289939]\n",
      "Loss at iteration 490: w=[ 1.80532565 -0.25928389], loss=0.1617827186482414, gradiant=[-0.01068879  0.00287271]\n",
      "Loss at iteration 491: w=[ 1.80543155 -0.25931236], loss=0.16178149925878443, gradiant=[-0.01059041  0.00284626]\n",
      "Loss at iteration 492: w=[ 1.80553648 -0.25934056], loss=0.16178030221308382, gradiant=[-0.01049293  0.00282007]\n",
      "Loss at iteration 493: w=[ 1.80564044 -0.2593685 ], loss=0.16177912710171882, gradiant=[-0.01039636  0.00279411]\n",
      "Loss at iteration 494: w=[ 1.80574345 -0.25939618], loss=0.16177797352277037, gradiant=[-0.01030066  0.00276839]\n",
      "Loss at iteration 495: w=[ 1.80584551 -0.25942361], loss=0.16177684108168477, gradiant=[-0.01020586  0.00274291]\n",
      "Loss at iteration 496: w=[ 1.80594663 -0.25945079], loss=0.16177572939113755, gradiant=[-0.01011192  0.00271767]\n",
      "Loss at iteration 497: w=[ 1.80604682 -0.25947771], loss=0.16177463807090145, gradiant=[-0.01001885  0.00269265]\n",
      "Loss at iteration 498: w=[ 1.80614608 -0.25950439], loss=0.16177356674771665, gradiant=[-0.00992663  0.00266787]\n",
      "Loss at iteration 499: w=[ 1.80624443 -0.25953083], loss=0.1617725150551627, gradiant=[-0.00983526  0.00264331]\n",
      "Loss at iteration 500: w=[ 1.80634188 -0.25955702], loss=0.1617714826335331, gradiant=[-0.00974474  0.00261898]\n",
      "Loss at iteration 501: w=[ 1.80643843 -0.25958296], loss=0.16177046912971302, gradiant=[-0.00965505  0.00259487]\n",
      "Loss at iteration 502: w=[ 1.80653409 -0.25960867], loss=0.16176947419705756, gradiant=[-0.00956618  0.00257099]\n",
      "Loss at iteration 503: w=[ 1.80662888 -0.25963415], loss=0.16176849749527383, gradiant=[-0.00947813  0.00254733]\n",
      "Loss at iteration 504: w=[ 1.80672278 -0.25965939], loss=0.16176753869030464, gradiant=[-0.00939089  0.00252388]\n",
      "Loss at iteration 505: w=[ 1.80681583 -0.25968439], loss=0.16176659745421323, gradiant=[-0.00930445  0.00250065]\n",
      "Loss at iteration 506: w=[ 1.80690802 -0.25970917], loss=0.1617656734650726, gradiant=[-0.00921881  0.00247764]\n",
      "Loss at iteration 507: w=[ 1.80699936 -0.25973372], loss=0.16176476640685428, gradiant=[-0.00913396  0.00245483]\n",
      "Loss at iteration 508: w=[ 1.80708986 -0.25975804], loss=0.16176387596932076, gradiant=[-0.00904989  0.00243224]\n",
      "Loss at iteration 509: w=[ 1.80717952 -0.25978214], loss=0.16176300184791925, gradiant=[-0.00896659  0.00240985]\n",
      "Loss at iteration 510: w=[ 1.80726836 -0.25980601], loss=0.16176214374367748, gradiant=[-0.00888406  0.00238767]\n",
      "Loss at iteration 511: w=[ 1.80735639 -0.25982967], loss=0.16176130136310118, gradiant=[-0.00880229  0.00236569]\n",
      "Loss at iteration 512: w=[ 1.8074436  -0.25985311], loss=0.1617604744180745, gradiant=[-0.00872127  0.00234392]\n",
      "Loss at iteration 513: w=[ 1.80753001 -0.25987633], loss=0.1617596626257606, gradiant=[-0.008641    0.00232234]\n",
      "Loss at iteration 514: w=[ 1.80761562 -0.25989934], loss=0.1617588657085052, gradiant=[-0.00856147  0.00230097]\n",
      "Loss at iteration 515: w=[ 1.80770045 -0.25992214], loss=0.16175808339374198, gradiant=[-0.00848267  0.00227979]\n",
      "Loss at iteration 516: w=[ 1.8077845  -0.25994473], loss=0.16175731541389896, gradiant=[-0.00840459  0.00225881]\n",
      "Loss at iteration 517: w=[ 1.80786777 -0.25996711], loss=0.16175656150630696, gradiant=[-0.00832723  0.00223801]\n",
      "Loss at iteration 518: w=[ 1.80795027 -0.25998928], loss=0.16175582141310985, gradiant=[-0.00825059  0.00221742]\n",
      "Loss at iteration 519: w=[ 1.80803202 -0.26001125], loss=0.16175509488117643, gradiant=[-0.00817465  0.002197  ]\n",
      "Loss at iteration 520: w=[ 1.80811301 -0.26003302], loss=0.161754381662014, gradiant=[-0.0080994   0.00217678]\n",
      "Loss at iteration 521: w=[ 1.80819326 -0.26005459], loss=0.16175368151168285, gradiant=[-0.00802486  0.00215675]\n",
      "Loss at iteration 522: w=[ 1.80827277 -0.26007596], loss=0.16175299419071354, gradiant=[-0.00795099  0.0021369 ]\n",
      "Loss at iteration 523: w=[ 1.80835155 -0.26009713], loss=0.16175231946402416, gradiant=[-0.00787781  0.00211723]\n",
      "Loss at iteration 524: w=[ 1.8084296  -0.26011811], loss=0.16175165710084072, gradiant=[-0.0078053   0.00209774]\n",
      "Loss at iteration 525: w=[ 1.80850694 -0.26013889], loss=0.16175100687461774, gradiant=[-0.00773346  0.00207843]\n",
      "Loss at iteration 526: w=[ 1.80858356 -0.26015949], loss=0.1617503685629607, gradiant=[-0.00766228  0.0020593 ]\n",
      "Loss at iteration 527: w=[ 1.80865948 -0.26017989], loss=0.1617497419475507, gradiant=[-0.00759175  0.00204035]\n",
      "Loss at iteration 528: w=[ 1.8087347 -0.2602001], loss=0.16174912681406872, gradiant=[-0.00752188  0.00202157]\n",
      "Loss at iteration 529: w=[ 1.80880922 -0.26022013], loss=0.16174852295212316, gradiant=[-0.00745264  0.00200296]\n",
      "Loss at iteration 530: w=[ 1.80888306 -0.26023998], loss=0.16174793015517738, gradiant=[-0.00738405  0.00198453]\n",
      "Loss at iteration 531: w=[ 1.80895623 -0.26025964], loss=0.16174734822047956, gradiant=[-0.00731608  0.00196626]\n",
      "Loss at iteration 532: w=[ 1.80902871 -0.26027912], loss=0.1617467769489928, gradiant=[-0.00724875  0.00194816]\n",
      "Loss at iteration 533: w=[ 1.80910053 -0.26029843], loss=0.16174621614532725, gradiant=[-0.00718203  0.00193023]\n",
      "Loss at iteration 534: w=[ 1.80917169 -0.26031755], loss=0.1617456656176734, gradiant=[-0.00711592  0.00191246]\n",
      "Loss at iteration 535: w=[ 1.8092422 -0.2603365], loss=0.1617451251777367, gradiant=[-0.00705043  0.00189486]\n",
      "Loss at iteration 536: w=[ 1.80931205 -0.26035527], loss=0.16174459464067242, gradiant=[-0.00698553  0.00187742]\n",
      "Loss at iteration 537: w=[ 1.80938126 -0.26037387], loss=0.16174407382502304, gradiant=[-0.00692124  0.00186014]\n",
      "Loss at iteration 538: w=[ 1.80944984 -0.2603923 ], loss=0.1617435625526561, gradiant=[-0.00685753  0.00184302]\n",
      "Loss at iteration 539: w=[ 1.80951778 -0.26041057], loss=0.16174306064870317, gradiant=[-0.00679441  0.00182606]\n",
      "Loss at iteration 540: w=[ 1.8095851  -0.26042866], loss=0.16174256794149985, gradiant=[-0.00673187  0.00180925]\n",
      "Loss at iteration 541: w=[ 1.8096518  -0.26044658], loss=0.16174208426252767, gradiant=[-0.00666991  0.0017926 ]\n",
      "Loss at iteration 542: w=[ 1.80971789 -0.26046434], loss=0.1617416094463555, gradiant=[-0.00660852  0.0017761 ]\n",
      "Loss at iteration 543: w=[ 1.80978336 -0.26048194], loss=0.16174114333058412, gradiant=[-0.0065477   0.00175975]\n",
      "Loss at iteration 544: w=[ 1.80984824 -0.26049938], loss=0.1617406857557896, gradiant=[-0.00648743  0.00174355]\n",
      "Loss at iteration 545: w=[ 1.80991251 -0.26051665], loss=0.16174023656546963, gradiant=[-0.00642772  0.0017275 ]\n",
      "Loss at iteration 546: w=[ 1.8099762  -0.26053377], loss=0.1617397956059891, gradiant=[-0.00636856  0.0017116 ]\n",
      "Loss at iteration 547: w=[ 1.8100393  -0.26055073], loss=0.16173936272652864, gradiant=[-0.00630994  0.00169585]\n",
      "Loss at iteration 548: w=[ 1.81010182 -0.26056753], loss=0.1617389377790322, gradiant=[-0.00625186  0.00168024]\n",
      "Loss at iteration 549: w=[ 1.81016376 -0.26058418], loss=0.16173852061815647, gradiant=[-0.00619432  0.00166477]\n",
      "Loss at iteration 550: w=[ 1.81022513 -0.26060067], loss=0.16173811110122174, gradiant=[-0.0061373   0.00164945]\n",
      "Loss at iteration 551: w=[ 1.81028594 -0.26061701], loss=0.16173770908816237, gradiant=[-0.00608081  0.00163427]\n",
      "Loss at iteration 552: w=[ 1.81034619 -0.26063321], loss=0.16173731444147965, gradiant=[-0.00602484  0.00161923]\n",
      "Loss at iteration 553: w=[ 1.81040589 -0.26064925], loss=0.1617369270261938, gradiant=[-0.00596939  0.00160432]\n",
      "Loss at iteration 554: w=[ 1.81046503 -0.26066515], loss=0.16173654670979887, gradiant=[-0.00591445  0.00158956]\n",
      "Loss at iteration 555: w=[ 1.81052363 -0.26068089], loss=0.16173617336221693, gradiant=[-0.00586001  0.00157493]\n",
      "Loss at iteration 556: w=[ 1.81058169 -0.2606965 ], loss=0.16173580685575298, gradiant=[-0.00580607  0.00156043]\n",
      "Loss at iteration 557: w=[ 1.81063922 -0.26071196], loss=0.1617354470650527, gradiant=[-0.00575263  0.00154607]\n",
      "Loss at iteration 558: w=[ 1.81069621 -0.26072728], loss=0.1617350938670578, gradiant=[-0.00569968  0.00153184]\n",
      "Loss at iteration 559: w=[ 1.81075269 -0.26074246], loss=0.16173474714096578, gradiant=[-0.00564722  0.00151774]\n",
      "Loss at iteration 560: w=[ 1.81080864 -0.26075749], loss=0.16173440676818698, gradiant=[-0.00559524  0.00150377]\n",
      "Loss at iteration 561: w=[ 1.81086408 -0.26077239], loss=0.16173407263230527, gradiant=[-0.00554374  0.00148993]\n",
      "Loss at iteration 562: w=[ 1.810919   -0.26078715], loss=0.1617337446190374, gradiant=[-0.00549272  0.00147621]\n",
      "Loss at iteration 563: w=[ 1.81097342 -0.26080178], loss=0.16173342261619417, gradiant=[-0.00544216  0.00146263]\n",
      "Loss at iteration 564: w=[ 1.81102735 -0.26081627], loss=0.1617331065136423, gradiant=[-0.00539207  0.00144916]\n",
      "Loss at iteration 565: w=[ 1.81108077 -0.26083063], loss=0.16173279620326653, gradiant=[-0.00534244  0.00143583]\n",
      "Loss at iteration 566: w=[ 1.8111337  -0.26084486], loss=0.16173249157893244, gradiant=[-0.00529327  0.00142261]\n",
      "Loss at iteration 567: w=[ 1.81118615 -0.26085895], loss=0.16173219253645094, gradiant=[-0.00524455  0.00140952]\n",
      "Loss at iteration 568: w=[ 1.81123811 -0.26087292], loss=0.16173189897354132, gradiant=[-0.00519628  0.00139654]\n",
      "Loss at iteration 569: w=[ 1.81128959 -0.26088675], loss=0.16173161078979748, gradiant=[-0.00514845  0.00138369]\n",
      "Loss at iteration 570: w=[ 1.81134061 -0.26090046], loss=0.16173132788665318, gradiant=[-0.00510106  0.00137095]\n",
      "Loss at iteration 571: w=[ 1.81139115 -0.26091405], loss=0.16173105016734812, gradiant=[-0.00505411  0.00135833]\n",
      "Loss at iteration 572: w=[ 1.81144122 -0.26092751], loss=0.16173077753689508, gradiant=[-0.00500759  0.00134583]\n",
      "Loss at iteration 573: w=[ 1.81149084 -0.26094084], loss=0.16173050990204718, gradiant=[-0.0049615   0.00133344]\n",
      "Loss at iteration 574: w=[ 1.81154    -0.26095405], loss=0.1617302471712667, gradiant=[-0.00491583  0.00132117]\n",
      "Loss at iteration 575: w=[ 1.8115887  -0.26096714], loss=0.16172998925469245, gradiant=[-0.00487059  0.00130901]\n",
      "Loss at iteration 576: w=[ 1.81163696 -0.26098011], loss=0.16172973606411062, gradiant=[-0.00482576  0.00129696]\n",
      "Loss at iteration 577: w=[ 1.81168477 -0.26099296], loss=0.16172948751292304, gradiant=[-0.00478134  0.00128503]\n",
      "Loss at iteration 578: w=[ 1.81173215 -0.26100569], loss=0.16172924351611914, gradiant=[-0.00473733  0.0012732 ]\n",
      "Loss at iteration 579: w=[ 1.81177908 -0.26101831], loss=0.16172900399024526, gradiant=[-0.00469373  0.00126148]\n",
      "Loss at iteration 580: w=[ 1.81182559 -0.26103081], loss=0.1617287688533775, gradiant=[-0.00465052  0.00124987]\n",
      "Loss at iteration 581: w=[ 1.81187167 -0.26104319], loss=0.16172853802509293, gradiant=[-0.00460772  0.00123836]\n",
      "Loss at iteration 582: w=[ 1.81191732 -0.26105546], loss=0.16172831142644228, gradiant=[-0.00456531  0.00122697]\n",
      "Loss at iteration 583: w=[ 1.81196255 -0.26106762], loss=0.16172808897992266, gradiant=[-0.00452329  0.00121567]\n",
      "Loss at iteration 584: w=[ 1.81200737 -0.26107966], loss=0.161727870609452, gradiant=[-0.00448166  0.00120448]\n",
      "Loss at iteration 585: w=[ 1.81205177 -0.2610916 ], loss=0.16172765624034147, gradiant=[-0.00444041  0.0011934 ]\n",
      "Loss at iteration 586: w=[ 1.81209577 -0.26110342], loss=0.16172744579927145, gradiant=[-0.00439954  0.00118241]\n",
      "Loss at iteration 587: w=[ 1.81213936 -0.26111514], loss=0.1617272392142658, gradiant=[-0.00435904  0.00117153]\n",
      "Loss at iteration 588: w=[ 1.81218255 -0.26112674], loss=0.16172703641466676, gradiant=[-0.00431892  0.00116075]\n",
      "Loss at iteration 589: w=[ 1.81222534 -0.26113824], loss=0.16172683733111165, gradiant=[-0.00427917  0.00115006]\n",
      "Loss at iteration 590: w=[ 1.81226774 -0.26114964], loss=0.16172664189550867, gradiant=[-0.00423978  0.00113948]\n",
      "Loss at iteration 591: w=[ 1.81230974 -0.26116093], loss=0.16172645004101388, gradiant=[-0.00420076  0.00112899]\n",
      "Loss at iteration 592: w=[ 1.81235137 -0.26117211], loss=0.16172626170200796, gradiant=[-0.00416209  0.0011186 ]\n",
      "Loss at iteration 593: w=[ 1.8123926 -0.2611832], loss=0.16172607681407408, gradiant=[-0.00412378  0.0011083 ]\n",
      "Loss at iteration 594: w=[ 1.81243346 -0.26119418], loss=0.16172589531397577, gradiant=[-0.00408583  0.0010981 ]\n",
      "Loss at iteration 595: w=[ 1.81247394 -0.26120506], loss=0.16172571713963524, gradiant=[-0.00404822  0.00108799]\n",
      "Loss at iteration 596: w=[ 1.81251405 -0.26121584], loss=0.1617255422301122, gradiant=[-0.00401096  0.00107798]\n",
      "Loss at iteration 597: w=[ 1.81255379 -0.26122652], loss=0.16172537052558306, gradiant=[-0.00397404  0.00106806]\n",
      "Loss at iteration 598: w=[ 1.81259317 -0.2612371 ], loss=0.16172520196732038, gradiant=[-0.00393746  0.00105823]\n",
      "Loss at iteration 599: w=[ 1.81263218 -0.26124759], loss=0.1617250364976729, gradiant=[-0.00390122  0.00104849]\n",
      "Loss at iteration 600: w=[ 1.81267083 -0.26125797], loss=0.16172487406004565, gradiant=[-0.00386532  0.00103884]\n",
      "Loss at iteration 601: w=[ 1.81270913 -0.26126827], loss=0.1617247145988806, gradiant=[-0.00382974  0.00102927]\n",
      "Loss at iteration 602: w=[ 1.81274708 -0.26127846], loss=0.16172455805963798, gradiant=[-0.00379449  0.0010198 ]\n",
      "Loss at iteration 603: w=[ 1.81278467 -0.26128857], loss=0.16172440438877742, gradiant=[-0.00375956  0.00101041]\n",
      "Loss at iteration 604: w=[ 1.81282192 -0.26129858], loss=0.16172425353373937, gradiant=[-0.00372496  0.00100111]\n",
      "Loss at iteration 605: w=[ 1.81285883 -0.2613085 ], loss=0.16172410544292729, gradiant=[-0.00369067  0.0009919 ]\n",
      "Loss at iteration 606: w=[ 1.81289539 -0.26131833], loss=0.16172396006569062, gradiant=[-0.0036567   0.00098277]\n",
      "Loss at iteration 607: w=[ 1.81293163 -0.26132806], loss=0.16172381735230623, gradiant=[-0.00362305  0.00097372]\n",
      "Loss at iteration 608: w=[ 1.81296752 -0.26133771], loss=0.16172367725396264, gradiant=[-0.0035897   0.00096476]\n",
      "Loss at iteration 609: w=[ 1.81300309 -0.26134727], loss=0.16172353972274242, gradiant=[-0.00355666  0.00095588]\n",
      "Loss at iteration 610: w=[ 1.81303833 -0.26135674], loss=0.16172340471160646, gradiant=[-0.00352392  0.00094708]\n",
      "Loss at iteration 611: w=[ 1.81307324 -0.26136613], loss=0.16172327217437732, gradiant=[-0.00349149  0.00093837]\n",
      "Loss at iteration 612: w=[ 1.81310784 -0.26137542], loss=0.16172314206572397, gradiant=[-0.00345935  0.00092973]\n",
      "Loss at iteration 613: w=[ 1.81314211 -0.26138463], loss=0.1617230143411455, gradiant=[-0.00342751  0.00092117]\n",
      "Loss at iteration 614: w=[ 1.81317607 -0.26139376], loss=0.16172288895695716, gradiant=[-0.00339596  0.00091269]\n",
      "Loss at iteration 615: w=[ 1.81320972 -0.2614028 ], loss=0.1617227658702739, gradiant=[-0.00336471  0.00090429]\n",
      "Loss at iteration 616: w=[ 1.81324306 -0.26141176], loss=0.16172264503899736, gradiant=[-0.00333374  0.00089597]\n",
      "Loss at iteration 617: w=[ 1.81327609 -0.26142064], loss=0.1617225264217997, gradiant=[-0.00330305  0.00088772]\n",
      "Loss at iteration 618: w=[ 1.81330881 -0.26142944], loss=0.16172240997811094, gradiant=[-0.00327265  0.00087955]\n",
      "Loss at iteration 619: w=[ 1.81334124 -0.26143815], loss=0.1617222956681042, gradiant=[-0.00324253  0.00087146]\n",
      "Loss at iteration 620: w=[ 1.81337336 -0.26144679], loss=0.16172218345268252, gradiant=[-0.00321268  0.00086344]\n",
      "Loss at iteration 621: w=[ 1.8134052  -0.26145534], loss=0.16172207329346538, gradiant=[-0.00318311  0.00085549]\n",
      "Loss at iteration 622: w=[ 1.81343673 -0.26146382], loss=0.1617219651527752, gradiant=[-0.00315381  0.00084761]\n",
      "Loss at iteration 623: w=[ 1.81346798 -0.26147221], loss=0.1617218589936254, gradiant=[-0.00312479  0.00083981]\n",
      "Loss at iteration 624: w=[ 1.81349894 -0.26148054], loss=0.16172175477970657, gradiant=[-0.00309602  0.00083208]\n",
      "Loss at iteration 625: w=[ 1.81352962 -0.26148878], loss=0.16172165247537487, gradiant=[-0.00306753  0.00082442]\n",
      "Loss at iteration 626: w=[ 1.81356001 -0.26149695], loss=0.16172155204563943, gradiant=[-0.00303929  0.00081684]\n",
      "Loss at iteration 627: w=[ 1.81359012 -0.26150504], loss=0.16172145345615094, gradiant=[-0.00301132  0.00080932]\n",
      "Loss at iteration 628: w=[ 1.81361996 -0.26151306], loss=0.161721356673189, gradiant=[-0.0029836   0.00080187]\n",
      "Loss at iteration 629: w=[ 1.81364952 -0.261521  ], loss=0.16172126166365128, gradiant=[-0.00295614  0.00079449]\n",
      "Loss at iteration 630: w=[ 1.81367881 -0.26152888], loss=0.16172116839504225, gradiant=[-0.00292893  0.00078718]\n",
      "Loss at iteration 631: w=[ 1.81370783 -0.26153668], loss=0.16172107683546155, gradiant=[-0.00290197  0.00077993]\n",
      "Loss at iteration 632: w=[ 1.81373658 -0.2615444 ], loss=0.16172098695359324, gradiant=[-0.00287526  0.00077275]\n",
      "Loss at iteration 633: w=[ 1.81376507 -0.26155206], loss=0.16172089871869555, gradiant=[-0.0028488   0.00076564]\n",
      "Loss at iteration 634: w=[ 1.8137933  -0.26155965], loss=0.16172081210058986, gradiant=[-0.00282258  0.00075859]\n",
      "Loss at iteration 635: w=[ 1.81382126 -0.26156716], loss=0.1617207270696505, gradiant=[-0.0027966   0.00075161]\n",
      "Loss at iteration 636: w=[ 1.81384897 -0.26157461], loss=0.1617206435967946, gradiant=[-0.00277086  0.00074469]\n",
      "Loss at iteration 637: w=[ 1.81387642 -0.26158199], loss=0.16172056165347234, gradiant=[-0.00274535  0.00073784]\n",
      "Loss at iteration 638: w=[ 1.81390363 -0.2615893 ], loss=0.16172048121165683, gradiant=[-0.00272009  0.00073105]\n",
      "Loss at iteration 639: w=[ 1.81393058 -0.26159654], loss=0.16172040224383502, gradiant=[-0.00269505  0.00072432]\n",
      "Loss at iteration 640: w=[ 1.81395728 -0.26160372], loss=0.16172032472299797, gradiant=[-0.00267024  0.00071765]\n",
      "Loss at iteration 641: w=[ 1.81398374 -0.26161083], loss=0.16172024862263118, gradiant=[-0.00264567  0.00071105]\n",
      "Loss at iteration 642: w=[ 1.81400995 -0.26161787], loss=0.16172017391670657, gradiant=[-0.00262131  0.0007045 ]\n",
      "Loss at iteration 643: w=[ 1.81403592 -0.26162485], loss=0.1617201005796728, gradiant=[-0.00259719  0.00069802]\n",
      "Loss at iteration 644: w=[ 1.81406165 -0.26163177], loss=0.16172002858644663, gradiant=[-0.00257328  0.00069159]\n",
      "Loss at iteration 645: w=[ 1.81408715 -0.26163862], loss=0.16171995791240434, gradiant=[-0.0025496   0.00068523]\n",
      "Loss at iteration 646: w=[ 1.81411241 -0.26164541], loss=0.1617198885333738, gradiant=[-0.00252613  0.00067892]\n",
      "Loss at iteration 647: w=[ 1.81413744 -0.26165214], loss=0.1617198204256255, gradiant=[-0.00250288  0.00067267]\n",
      "Loss at iteration 648: w=[ 1.81416224 -0.2616588 ], loss=0.16171975356586493, gradiant=[-0.00247984  0.00066648]\n",
      "Loss at iteration 649: w=[ 1.81418681 -0.2616654 ], loss=0.16171968793122404, gradiant=[-0.00245702  0.00066034]\n",
      "Loss at iteration 650: w=[ 1.81421115 -0.26167195], loss=0.16171962349925423, gradiant=[-0.0024344   0.00065427]\n",
      "Loss at iteration 651: w=[ 1.81423527 -0.26167843], loss=0.16171956024791811, gradiant=[-0.002412    0.00064824]\n",
      "Loss at iteration 652: w=[ 1.81425917 -0.26168485], loss=0.16171949815558206, gradiant=[-0.00238979  0.00064228]\n",
      "Loss at iteration 653: w=[ 1.81428285 -0.26169122], loss=0.16171943720100884, gradiant=[-0.0023678   0.00063637]\n",
      "Loss at iteration 654: w=[ 1.81430631 -0.26169752], loss=0.16171937736335049, gradiant=[-0.002346    0.00063051]\n",
      "Loss at iteration 655: w=[ 1.81432955 -0.26170377], loss=0.16171931862214076, gradiant=[-0.00232441  0.00062471]\n",
      "Loss at iteration 656: w=[ 1.81435258 -0.26170996], loss=0.16171926095728886, gradiant=[-0.00230302  0.00061896]\n",
      "Loss at iteration 657: w=[ 1.8143754  -0.26171609], loss=0.16171920434907178, gradiant=[-0.00228182  0.00061326]\n",
      "Loss at iteration 658: w=[ 1.81439801 -0.26172217], loss=0.16171914877812815, gradiant=[-0.00226082  0.00060761]\n",
      "Loss at iteration 659: w=[ 1.81442041 -0.26172819], loss=0.16171909422545114, gradiant=[-0.00224001  0.00060202]\n",
      "Loss at iteration 660: w=[ 1.8144426  -0.26173415], loss=0.16171904067238224, gradiant=[-0.00221939  0.00059648]\n",
      "Loss at iteration 661: w=[ 1.81446459 -0.26174006], loss=0.16171898810060525, gradiant=[-0.00219896  0.00059099]\n",
      "Loss at iteration 662: w=[ 1.81448638 -0.26174592], loss=0.16171893649213875, gradiant=[-0.00217872  0.00058555]\n",
      "Loss at iteration 663: w=[ 1.81450797 -0.26175172], loss=0.1617188858293317, gradiant=[-0.00215867  0.00058016]\n",
      "Loss at iteration 664: w=[ 1.81452935 -0.26175747], loss=0.16171883609485588, gradiant=[-0.0021388   0.00057482]\n",
      "Loss at iteration 665: w=[ 1.81455054 -0.26176316], loss=0.16171878727170103, gradiant=[-0.00211911  0.00056953]\n",
      "Loss at iteration 666: w=[ 1.81457154 -0.26176881], loss=0.16171873934316805, gradiant=[-0.00209961  0.00056429]\n",
      "Loss at iteration 667: w=[ 1.81459234 -0.2617744 ], loss=0.16171869229286448, gradiant=[-0.00208028  0.00055909]\n",
      "Loss at iteration 668: w=[ 1.81461296 -0.26177994], loss=0.16171864610469755, gradiant=[-0.00206114  0.00055395]\n",
      "Loss at iteration 669: w=[ 1.81463338 -0.26178542], loss=0.16171860076286998, gradiant=[-0.00204217  0.00054885]\n",
      "Loss at iteration 670: w=[ 1.81465361 -0.26179086], loss=0.16171855625187337, gradiant=[-0.00202337  0.0005438 ]\n",
      "Loss at iteration 671: w=[ 1.81467366 -0.26179625], loss=0.16171851255648428, gradiant=[-0.00200475  0.00053879]\n",
      "Loss at iteration 672: w=[ 1.81469352 -0.26180159], loss=0.1617184696617572, gradiant=[-0.00198629  0.00053383]\n",
      "Loss at iteration 673: w=[ 1.8147132  -0.26180688], loss=0.1617184275530215, gradiant=[-0.00196801  0.00052892]\n",
      "Loss at iteration 674: w=[ 1.8147327  -0.26181212], loss=0.16171838621587442, gradiant=[-0.0019499   0.00052405]\n",
      "Loss at iteration 675: w=[ 1.81475202 -0.26181731], loss=0.16171834563617798, gradiant=[-0.00193195  0.00051923]\n",
      "Loss at iteration 676: w=[ 1.81477116 -0.26182245], loss=0.16171830580005264, gradiant=[-0.00191417  0.00051445]\n",
      "Loss at iteration 677: w=[ 1.81479013 -0.26182755], loss=0.16171826669387368, gradiant=[-0.00189655  0.00050971]\n",
      "Loss at iteration 678: w=[ 1.81480892 -0.2618326 ], loss=0.1617182283042654, gradiant=[-0.00187909  0.00050502]\n",
      "Loss at iteration 679: w=[ 1.81482754 -0.26183761], loss=0.16171819061809767, gradiant=[-0.0018618   0.00050037]\n",
      "Loss at iteration 680: w=[ 1.81484598 -0.26184256], loss=0.16171815362248093, gradiant=[-0.00184466  0.00049577]\n",
      "Loss at iteration 681: w=[ 1.81486426 -0.26184748], loss=0.1617181173047617, gradiant=[-0.00182768  0.00049121]\n",
      "Loss at iteration 682: w=[ 1.81488237 -0.26185234], loss=0.1617180816525184, gradiant=[-0.00181086  0.00048668]\n",
      "Loss at iteration 683: w=[ 1.81490031 -0.26185716], loss=0.16171804665355693, gradiant=[-0.00179419  0.0004822 ]\n",
      "Loss at iteration 684: w=[ 1.81491809 -0.26186194], loss=0.1617180122959067, gradiant=[-0.00177768  0.00047777]\n",
      "Loss at iteration 685: w=[ 1.8149357  -0.26186668], loss=0.1617179785678167, gradiant=[-0.00176132  0.00047337]\n",
      "Loss at iteration 686: w=[ 1.81495315 -0.26187137], loss=0.161717945457751, gradiant=[-0.00174511  0.00046901]\n",
      "Loss at iteration 687: w=[ 1.81497044 -0.26187601], loss=0.16171791295438498, gradiant=[-0.00172904  0.00046469]\n",
      "Loss at iteration 688: w=[ 1.81498757 -0.26188062], loss=0.16171788104660145, gradiant=[-0.00171313  0.00046042]\n",
      "Loss at iteration 689: w=[ 1.81500455 -0.26188518], loss=0.1617178497234876, gradiant=[-0.00169736  0.00045618]\n",
      "Loss at iteration 690: w=[ 1.81502136 -0.2618897 ], loss=0.16171781897432974, gradiant=[-0.00168174  0.00045198]\n",
      "Loss at iteration 691: w=[ 1.81503803 -0.26189418], loss=0.16171778878861098, gradiant=[-0.00166626  0.00044782]\n",
      "Loss at iteration 692: w=[ 1.81505454 -0.26189861], loss=0.1617177591560071, gradiant=[-0.00165092  0.0004437 ]\n",
      "Loss at iteration 693: w=[ 1.81507089 -0.26190301], loss=0.16171773006638274, gradiant=[-0.00163573  0.00043962]\n",
      "Loss at iteration 694: w=[ 1.8150871  -0.26190737], loss=0.16171770150978876, gradiant=[-0.00162067  0.00043557]\n",
      "Loss at iteration 695: w=[ 1.81510316 -0.26191168], loss=0.16171767347645793, gradiant=[-0.00160575  0.00043156]\n",
      "Loss at iteration 696: w=[ 1.81511907 -0.26191596], loss=0.16171764595680216, gradiant=[-0.00159097  0.00042759]\n",
      "Loss at iteration 697: w=[ 1.81513483 -0.26192019], loss=0.1617176189414089, gradiant=[-0.00157633  0.00042365]\n",
      "Loss at iteration 698: w=[ 1.81515045 -0.26192439], loss=0.16171759242103834, gradiant=[-0.00156182  0.00041975]\n",
      "Loss at iteration 699: w=[ 1.81516592 -0.26192855], loss=0.1617175663866196, gradiant=[-0.00154745  0.00041589]\n",
      "Loss at iteration 700: w=[ 1.81518125 -0.26193267], loss=0.16171754082924844, gradiant=[-0.0015332   0.00041206]\n",
      "Loss at iteration 701: w=[ 1.81519645 -0.26193675], loss=0.16171751574018348, gradiant=[-0.00151909  0.00040827]\n",
      "Loss at iteration 702: w=[ 1.8152115 -0.2619408], loss=0.16171749111084377, gradiant=[-0.00150511  0.00040451]\n",
      "Loss at iteration 703: w=[ 1.81522641 -0.26194481], loss=0.16171746693280512, gradiant=[-0.00149126  0.00040079]\n",
      "Loss at iteration 704: w=[ 1.81524118 -0.26194878], loss=0.16171744319779827, gradiant=[-0.00147753  0.0003971 ]\n",
      "Loss at iteration 705: w=[ 1.81525582 -0.26195271], loss=0.1617174198977051, gradiant=[-0.00146393  0.00039344]\n",
      "Loss at iteration 706: w=[ 1.81527033 -0.26195661], loss=0.16171739702455662, gradiant=[-0.00145046  0.00038982]\n",
      "Loss at iteration 707: w=[ 1.8152847  -0.26196047], loss=0.1617173745705292, gradiant=[-0.00143711  0.00038623]\n",
      "Loss at iteration 708: w=[ 1.81529894 -0.2619643 ], loss=0.16171735252794323, gradiant=[-0.00142388  0.00038268]\n",
      "Loss at iteration 709: w=[ 1.81531305 -0.26196809], loss=0.16171733088925952, gradiant=[-0.00141077  0.00037916]\n",
      "Loss at iteration 710: w=[ 1.81532702 -0.26197185], loss=0.16171730964707703, gradiant=[-0.00139779  0.00037567]\n",
      "Loss at iteration 711: w=[ 1.81534087 -0.26197557], loss=0.16171728879413055, gradiant=[-0.00138492  0.00037221]\n",
      "Loss at iteration 712: w=[ 1.81535459 -0.26197926], loss=0.16171726832328764, gradiant=[-0.00137217  0.00036878]\n",
      "Loss at iteration 713: w=[ 1.81536819 -0.26198291], loss=0.16171724822754685, gradiant=[-0.00135955  0.00036539]\n",
      "Loss at iteration 714: w=[ 1.81538166 -0.26198653], loss=0.16171722850003492, gradiant=[-0.00134703  0.00036203]\n",
      "Loss at iteration 715: w=[ 1.81539501 -0.26199012], loss=0.16171720913400434, gradiant=[-0.00133463  0.00035869]\n",
      "Loss at iteration 716: w=[ 1.81540823 -0.26199367], loss=0.16171719012283176, gradiant=[-0.00132235  0.00035539]\n",
      "Loss at iteration 717: w=[ 1.81542133 -0.26199719], loss=0.16171717146001457, gradiant=[-0.00131018  0.00035212]\n",
      "Loss at iteration 718: w=[ 1.81543431 -0.26200068], loss=0.16171715313916957, gradiant=[-0.00129812  0.00034888]\n",
      "Loss at iteration 719: w=[ 1.81544717 -0.26200414], loss=0.1617171351540308, gradiant=[-0.00128617  0.00034567]\n",
      "Loss at iteration 720: w=[ 1.81545992 -0.26200756], loss=0.16171711749844683, gradiant=[-0.00127433  0.00034249]\n",
      "Loss at iteration 721: w=[ 1.81547254 -0.26201096], loss=0.1617171001663789, gradiant=[-0.0012626   0.00033934]\n",
      "Loss at iteration 722: w=[ 1.81548505 -0.26201432], loss=0.16171708315189895, gradiant=[-0.00125098  0.00033621]\n",
      "Loss at iteration 723: w=[ 1.81549745 -0.26201765], loss=0.16171706644918765, gradiant=[-0.00123947  0.00033312]\n",
      "Loss at iteration 724: w=[ 1.81550973 -0.26202095], loss=0.16171705005253215, gradiant=[-0.00122806  0.00033005]\n",
      "Loss at iteration 725: w=[ 1.8155219  -0.26202422], loss=0.1617170339563246, gradiant=[-0.00121676  0.00032701]\n",
      "Loss at iteration 726: w=[ 1.81553395 -0.26202746], loss=0.1617170181550594, gradiant=[-0.00120556  0.000324  ]\n",
      "Loss at iteration 727: w=[ 1.8155459  -0.26203067], loss=0.16171700264333236, gradiant=[-0.00119446  0.00032102]\n",
      "Loss at iteration 728: w=[ 1.81555773 -0.26203385], loss=0.16171698741583773, gradiant=[-0.00118347  0.00031807]\n",
      "Loss at iteration 729: w=[ 1.81556946 -0.262037  ], loss=0.1617169724673677, gradiant=[-0.00117257  0.00031514]\n",
      "Loss at iteration 730: w=[ 1.81558108 -0.26204013], loss=0.16171695779280915, gradiant=[-0.00116178  0.00031224]\n",
      "Loss at iteration 731: w=[ 1.81559259 -0.26204322], loss=0.16171694338714335, gradiant=[-0.00115109  0.00030936]\n",
      "Loss at iteration 732: w=[ 1.81560399 -0.26204628], loss=0.16171692924544284, gradiant=[-0.00114049  0.00030652]\n",
      "Loss at iteration 733: w=[ 1.81561529 -0.26204932], loss=0.161716915362871, gradiant=[-0.00113    0.0003037]\n",
      "Loss at iteration 734: w=[ 1.81562649 -0.26205233], loss=0.16171690173467962, gradiant=[-0.00111959  0.0003009 ]\n",
      "Loss at iteration 735: w=[ 1.81563758 -0.26205531], loss=0.16171688835620754, gradiant=[-0.00110929  0.00029813]\n",
      "Loss at iteration 736: w=[ 1.81564857 -0.26205827], loss=0.16171687522287864, gradiant=[-0.00109908  0.00029539]\n",
      "Loss at iteration 737: w=[ 1.81565946 -0.26206119], loss=0.16171686233020152, gradiant=[-0.00108896  0.00029267]\n",
      "Loss at iteration 738: w=[ 1.81567025 -0.26206409], loss=0.16171684967376618, gradiant=[-0.00107894  0.00028997]\n",
      "Loss at iteration 739: w=[ 1.81568094 -0.26206697], loss=0.16171683724924374, gradiant=[-0.00106901  0.00028731]\n",
      "Loss at iteration 740: w=[ 1.81569153 -0.26206981], loss=0.161716825052385, gradiant=[-0.00105917  0.00028466]\n",
      "Loss at iteration 741: w=[ 1.81570203 -0.26207263], loss=0.16171681307901803, gradiant=[-0.00104942  0.00028204]\n",
      "Loss at iteration 742: w=[ 1.81571242 -0.26207543], loss=0.16171680132504765, gradiant=[-0.00103976  0.00027944]\n",
      "Loss at iteration 743: w=[ 1.81572273 -0.2620782 ], loss=0.16171678978645407, gradiant=[-0.00103019  0.00027687]\n",
      "Loss at iteration 744: w=[ 1.81573293 -0.26208094], loss=0.16171677845929025, gradiant=[-0.00102071  0.00027432]\n",
      "Loss at iteration 745: w=[ 1.81574305 -0.26208366], loss=0.16171676733968227, gradiant=[-0.00101131  0.0002718 ]\n",
      "Loss at iteration 746: w=[ 1.81575307 -0.26208635], loss=0.1617167564238271, gradiant=[-0.00100201  0.0002693 ]\n",
      "Loss at iteration 747: w=[ 1.81576299 -0.26208902], loss=0.16171674570799097, gradiant=[-0.00099278  0.00026682]\n",
      "Loss at iteration 748: w=[ 1.81577283 -0.26209166], loss=0.1617167351885087, gradiant=[-0.00098365  0.00026436]\n",
      "Loss at iteration 749: w=[ 1.81578258 -0.26209428], loss=0.16171672486178265, gradiant=[-0.00097459  0.00026193]\n",
      "Loss at iteration 750: w=[ 1.81579223 -0.26209688], loss=0.16171671472428079, gradiant=[-0.00096562  0.00025952]\n",
      "Loss at iteration 751: w=[ 1.8158018  -0.26209945], loss=0.16171670477253552, gradiant=[-0.00095673  0.00025713]\n",
      "Loss at iteration 752: w=[ 1.81581128 -0.262102  ], loss=0.16171669500314348, gradiant=[-0.00094793  0.00025476]\n",
      "Loss at iteration 753: w=[ 1.81582067 -0.26210452], loss=0.1617166854127629, gradiant=[-0.0009392   0.00025242]\n",
      "Loss at iteration 754: w=[ 1.81582998 -0.26210702], loss=0.1617166759981139, gradiant=[-0.00093056  0.0002501 ]\n",
      "Loss at iteration 755: w=[ 1.8158392 -0.2621095], loss=0.1617166667559763, gradiant=[-0.00092199  0.00024779]\n",
      "Loss at iteration 756: w=[ 1.81584833 -0.26211195], loss=0.1617166576831892, gradiant=[-0.00091351  0.00024551]\n",
      "Loss at iteration 757: w=[ 1.81585738 -0.26211439], loss=0.16171664877664935, gradiant=[-0.0009051   0.00024325]\n",
      "Loss at iteration 758: w=[ 1.81586635 -0.2621168 ], loss=0.16171664003331043, gradiant=[-0.00089677  0.00024101]\n",
      "Loss at iteration 759: w=[ 1.81587523 -0.26211918], loss=0.1617166314501819, gradiant=[-0.00088851  0.0002388 ]\n",
      "Loss at iteration 760: w=[ 1.81588404 -0.26212155], loss=0.16171662302432843, gradiant=[-0.00088034  0.0002366 ]\n",
      "Loss at iteration 761: w=[ 1.81589276 -0.26212389], loss=0.16171661475286792, gradiant=[-0.00087223  0.00023442]\n",
      "Loss at iteration 762: w=[ 1.8159014  -0.26212622], loss=0.1617166066329713, gradiant=[-0.00086421  0.00023226]\n",
      "Loss at iteration 763: w=[ 1.81590997 -0.26212852], loss=0.1617165986618615, gradiant=[-0.00085625  0.00023012]\n",
      "Loss at iteration 764: w=[ 1.81591845 -0.2621308 ], loss=0.16171659083681217, gradiant=[-0.00084837  0.00022801]\n",
      "Loss at iteration 765: w=[ 1.81592685 -0.26213306], loss=0.16171658315514686, gradiant=[-0.00084056  0.00022591]\n",
      "Loss at iteration 766: w=[ 1.81593518 -0.2621353 ], loss=0.16171657561423824, gradiant=[-0.00083282  0.00022383]\n",
      "Loss at iteration 767: w=[ 1.81594343 -0.26213751], loss=0.161716568211507, gradiant=[-0.00082516  0.00022177]\n",
      "Loss at iteration 768: w=[ 1.81595161 -0.26213971], loss=0.16171656094442166, gradiant=[-0.00081756  0.00021973]\n",
      "Loss at iteration 769: w=[ 1.81595971 -0.26214189], loss=0.16171655381049618, gradiant=[-0.00081004  0.0002177 ]\n",
      "Loss at iteration 770: w=[ 1.81596774 -0.26214404], loss=0.16171654680729092, gradiant=[-0.00080258  0.0002157 ]\n",
      "Loss at iteration 771: w=[ 1.81597569 -0.26214618], loss=0.16171653993241034, gradiant=[-0.0007952   0.00021372]\n",
      "Loss at iteration 772: w=[ 1.81598357 -0.2621483 ], loss=0.16171653318350326, gradiant=[-0.00078788  0.00021175]\n",
      "Loss at iteration 773: w=[ 1.81599137 -0.2621504 ], loss=0.1617165265582613, gradiant=[-0.00078063  0.0002098 ]\n",
      "Loss at iteration 774: w=[ 1.81599911 -0.26215248], loss=0.16171652005441844, gradiant=[-0.00077344  0.00020787]\n",
      "Loss at iteration 775: w=[ 1.81600677 -0.26215454], loss=0.1617165136697502, gradiant=[-0.00076632  0.00020596]\n",
      "Loss at iteration 776: w=[ 1.81601436 -0.26215658], loss=0.16171650740207272, gradiant=[-0.00075927  0.00020406]\n",
      "Loss at iteration 777: w=[ 1.81602189 -0.2621586 ], loss=0.16171650124924264, gradiant=[-0.00075228  0.00020218]\n",
      "Loss at iteration 778: w=[ 1.81602934 -0.2621606 ], loss=0.1617164952091552, gradiant=[-0.00074536  0.00020032]\n",
      "Loss at iteration 779: w=[ 1.81603672 -0.26216259], loss=0.16171648927974464, gradiant=[-0.00073849  0.00019848]\n",
      "Loss at iteration 780: w=[ 1.81604404 -0.26216455], loss=0.1617164834589831, gradiant=[-0.0007317   0.00019665]\n",
      "Loss at iteration 781: w=[ 1.81605129 -0.2621665 ], loss=0.16171647774487952, gradiant=[-0.00072496  0.00019484]\n",
      "Loss at iteration 782: w=[ 1.81605847 -0.26216843], loss=0.16171647213547952, gradiant=[-0.00071829  0.00019305]\n",
      "Loss at iteration 783: w=[ 1.81606559 -0.26217034], loss=0.1617164666288647, gradiant=[-0.00071168  0.00019127]\n",
      "Loss at iteration 784: w=[ 1.81607264 -0.26217224], loss=0.16171646122315161, gradiant=[-0.00070513  0.00018951]\n",
      "Loss at iteration 785: w=[ 1.81607963 -0.26217412], loss=0.1617164559164912, gradiant=[-0.00069864  0.00018776]\n",
      "Loss at iteration 786: w=[ 1.81608655 -0.26217598], loss=0.16171645070706864, gradiant=[-0.00069221  0.00018604]\n",
      "Loss at iteration 787: w=[ 1.81609341 -0.26217782], loss=0.16171644559310216, gradiant=[-0.00068584  0.00018432]\n",
      "Loss at iteration 788: w=[ 1.8161002  -0.26217965], loss=0.16171644057284254, gradiant=[-0.00067952  0.00018263]\n",
      "Loss at iteration 789: w=[ 1.81610694 -0.26218146], loss=0.16171643564457266, gradiant=[-0.00067327  0.00018095]\n",
      "Loss at iteration 790: w=[ 1.81611361 -0.26218325], loss=0.16171643080660722, gradiant=[-0.00066707  0.00017928]\n",
      "Loss at iteration 791: w=[ 1.81612022 -0.26218503], loss=0.1617164260572912, gradiant=[-0.00066093  0.00017763]\n",
      "Loss at iteration 792: w=[ 1.81612677 -0.26218679], loss=0.16171642139500048, gradiant=[-0.00065485  0.000176  ]\n",
      "Loss at iteration 793: w=[ 1.81613325 -0.26218853], loss=0.16171641681814017, gradiant=[-0.00064882  0.00017438]\n",
      "Loss at iteration 794: w=[ 1.81613968 -0.26219026], loss=0.161716412325145, gradiant=[-0.00064285  0.00017277]\n",
      "Loss at iteration 795: w=[ 1.81614605 -0.26219197], loss=0.16171640791447822, gradiant=[-0.00063693  0.00017118]\n",
      "Loss at iteration 796: w=[ 1.81615236 -0.26219366], loss=0.16171640358463132, gradiant=[-0.00063107  0.00016961]\n",
      "Loss at iteration 797: w=[ 1.81615861 -0.26219534], loss=0.1617163993341233, gradiant=[-0.00062526  0.00016804]\n",
      "Loss at iteration 798: w=[ 1.81616481 -0.26219701], loss=0.16171639516150052, gradiant=[-0.00061951  0.0001665 ]\n",
      "Loss at iteration 799: w=[ 1.81617095 -0.26219866], loss=0.16171639106533575, gradiant=[-0.0006138   0.00016497]\n",
      "Loss at iteration 800: w=[ 1.81617703 -0.26220029], loss=0.16171638704422778, gradiant=[-0.00060816  0.00016345]\n",
      "Loss at iteration 801: w=[ 1.81618306 -0.26220191], loss=0.1617163830968017, gradiant=[-0.00060256  0.00016194]\n",
      "Loss at iteration 802: w=[ 1.81618903 -0.26220352], loss=0.16171637922170695, gradiant=[-0.00059701  0.00016045]\n",
      "Loss at iteration 803: w=[ 1.81619494 -0.26220511], loss=0.16171637541761846, gradiant=[-0.00059152  0.00015898]\n",
      "Loss at iteration 804: w=[ 1.8162008  -0.26220668], loss=0.16171637168323516, gradiant=[-0.00058607  0.00015751]\n",
      "Loss at iteration 805: w=[ 1.81620661 -0.26220824], loss=0.16171636801727957, gradiant=[-0.00058068  0.00015606]\n",
      "Loss at iteration 806: w=[ 1.81621236 -0.26220979], loss=0.161716364418498, gradiant=[-0.00057533  0.00015463]\n",
      "Loss at iteration 807: w=[ 1.81621806 -0.26221132], loss=0.16171636088565944, gradiant=[-0.00057004  0.0001532 ]\n",
      "Loss at iteration 808: w=[ 1.81622371 -0.26221284], loss=0.1617163574175556, gradiant=[-0.00056479  0.00015179]\n",
      "Loss at iteration 809: w=[ 1.81622931 -0.26221434], loss=0.16171635401300039, gradiant=[-0.00055959  0.0001504 ]\n",
      "Loss at iteration 810: w=[ 1.81623485 -0.26221583], loss=0.16171635067082932, gradiant=[-0.00055444  0.00014901]\n",
      "Loss at iteration 811: w=[ 1.81624034 -0.26221731], loss=0.1617163473898992, gradiant=[-0.00054934  0.00014764]\n",
      "Loss at iteration 812: w=[ 1.81624579 -0.26221877], loss=0.16171634416908795, gradiant=[-0.00054428  0.00014628]\n",
      "Loss at iteration 813: w=[ 1.81625118 -0.26222022], loss=0.16171634100729404, gradiant=[-0.00053927  0.00014493]\n",
      "Loss at iteration 814: w=[ 1.81625652 -0.26222166], loss=0.16171633790343595, gradiant=[-0.00053431  0.0001436 ]\n",
      "Loss at iteration 815: w=[ 1.81626182 -0.26222308], loss=0.16171633485645215, gradiant=[-0.00052939  0.00014228]\n",
      "Loss at iteration 816: w=[ 1.81626706 -0.26222449], loss=0.16171633186530032, gradiant=[-0.00052452  0.00014097]\n",
      "Loss at iteration 817: w=[ 1.81627226 -0.26222589], loss=0.1617163289289577, gradiant=[-0.00051969  0.00013967]\n",
      "Loss at iteration 818: w=[ 1.81627741 -0.26222727], loss=0.16171632604641964, gradiant=[-0.00051491  0.00013839]\n",
      "Loss at iteration 819: w=[ 1.81628251 -0.26222864], loss=0.16171632321670049, gradiant=[-0.00051017  0.00013711]\n",
      "Loss at iteration 820: w=[ 1.81628756 -0.26223   ], loss=0.16171632043883227, gradiant=[-0.00050547  0.00013585]\n",
      "Loss at iteration 821: w=[ 1.81629257 -0.26223135], loss=0.16171631771186504, gradiant=[-0.00050082  0.0001346 ]\n",
      "Loss at iteration 822: w=[ 1.81629753 -0.26223268], loss=0.16171631503486594, gradiant=[-0.00049621  0.00013336]\n",
      "Loss at iteration 823: w=[ 1.81630245 -0.262234  ], loss=0.16171631240691953, gradiant=[-0.00049164  0.00013213]\n",
      "Loss at iteration 824: w=[ 1.81630732 -0.26223531], loss=0.1617163098271267, gradiant=[-0.00048712  0.00013092]\n",
      "Loss at iteration 825: w=[ 1.81631215 -0.26223661], loss=0.16171630729460548, gradiant=[-0.00048263  0.00012971]\n",
      "Loss at iteration 826: w=[ 1.81631693 -0.26223789], loss=0.16171630480848936, gradiant=[-0.00047819  0.00012852]\n",
      "Loss at iteration 827: w=[ 1.81632167 -0.26223917], loss=0.16171630236792817, gradiant=[-0.00047379  0.00012734]\n",
      "Loss at iteration 828: w=[ 1.81632636 -0.26224043], loss=0.1617162999720872, gradiant=[-0.00046943  0.00012616]\n",
      "Loss at iteration 829: w=[ 1.81633101 -0.26224168], loss=0.16171629762014694, gradiant=[-0.00046511  0.000125  ]\n",
      "Loss at iteration 830: w=[ 1.81633562 -0.26224292], loss=0.16171629531130294, gradiant=[-0.00046083  0.00012385]\n",
      "Loss at iteration 831: w=[ 1.81634019 -0.26224414], loss=0.1617162930447655, gradiant=[-0.00045659  0.00012271]\n",
      "Loss at iteration 832: w=[ 1.81634471 -0.26224536], loss=0.1617162908197596, gradiant=[-0.00045238  0.00012158]\n",
      "Loss at iteration 833: w=[ 1.81634919 -0.26224656], loss=0.1617162886355241, gradiant=[-0.00044822  0.00012046]\n",
      "Loss at iteration 834: w=[ 1.81635363 -0.26224776], loss=0.16171628649131187, gradiant=[-0.00044409  0.00011935]\n",
      "Loss at iteration 835: w=[ 1.81635803 -0.26224894], loss=0.16171628438638957, gradiant=[-0.00044001  0.00011826]\n",
      "Loss at iteration 836: w=[ 1.81636239 -0.26225011], loss=0.16171628232003735, gradiant=[-0.00043596  0.00011717]\n",
      "Loss at iteration 837: w=[ 1.81636671 -0.26225127], loss=0.1617162802915484, gradiant=[-0.00043194  0.00011609]\n",
      "Loss at iteration 838: w=[ 1.81637099 -0.26225242], loss=0.16171627830022878, gradiant=[-0.00042797  0.00011502]\n",
      "Loss at iteration 839: w=[ 1.81637523 -0.26225356], loss=0.16171627634539776, gradiant=[-0.00042403  0.00011396]\n",
      "Loss at iteration 840: w=[ 1.81637943 -0.26225469], loss=0.16171627442638636, gradiant=[-0.00042013  0.00011291]\n",
      "Loss at iteration 841: w=[ 1.8163836  -0.26225581], loss=0.16171627254253845, gradiant=[-0.00041626  0.00011187]\n",
      "Loss at iteration 842: w=[ 1.81638772 -0.26225692], loss=0.1617162706932097, gradiant=[-0.00041243  0.00011084]\n",
      "Loss at iteration 843: w=[ 1.81639181 -0.26225802], loss=0.16171626887776744, gradiant=[-0.00040863  0.00010982]\n",
      "Loss at iteration 844: w=[ 1.81639586 -0.26225911], loss=0.161716267095591, gradiant=[-0.00040487  0.00010881]\n",
      "Loss at iteration 845: w=[ 1.81639987 -0.26226018], loss=0.16171626534607045, gradiant=[-0.00040114  0.00010781]\n",
      "Loss at iteration 846: w=[ 1.81640384 -0.26226125], loss=0.16171626362860775, gradiant=[-0.00039745  0.00010682]\n",
      "Loss at iteration 847: w=[ 1.81640778 -0.26226231], loss=0.16171626194261546, gradiant=[-0.00039379  0.00010584]\n",
      "Loss at iteration 848: w=[ 1.81641168 -0.26226336], loss=0.1617162602875167, gradiant=[-0.00039017  0.00010486]\n",
      "Loss at iteration 849: w=[ 1.81641555 -0.2622644 ], loss=0.16171625866274555, gradiant=[-0.00038658  0.0001039 ]\n",
      "Loss at iteration 850: w=[ 1.81641938 -0.26226543], loss=0.16171625706774623, gradiant=[-0.00038302  0.00010294]\n",
      "Loss at iteration 851: w=[ 1.81642317 -0.26226645], loss=0.16171625550197327, gradiant=[-0.00037949  0.00010199]\n",
      "Loss at iteration 852: w=[ 1.81642693 -0.26226746], loss=0.1617162539648912, gradiant=[-0.000376    0.00010105]\n",
      "Loss at iteration 853: w=[ 1.81643066 -0.26226846], loss=0.16171625245597412, gradiant=[-0.00037254  0.00010012]\n",
      "Loss at iteration 854: w=[ 1.81643435 -0.26226945], loss=0.161716250974706, gradiant=[-3.69112223e-04  9.92020289e-05]\n",
      "Loss at iteration 855: w=[ 1.81643801 -0.26227043], loss=0.1617162495205801, gradiant=[-3.65714832e-04  9.82889515e-05]\n",
      "Loss at iteration 856: w=[ 1.81644163 -0.26227141], loss=0.16171624809309942, gradiant=[-3.62348712e-04  9.73842783e-05]\n",
      "Loss at iteration 857: w=[ 1.81644522 -0.26227237], loss=0.16171624669177542, gradiant=[-3.59013575e-04  9.64879318e-05]\n",
      "Loss at iteration 858: w=[ 1.81644878 -0.26227333], loss=0.16171624531612888, gradiant=[-3.55709134e-04  9.55998356e-05]\n",
      "Loss at iteration 859: w=[ 1.8164523  -0.26227428], loss=0.16171624396568937, gradiant=[-3.52435109e-04  9.47199136e-05]\n",
      "Loss at iteration 860: w=[ 1.81645579 -0.26227521], loss=0.1617162426399948, gradiant=[-3.49191218e-04  9.38480905e-05]\n",
      "Loss at iteration 861: w=[ 1.81645925 -0.26227614], loss=0.16171624133859208, gradiant=[-3.45977185e-04  9.29842920e-05]\n",
      "Loss at iteration 862: w=[ 1.81646268 -0.26227707], loss=0.16171624006103566, gradiant=[-3.42792735e-04  9.21284440e-05]\n",
      "Loss at iteration 863: w=[ 1.81646608 -0.26227798], loss=0.16171623880688907, gradiant=[-3.39637595e-04  9.12804734e-05]\n",
      "Loss at iteration 864: w=[ 1.81646944 -0.26227888], loss=0.16171623757572307, gradiant=[-3.36511495e-04  9.04403078e-05]\n",
      "Loss at iteration 865: w=[ 1.81647278 -0.26227978], loss=0.16171623636711657, gradiant=[-3.33414169e-04  8.96078752e-05]\n",
      "Loss at iteration 866: w=[ 1.81647608 -0.26228067], loss=0.16171623518065623, gradiant=[-3.30345351e-04  8.87831045e-05]\n",
      "Loss at iteration 867: w=[ 1.81647935 -0.26228155], loss=0.16171623401593618, gradiant=[-3.27304779e-04  8.79659252e-05]\n",
      "Loss at iteration 868: w=[ 1.8164826  -0.26228242], loss=0.16171623287255832, gradiant=[-3.24292194e-04  8.71562674e-05]\n",
      "Loss at iteration 869: w=[ 1.81648581 -0.26228328], loss=0.16171623175013122, gradiant=[-3.21307337e-04  8.63540618e-05]\n",
      "Loss at iteration 870: w=[ 1.81648899 -0.26228414], loss=0.16171623064827131, gradiant=[-3.18349953e-04  8.55592400e-05]\n",
      "Loss at iteration 871: w=[ 1.81649215 -0.26228498], loss=0.16171622956660142, gradiant=[-3.15419790e-04  8.47717338e-05]\n",
      "Loss at iteration 872: w=[ 1.81649527 -0.26228582], loss=0.161716228504752, gradiant=[-3.12516596e-04  8.39914760e-05]\n",
      "Loss at iteration 873: w=[ 1.81649837 -0.26228666], loss=0.1617162274623595, gradiant=[-3.09640125e-04  8.32183999e-05]\n",
      "Loss at iteration 874: w=[ 1.81650144 -0.26228748], loss=0.16171622643906747, gradiant=[-3.06790128e-04  8.24524394e-05]\n",
      "Loss at iteration 875: w=[ 1.81650448 -0.2622883 ], loss=0.16171622543452593, gradiant=[-3.03966364e-04  8.16935289e-05]\n",
      "Loss at iteration 876: w=[ 1.81650749 -0.26228911], loss=0.1617162244483914, gradiant=[-3.01168591e-04  8.09416036e-05]\n",
      "Loss at iteration 877: w=[ 1.81651047 -0.26228991], loss=0.16171622348032644, gradiant=[-2.98396568e-04  8.01965992e-05]\n",
      "Loss at iteration 878: w=[ 1.81651343 -0.2622907 ], loss=0.16171622253000018, gradiant=[-2.95650060e-04  7.94584519e-05]\n",
      "Loss at iteration 879: w=[ 1.81651636 -0.26229149], loss=0.16171622159708743, gradiant=[-2.92928832e-04  7.87270988e-05]\n",
      "Loss at iteration 880: w=[ 1.81651926 -0.26229227], loss=0.161716220681269, gradiant=[-2.90232650e-04  7.80024772e-05]\n",
      "Loss at iteration 881: w=[ 1.81652214 -0.26229304], loss=0.16171621978223183, gradiant=[-2.87561285e-04  7.72845251e-05]\n",
      "Loss at iteration 882: w=[ 1.81652498 -0.26229381], loss=0.16171621889966825, gradiant=[-2.84914507e-04  7.65731813e-05]\n",
      "Loss at iteration 883: w=[ 1.81652781 -0.26229457], loss=0.1617162180332766, gradiant=[-2.82292091e-04  7.58683848e-05]\n",
      "Loss at iteration 884: w=[ 1.8165306  -0.26229532], loss=0.1617162171827605, gradiant=[-2.79693812e-04  7.51700754e-05]\n",
      "Loss at iteration 885: w=[ 1.81653338 -0.26229606], loss=0.16171621634782912, gradiant=[-2.77119448e-04  7.44781934e-05]\n",
      "Loss at iteration 886: w=[ 1.81653612 -0.2622968 ], loss=0.16171621552819673, gradiant=[-2.74568780e-04  7.37926796e-05]\n",
      "Loss at iteration 887: w=[ 1.81653884 -0.26229753], loss=0.16171621472358308, gradiant=[-2.72041588e-04  7.31134755e-05]\n",
      "Loss at iteration 888: w=[ 1.81654154 -0.26229826], loss=0.16171621393371277, gradiant=[-2.69537657e-04  7.24405229e-05]\n",
      "Loss at iteration 889: w=[ 1.81654421 -0.26229898], loss=0.16171621315831602, gradiant=[-2.67056773e-04  7.17737644e-05]\n",
      "Loss at iteration 890: w=[ 1.81654685 -0.26229969], loss=0.1617162123971273, gradiant=[-2.64598723e-04  7.11131427e-05]\n",
      "Loss at iteration 891: w=[ 1.81654948 -0.26230039], loss=0.1617162116498865, gradiant=[-2.62163298e-04  7.04586017e-05]\n",
      "Loss at iteration 892: w=[ 1.81655207 -0.26230109], loss=0.16171621091633775, gradiant=[-2.59750289e-04  6.98100851e-05]\n",
      "Loss at iteration 893: w=[ 1.81655465 -0.26230178], loss=0.16171621019623056, gradiant=[-2.57359490e-04  6.91675377e-05]\n",
      "Loss at iteration 894: w=[ 1.8165572  -0.26230247], loss=0.16171620948931845, gradiant=[-2.54990697e-04  6.85309044e-05]\n",
      "Loss at iteration 895: w=[ 1.81655972 -0.26230315], loss=0.1617162087953594, gradiant=[-2.52643706e-04  6.79001308e-05]\n",
      "Loss at iteration 896: w=[ 1.81656223 -0.26230382], loss=0.1617162081141163, gradiant=[-2.50318317e-04  6.72751630e-05]\n",
      "Loss at iteration 897: w=[ 1.81656471 -0.26230449], loss=0.16171620744535614, gradiant=[-2.48014332e-04  6.66559475e-05]\n",
      "Loss at iteration 898: w=[ 1.81656716 -0.26230515], loss=0.16171620678885035, gradiant=[-2.45731554e-04  6.60424314e-05]\n",
      "Loss at iteration 899: w=[ 1.8165696 -0.2623058], loss=0.16171620614437388, gradiant=[-2.43469786e-04  6.54345623e-05]\n",
      "Loss at iteration 900: w=[ 1.81657201 -0.26230645], loss=0.16171620551170685, gradiant=[-2.41228836e-04  6.48322881e-05]\n",
      "Loss at iteration 901: w=[ 1.8165744  -0.26230709], loss=0.1617162048906325, gradiant=[-2.39008513e-04  6.42355574e-05]\n",
      "Loss at iteration 902: w=[ 1.81657677 -0.26230773], loss=0.16171620428093858, gradiant=[-2.36808626e-04  6.36443191e-05]\n",
      "Loss at iteration 903: w=[ 1.81657911 -0.26230836], loss=0.16171620368241654, gradiant=[-2.34628987e-04  6.30585228e-05]\n",
      "Loss at iteration 904: w=[ 1.81658144 -0.26230898], loss=0.16171620309486157, gradiant=[-2.32469410e-04  6.24781182e-05]\n",
      "Loss at iteration 905: w=[ 1.81658374 -0.2623096 ], loss=0.1617162025180729, gradiant=[-2.30329710e-04  6.19030558e-05]\n",
      "Loss at iteration 906: w=[ 1.81658602 -0.26231021], loss=0.16171620195185318, gradiant=[-2.28209704e-04  6.13332863e-05]\n",
      "Loss at iteration 907: w=[ 1.81658829 -0.26231082], loss=0.16171620139600873, gradiant=[-2.26109212e-04  6.07687612e-05]\n",
      "Loss at iteration 908: w=[ 1.81659053 -0.26231142], loss=0.1617162008503492, gradiant=[-2.24028052e-04  6.02094321e-05]\n",
      "Loss at iteration 909: w=[ 1.81659275 -0.26231202], loss=0.16171620031468847, gradiant=[-2.21966049e-04  5.96552512e-05]\n",
      "Loss at iteration 910: w=[ 1.81659495 -0.26231261], loss=0.16171619978884286, gradiant=[-2.19923024e-04  5.91061711e-05]\n",
      "Loss at iteration 911: w=[ 1.81659712 -0.2623132 ], loss=0.16171619927263264, gradiant=[-2.17898804e-04  5.85621448e-05]\n",
      "Loss at iteration 912: w=[ 1.81659928 -0.26231378], loss=0.16171619876588142, gradiant=[-2.15893215e-04  5.80231259e-05]\n",
      "Loss at iteration 913: w=[ 1.81660142 -0.26231435], loss=0.16171619826841585, gradiant=[-2.13906086e-04  5.74890682e-05]\n",
      "Loss at iteration 914: w=[ 1.81660354 -0.26231492], loss=0.1617161977800656, gradiant=[-2.11937247e-04  5.69599261e-05]\n",
      "Loss at iteration 915: w=[ 1.81660564 -0.26231549], loss=0.16171619730066378, gradiant=[-2.09986530e-04  5.64356544e-05]\n",
      "Loss at iteration 916: w=[ 1.81660772 -0.26231605], loss=0.16171619683004637, gradiant=[-2.08053768e-04  5.59162081e-05]\n",
      "Loss at iteration 917: w=[ 1.81660978 -0.2623166 ], loss=0.16171619636805235, gradiant=[-2.06138795e-04  5.54015430e-05]\n",
      "Loss at iteration 918: w=[ 1.81661183 -0.26231715], loss=0.1617161959145238, gradiant=[-2.04241448e-04  5.48916150e-05]\n",
      "Loss at iteration 919: w=[ 1.81661385 -0.26231769], loss=0.1617161954693058, gradiant=[-2.02361564e-04  5.43863804e-05]\n",
      "Loss at iteration 920: w=[ 1.81661585 -0.26231823], loss=0.1617161950322456, gradiant=[-2.00498984e-04  5.38857961e-05]\n",
      "Loss at iteration 921: w=[ 1.81661784 -0.26231877], loss=0.16171619460319403, gradiant=[-1.98653547e-04  5.33898193e-05]\n",
      "Loss at iteration 922: w=[ 1.81661981 -0.26231929], loss=0.16171619418200428, gradiant=[-1.96825096e-04  5.28984077e-05]\n",
      "Loss at iteration 923: w=[ 1.81662176 -0.26231982], loss=0.16171619376853244, gradiant=[-1.95013474e-04  5.24115190e-05]\n",
      "Loss at iteration 924: w=[ 1.81662369 -0.26232034], loss=0.16171619336263676, gradiant=[-1.93218527e-04  5.19291118e-05]\n",
      "Loss at iteration 925: w=[ 1.81662561 -0.26232085], loss=0.16171619296417872, gradiant=[-1.91440101e-04  5.14511447e-05]\n",
      "Loss at iteration 926: w=[ 1.8166275  -0.26232136], loss=0.16171619257302175, gradiant=[-1.89678044e-04  5.09775771e-05]\n",
      "Loss at iteration 927: w=[ 1.81662938 -0.26232187], loss=0.16171619218903235, gradiant=[-1.87932205e-04  5.05083681e-05]\n",
      "Loss at iteration 928: w=[ 1.81663124 -0.26232237], loss=0.16171619181207908, gradiant=[-1.86202436e-04  5.00434780e-05]\n",
      "Loss at iteration 929: w=[ 1.81663309 -0.26232286], loss=0.16171619144203297, gradiant=[-1.84488587e-04  4.95828667e-05]\n",
      "Loss at iteration 930: w=[ 1.81663492 -0.26232336], loss=0.1617161910787675, gradiant=[-1.82790514e-04  4.91264951e-05]\n",
      "Loss at iteration 931: w=[ 1.81663673 -0.26232384], loss=0.16171619072215843, gradiant=[-1.81108069e-04  4.86743240e-05]\n",
      "Loss at iteration 932: w=[ 1.81663852 -0.26232432], loss=0.1617161903720838, gradiant=[-1.79441111e-04  4.82263147e-05]\n",
      "Loss at iteration 933: w=[ 1.8166403 -0.2623248], loss=0.1617161900284238, gradiant=[-1.77789495e-04  4.77824291e-05]\n",
      "Loss at iteration 934: w=[ 1.81664206 -0.26232528], loss=0.16171618969106077, gradiant=[-1.76153081e-04  4.73426290e-05]\n",
      "Loss at iteration 935: w=[ 1.81664381 -0.26232574], loss=0.1617161893598797, gradiant=[-1.74531729e-04  4.69068770e-05]\n",
      "Loss at iteration 936: w=[ 1.81664554 -0.26232621], loss=0.16171618903476698, gradiant=[-1.72925301e-04  4.64751357e-05]\n",
      "Loss at iteration 937: w=[ 1.81664725 -0.26232667], loss=0.16171618871561153, gradiant=[-1.71333658e-04  4.60473683e-05]\n",
      "Loss at iteration 938: w=[ 1.81664895 -0.26232713], loss=0.16171618840230428, gradiant=[-1.69756665e-04  4.56235382e-05]\n",
      "Loss at iteration 939: w=[ 1.81665063 -0.26232758], loss=0.16171618809473795, gradiant=[-1.68194187e-04  4.52036090e-05]\n",
      "Loss at iteration 940: w=[ 1.8166523  -0.26232803], loss=0.16171618779280744, gradiant=[-1.66646091e-04  4.47875450e-05]\n",
      "Loss at iteration 941: w=[ 1.81665395 -0.26232847], loss=0.16171618749640937, gradiant=[-1.65112243e-04  4.43753106e-05]\n",
      "Loss at iteration 942: w=[ 1.81665558 -0.26232891], loss=0.1617161872054423, gradiant=[-1.63592514e-04  4.39668704e-05]\n",
      "Loss at iteration 943: w=[ 1.8166572  -0.26232934], loss=0.16171618691980708, gradiant=[-1.62086772e-04  4.35621896e-05]\n",
      "Loss at iteration 944: w=[ 1.81665881 -0.26232978], loss=0.16171618663940562, gradiant=[-1.60594890e-04  4.31612336e-05]\n",
      "Loss at iteration 945: w=[ 1.8166604 -0.2623302], loss=0.1617161863641421, gradiant=[-1.59116739e-04  4.27639680e-05]\n",
      "Loss at iteration 946: w=[ 1.81666198 -0.26233063], loss=0.16171618609392258, gradiant=[-1.57652193e-04  4.23703591e-05]\n",
      "Loss at iteration 947: w=[ 1.81666354 -0.26233105], loss=0.16171618582865432, gradiant=[-1.56201127e-04  4.19803729e-05]\n",
      "Loss at iteration 948: w=[ 1.81666509 -0.26233146], loss=0.1617161855682468, gradiant=[-1.54763418e-04  4.15939763e-05]\n",
      "Loss at iteration 949: w=[ 1.81666662 -0.26233188], loss=0.16171618531261095, gradiant=[-1.53338941e-04  4.12111361e-05]\n",
      "Loss at iteration 950: w=[ 1.81666814 -0.26233228], loss=0.16171618506165936, gradiant=[-1.51927576e-04  4.08318197e-05]\n",
      "Loss at iteration 951: w=[ 1.81666964 -0.26233269], loss=0.1617161848153062, gradiant=[-1.50529201e-04  4.04559946e-05]\n",
      "Loss at iteration 952: w=[ 1.81667114 -0.26233309], loss=0.16171618457346693, gradiant=[-1.49143696e-04  4.00836287e-05]\n",
      "Loss at iteration 953: w=[ 1.81667261 -0.26233349], loss=0.1617161843360591, gradiant=[-1.47770945e-04  3.97146901e-05]\n",
      "Loss at iteration 954: w=[ 1.81667408 -0.26233388], loss=0.16171618410300165, gradiant=[-1.46410828e-04  3.93491474e-05]\n",
      "Loss at iteration 955: w=[ 1.81667553 -0.26233427], loss=0.16171618387421458, gradiant=[-1.45063231e-04  3.89869690e-05]\n",
      "Loss at iteration 956: w=[ 1.81667697 -0.26233466], loss=0.16171618364961976, gradiant=[-1.43728037e-04  3.86281244e-05]\n",
      "Loss at iteration 957: w=[ 1.81667839 -0.26233504], loss=0.16171618342914018, gradiant=[-1.42405132e-04  3.82725826e-05]\n",
      "Loss at iteration 958: w=[ 1.8166798  -0.26233542], loss=0.16171618321270076, gradiant=[-1.41094404e-04  3.79203133e-05]\n",
      "Loss at iteration 959: w=[ 1.8166812  -0.26233579], loss=0.16171618300022728, gradiant=[-1.39795740e-04  3.75712863e-05]\n",
      "Loss at iteration 960: w=[ 1.81668258 -0.26233617], loss=0.16171618279164723, gradiant=[-1.38509029e-04  3.72254719e-05]\n",
      "Loss at iteration 961: w=[ 1.81668396 -0.26233653], loss=0.16171618258688908, gradiant=[-1.37234161e-04  3.68828404e-05]\n",
      "Loss at iteration 962: w=[ 1.81668532 -0.2623369 ], loss=0.1617161823858827, gradiant=[-1.35971028e-04  3.65433626e-05]\n",
      "Loss at iteration 963: w=[ 1.81668666 -0.26233726], loss=0.16171618218855965, gradiant=[-1.34719520e-04  3.62070094e-05]\n",
      "Loss at iteration 964: w=[ 1.816688   -0.26233762], loss=0.16171618199485222, gradiant=[-1.33479532e-04  3.58737521e-05]\n",
      "Loss at iteration 965: w=[ 1.81668932 -0.26233798], loss=0.1617161818046944, gradiant=[-1.32250957e-04  3.55435622e-05]\n",
      "Loss at iteration 966: w=[ 1.81669063 -0.26233833], loss=0.16171618161802082, gradiant=[-1.31033690e-04  3.52164114e-05]\n",
      "Loss at iteration 967: w=[ 1.81669193 -0.26233868], loss=0.16171618143476782, gradiant=[-1.29827627e-04  3.48922717e-05]\n",
      "Loss at iteration 968: w=[ 1.81669322 -0.26233902], loss=0.1617161812548727, gradiant=[-1.28632665e-04  3.45711156e-05]\n",
      "Loss at iteration 969: w=[ 1.81669449 -0.26233937], loss=0.16171618107827393, gradiant=[-1.27448701e-04  3.42529154e-05]\n",
      "Loss at iteration 970: w=[ 1.81669575 -0.26233971], loss=0.16171618090491108, gradiant=[-1.26275635e-04  3.39376440e-05]\n",
      "Loss at iteration 971: w=[ 1.816697   -0.26234004], loss=0.16171618073472493, gradiant=[-1.25113366e-04  3.36252744e-05]\n",
      "Loss at iteration 972: w=[ 1.81669824 -0.26234037], loss=0.16171618056765721, gradiant=[-1.23961795e-04  3.33157799e-05]\n",
      "Loss at iteration 973: w=[ 1.81669947 -0.2623407 ], loss=0.16171618040365077, gradiant=[-1.22820823e-04  3.30091341e-05]\n",
      "Loss at iteration 974: w=[ 1.81670069 -0.26234103], loss=0.1617161802426496, gradiant=[-1.21690353e-04  3.27053108e-05]\n",
      "Loss at iteration 975: w=[ 1.81670189 -0.26234136], loss=0.1617161800845985, gradiant=[-1.20570289e-04  3.24042839e-05]\n",
      "Loss at iteration 976: w=[ 1.81670309 -0.26234168], loss=0.16171617992944365, gradiant=[-1.19460533e-04  3.21060277e-05]\n",
      "Loss at iteration 977: w=[ 1.81670427 -0.26234199], loss=0.16171617977713154, gradiant=[-1.18360992e-04  3.18105167e-05]\n",
      "Loss at iteration 978: w=[ 1.81670545 -0.26234231], loss=0.1617161796276106, gradiant=[-1.17271571e-04  3.15177257e-05]\n",
      "Loss at iteration 979: w=[ 1.81670661 -0.26234262], loss=0.1617161794808294, gradiant=[-1.16192178e-04  3.12276295e-05]\n",
      "Loss at iteration 980: w=[ 1.81670776 -0.26234293], loss=0.16171617933673765, gradiant=[-1.15122719e-04  3.09402035e-05]\n",
      "Loss at iteration 981: w=[ 1.8167089  -0.26234324], loss=0.16171617919528627, gradiant=[-1.14063104e-04  3.06554231e-05]\n",
      "Loss at iteration 982: w=[ 1.81671003 -0.26234354], loss=0.16171617905642688, gradiant=[-1.13013242e-04  3.03732638e-05]\n",
      "Loss at iteration 983: w=[ 1.81671115 -0.26234384], loss=0.16171617892011186, gradiant=[-1.11973043e-04  3.00937016e-05]\n",
      "Loss at iteration 984: w=[ 1.81671226 -0.26234414], loss=0.16171617878629457, gradiant=[-1.10942419e-04  2.98167125e-05]\n",
      "Loss at iteration 985: w=[ 1.81671336 -0.26234444], loss=0.16171617865492938, gradiant=[-1.09921280e-04  2.95422729e-05]\n",
      "Loss at iteration 986: w=[ 1.81671445 -0.26234473], loss=0.1617161785259712, gradiant=[-1.08909540e-04  2.92703592e-05]\n",
      "Loss at iteration 987: w=[ 1.81671553 -0.26234502], loss=0.16171617839937616, gradiant=[-1.07907113e-04  2.90009484e-05]\n",
      "Loss at iteration 988: w=[ 1.81671659 -0.26234531], loss=0.16171617827510054, gradiant=[-1.06913912e-04  2.87340172e-05]\n",
      "Loss at iteration 989: w=[ 1.81671765 -0.26234559], loss=0.16171617815310235, gradiant=[-1.05929853e-04  2.84695430e-05]\n",
      "Loss at iteration 990: w=[ 1.8167187  -0.26234587], loss=0.16171617803333965, gradiant=[-1.04954851e-04  2.82075030e-05]\n",
      "Loss at iteration 991: w=[ 1.81671974 -0.26234615], loss=0.1617161779157714, gradiant=[-1.03988823e-04  2.79478749e-05]\n",
      "Loss at iteration 992: w=[ 1.81672077 -0.26234643], loss=0.1617161778003573, gradiant=[-1.03031687e-04  2.76906365e-05]\n",
      "Loss at iteration 993: w=[ 1.81672179 -0.2623467 ], loss=0.16171617768705815, gradiant=[-1.02083361e-04  2.74357658e-05]\n",
      "Loss at iteration 994: w=[ 1.81672281 -0.26234698], loss=0.16171617757583512, gradiant=[-1.01143763e-04  2.71832409e-05]\n",
      "Loss at iteration 995: w=[ 1.81672381 -0.26234725], loss=0.1617161774666499, gradiant=[-1.00212813e-04  2.69330404e-05]\n",
      "Loss at iteration 996: w=[ 1.8167248  -0.26234751], loss=0.16171617735946553, gradiant=[-9.92904324e-05  2.66851427e-05]\n",
      "Loss at iteration 997: w=[ 1.81672578 -0.26234778], loss=0.16171617725424514, gradiant=[-9.83765413e-05  2.64395268e-05]\n",
      "Loss at iteration 998: w=[ 1.81672676 -0.26234804], loss=0.16171617715095282, gradiant=[-9.74710619e-05  2.61961715e-05]\n",
      "Loss at iteration 999: w=[ 1.81672773 -0.2623483 ], loss=0.1617161770495531, gradiant=[-9.65739167e-05  2.59550562e-05]\n",
      "Loss at iteration 1000: w=[ 1.81672868 -0.26234856], loss=0.16171617695001148, gradiant=[-9.56850291e-05  2.57161601e-05]\n",
      "Loss at iteration 1001: w=[ 1.81672963 -0.26234881], loss=0.1617161768522939, gradiant=[-9.48043229e-05  2.54794629e-05]\n",
      "Loss at iteration 1002: w=[ 1.81673057 -0.26234906], loss=0.16171617675636676, gradiant=[-9.39317229e-05  2.52449442e-05]\n",
      "Loss at iteration 1003: w=[ 1.8167315  -0.26234931], loss=0.16171617666219734, gradiant=[-9.30671546e-05  2.50125842e-05]\n",
      "Loss at iteration 1004: w=[ 1.81673242 -0.26234956], loss=0.16171617656975357, gradiant=[-9.22105439e-05  2.47823628e-05]\n",
      "Loss at iteration 1005: w=[ 1.81673334 -0.26234981], loss=0.1617161764790036, gradiant=[-9.13618177e-05  2.45542605e-05]\n",
      "Loss at iteration 1006: w=[ 1.81673424 -0.26235005], loss=0.16171617638991653, gradiant=[-9.05209033e-05  2.43282577e-05]\n",
      "Loss at iteration 1007: w=[ 1.81673514 -0.26235029], loss=0.16171617630246182, gradiant=[-8.96877289e-05  2.41043350e-05]\n",
      "Loss at iteration 1008: w=[ 1.81673603 -0.26235053], loss=0.1617161762166097, gradiant=[-8.88622233e-05  2.38824734e-05]\n",
      "Loss at iteration 1009: w=[ 1.81673691 -0.26235077], loss=0.1617161761323307, gradiant=[-8.80443157e-05  2.36626538e-05]\n",
      "Loss at iteration 1010: w=[ 1.81673778 -0.262351  ], loss=0.16171617604959596, gradiant=[-8.72339364e-05  2.34448575e-05]\n",
      "Loss at iteration 1011: w=[ 1.81673864 -0.26235123], loss=0.16171617596837734, gradiant=[-8.64310159e-05  2.32290659e-05]\n",
      "Loss at iteration 1012: w=[ 1.8167395  -0.26235146], loss=0.16171617588864678, gradiant=[-8.56354858e-05  2.30152604e-05]\n",
      "Loss at iteration 1013: w=[ 1.81674035 -0.26235169], loss=0.16171617581037728, gradiant=[-8.48472778e-05  2.28034229e-05]\n",
      "Loss at iteration 1014: w=[ 1.81674119 -0.26235192], loss=0.16171617573354202, gradiant=[-8.40663247e-05  2.25935351e-05]\n",
      "Loss at iteration 1015: w=[ 1.81674202 -0.26235214], loss=0.16171617565811455, gradiant=[-8.32925597e-05  2.23855792e-05]\n",
      "Loss at iteration 1016: w=[ 1.81674285 -0.26235236], loss=0.16171617558406937, gradiant=[-8.25259166e-05  2.21795374e-05]\n",
      "Loss at iteration 1017: w=[ 1.81674367 -0.26235258], loss=0.1617161755113807, gradiant=[-8.17663298e-05  2.19753921e-05]\n",
      "Loss at iteration 1018: w=[ 1.81674448 -0.2623528 ], loss=0.16171617544002415, gradiant=[-8.10137344e-05  2.17731257e-05]\n",
      "Loss at iteration 1019: w=[ 1.81674528 -0.26235302], loss=0.1617161753699751, gradiant=[-8.02680661e-05  2.15727210e-05]\n",
      "Loss at iteration 1020: w=[ 1.81674607 -0.26235323], loss=0.16171617530120952, gradiant=[-7.95292611e-05  2.13741609e-05]\n",
      "Loss at iteration 1021: w=[ 1.81674686 -0.26235344], loss=0.1617161752337041, gradiant=[-7.87972562e-05  2.11774284e-05]\n",
      "Loss at iteration 1022: w=[ 1.81674764 -0.26235365], loss=0.16171617516743556, gradiant=[-7.80719888e-05  2.09825067e-05]\n",
      "Loss at iteration 1023: w=[ 1.81674842 -0.26235386], loss=0.16171617510238134, gradiant=[-7.73533970e-05  2.07893791e-05]\n",
      "Loss at iteration 1024: w=[ 1.81674918 -0.26235406], loss=0.16171617503851918, gradiant=[-7.66414192e-05  2.05980290e-05]\n",
      "Loss at iteration 1025: w=[ 1.81674994 -0.26235427], loss=0.16171617497582708, gradiant=[-7.59359946e-05  2.04084402e-05]\n",
      "Loss at iteration 1026: w=[ 1.81675069 -0.26235447], loss=0.16171617491428383, gradiant=[-7.52370630e-05  2.02205964e-05]\n",
      "Loss at iteration 1027: w=[ 1.81675144 -0.26235467], loss=0.16171617485386827, gradiant=[-7.45445644e-05  2.00344816e-05]\n",
      "Loss at iteration 1028: w=[ 1.81675218 -0.26235487], loss=0.1617161747945598, gradiant=[-7.38584398e-05  1.98500798e-05]\n",
      "Loss at iteration 1029: w=[ 1.81675291 -0.26235507], loss=0.16171617473633804, gradiant=[-7.31786303e-05  1.96673753e-05]\n",
      "Loss at iteration 1030: w=[ 1.81675363 -0.26235526], loss=0.16171617467918303, gradiant=[-7.25050781e-05  1.94863524e-05]\n",
      "Loss at iteration 1031: w=[ 1.81675435 -0.26235545], loss=0.16171617462307541, gradiant=[-7.18377253e-05  1.93069957e-05]\n",
      "Loss at iteration 1032: w=[ 1.81675506 -0.26235565], loss=0.1617161745679959, gradiant=[-7.11765150e-05  1.91292898e-05]\n",
      "Loss at iteration 1033: w=[ 1.81675577 -0.26235584], loss=0.16171617451392553, gradiant=[-7.05213906e-05  1.89532197e-05]\n",
      "Loss at iteration 1034: w=[ 1.81675647 -0.26235602], loss=0.16171617446084616, gradiant=[-6.98722962e-05  1.87787700e-05]\n",
      "Loss at iteration 1035: w=[ 1.81675716 -0.26235621], loss=0.16171617440873925, gradiant=[-6.92291761e-05  1.86059261e-05]\n",
      "Loss at iteration 1036: w=[ 1.81675785 -0.26235639], loss=0.16171617435758717, gradiant=[-6.85919755e-05  1.84346730e-05]\n",
      "Loss at iteration 1037: w=[ 1.81675853 -0.26235658], loss=0.16171617430737223, gradiant=[-6.79606397e-05  1.82649962e-05]\n",
      "Loss at iteration 1038: w=[ 1.8167592  -0.26235676], loss=0.16171617425807758, gradiant=[-6.73351150e-05  1.80968811e-05]\n",
      "Loss at iteration 1039: w=[ 1.81675987 -0.26235694], loss=0.1617161742096862, gradiant=[-6.67153477e-05  1.79303135e-05]\n",
      "Loss at iteration 1040: w=[ 1.81676053 -0.26235711], loss=0.16171617416218156, gradiant=[-6.61012849e-05  1.77652788e-05]\n",
      "Loss at iteration 1041: w=[ 1.81676118 -0.26235729], loss=0.1617161741155472, gradiant=[-6.54928741e-05  1.76017633e-05]\n",
      "Loss at iteration 1042: w=[ 1.81676183 -0.26235746], loss=0.1617161740697675, gradiant=[-6.48900632e-05  1.74397528e-05]\n",
      "Loss at iteration 1043: w=[ 1.81676247 -0.26235764], loss=0.16171617402482671, gradiant=[-6.42928007e-05  1.72792335e-05]\n",
      "Loss at iteration 1044: w=[ 1.81676311 -0.26235781], loss=0.16171617398070937, gradiant=[-6.37010356e-05  1.71201915e-05]\n",
      "Loss at iteration 1045: w=[ 1.81676374 -0.26235798], loss=0.16171617393740023, gradiant=[-6.31147171e-05  1.69626136e-05]\n",
      "Loss at iteration 1046: w=[ 1.81676437 -0.26235815], loss=0.16171617389488493, gradiant=[-6.25337953e-05  1.68064859e-05]\n",
      "Loss at iteration 1047: w=[ 1.81676499 -0.26235831], loss=0.16171617385314846, gradiant=[-6.19582204e-05  1.66517953e-05]\n",
      "Loss at iteration 1048: w=[ 1.8167656  -0.26235848], loss=0.16171617381217696, gradiant=[-6.13879433e-05  1.64985284e-05]\n",
      "Loss at iteration 1049: w=[ 1.81676621 -0.26235864], loss=0.16171617377195613, gradiant=[-6.08229151e-05  1.63466724e-05]\n",
      "Loss at iteration 1050: w=[ 1.81676681 -0.2623588 ], loss=0.16171617373247216, gradiant=[-6.02630875e-05  1.61962140e-05]\n",
      "Loss at iteration 1051: w=[ 1.81676741 -0.26235896], loss=0.16171617369371186, gradiant=[-5.97084128e-05  1.60471405e-05]\n",
      "Loss at iteration 1052: w=[ 1.816768   -0.26235912], loss=0.1617161736556617, gradiant=[-5.91588433e-05  1.58994390e-05]\n",
      "Loss at iteration 1053: w=[ 1.81676859 -0.26235928], loss=0.16171617361830884, gradiant=[-5.86143323e-05  1.57530971e-05]\n",
      "Loss at iteration 1054: w=[ 1.81676917 -0.26235944], loss=0.16171617358164042, gradiant=[-5.80748330e-05  1.56081021e-05]\n",
      "Loss at iteration 1055: w=[ 1.81676974 -0.26235959], loss=0.16171617354564377, gradiant=[-5.75402994e-05  1.54644417e-05]\n",
      "Loss at iteration 1056: w=[ 1.81677031 -0.26235974], loss=0.16171617351030673, gradiant=[-5.70106858e-05  1.53221035e-05]\n",
      "Loss at iteration 1057: w=[ 1.81677088 -0.2623599 ], loss=0.1617161734756174, gradiant=[-5.64859468e-05  1.51810755e-05]\n",
      "Loss at iteration 1058: w=[ 1.81677144 -0.26236005], loss=0.16171617344156342, gradiant=[-5.59660377e-05  1.50413455e-05]\n",
      "Loss at iteration 1059: w=[ 1.81677199 -0.2623602 ], loss=0.1617161734081337, gradiant=[-5.54509139e-05  1.49029017e-05]\n",
      "Loss at iteration 1060: w=[ 1.81677254 -0.26236034], loss=0.1617161733753164, gradiant=[-5.49405314e-05  1.47657320e-05]\n",
      "Loss at iteration 1061: w=[ 1.81677309 -0.26236049], loss=0.1617161733431004, gradiant=[-5.44348466e-05  1.46298250e-05]\n",
      "Loss at iteration 1062: w=[ 1.81677363 -0.26236063], loss=0.16171617331147475, gradiant=[-5.39338163e-05  1.44951688e-05]\n",
      "Loss at iteration 1063: w=[ 1.81677416 -0.26236078], loss=0.1617161732804286, gradiant=[-5.34373975e-05  1.43617521e-05]\n",
      "Loss at iteration 1064: w=[ 1.81677469 -0.26236092], loss=0.1617161732499515, gradiant=[-5.29455479e-05  1.42295634e-05]\n",
      "Loss at iteration 1065: w=[ 1.81677521 -0.26236106], loss=0.16171617322003262, gradiant=[-5.24582253e-05  1.40985914e-05]\n",
      "Loss at iteration 1066: w=[ 1.81677573 -0.2623612 ], loss=0.16171617319066206, gradiant=[-5.19753882e-05  1.39688248e-05]\n",
      "Loss at iteration 1067: w=[ 1.81677625 -0.26236134], loss=0.16171617316182968, gradiant=[-5.14969953e-05  1.38402526e-05]\n",
      "Loss at iteration 1068: w=[ 1.81677676 -0.26236148], loss=0.16171617313352557, gradiant=[-5.10230056e-05  1.37128639e-05]\n",
      "Loss at iteration 1069: w=[ 1.81677726 -0.26236161], loss=0.16171617310574016, gradiant=[-5.05533785e-05  1.35866477e-05]\n",
      "Loss at iteration 1070: w=[ 1.81677777 -0.26236175], loss=0.16171617307846395, gradiant=[-5.00880741e-05  1.34615931e-05]\n",
      "Loss at iteration 1071: w=[ 1.81677826 -0.26236188], loss=0.1617161730516874, gradiant=[-4.96270524e-05  1.33376897e-05]\n",
      "Loss at iteration 1072: w=[ 1.81677875 -0.26236201], loss=0.16171617302540153, gradiant=[-4.91702740e-05  1.32149266e-05]\n",
      "Loss at iteration 1073: w=[ 1.81677924 -0.26236214], loss=0.16171617299959729, gradiant=[-4.87176999e-05  1.30932935e-05]\n",
      "Loss at iteration 1074: w=[ 1.81677972 -0.26236227], loss=0.16171617297426594, gradiant=[-4.82692915e-05  1.29727799e-05]\n",
      "Loss at iteration 1075: w=[ 1.8167802 -0.2623624], loss=0.16171617294939883, gradiant=[-4.78250102e-05  1.28533756e-05]\n",
      "Loss at iteration 1076: w=[ 1.81678068 -0.26236253], loss=0.16171617292498727, gradiant=[-4.73848183e-05  1.27350703e-05]\n",
      "Loss at iteration 1077: w=[ 1.81678114 -0.26236266], loss=0.16171617290102291, gradiant=[-4.69486779e-05  1.26178539e-05]\n",
      "Loss at iteration 1078: w=[ 1.81678161 -0.26236278], loss=0.16171617287749787, gradiant=[-4.65165519e-05  1.25017164e-05]\n",
      "Loss at iteration 1079: w=[ 1.81678207 -0.2623629 ], loss=0.16171617285440384, gradiant=[-4.60884033e-05  1.23866478e-05]\n",
      "Loss at iteration 1080: w=[ 1.81678253 -0.26236303], loss=0.16171617283173292, gradiant=[-4.56641954e-05  1.22726384e-05]\n",
      "Loss at iteration 1081: w=[ 1.81678298 -0.26236315], loss=0.16171617280947748, gradiant=[-4.52438921e-05  1.21596783e-05]\n",
      "Loss at iteration 1082: w=[ 1.81678343 -0.26236327], loss=0.16171617278762976, gradiant=[-4.48274573e-05  1.20477579e-05]\n",
      "Loss at iteration 1083: w=[ 1.81678387 -0.26236339], loss=0.1617161727661825, gradiant=[-4.44148555e-05  1.19368677e-05]\n",
      "Loss at iteration 1084: w=[ 1.81678431 -0.26236351], loss=0.16171617274512817, gradiant=[-4.40060513e-05  1.18269981e-05]\n",
      "Loss at iteration 1085: w=[ 1.81678475 -0.26236362], loss=0.1617161727244596, gradiant=[-4.36010099e-05  1.17181399e-05]\n",
      "Loss at iteration 1086: w=[ 1.81678518 -0.26236374], loss=0.16171617270416977, gradiant=[-4.31996966e-05  1.16102835e-05]\n",
      "Loss at iteration 1087: w=[ 1.81678561 -0.26236385], loss=0.16171617268425167, gradiant=[-4.28020770e-05  1.15034199e-05]\n",
      "Loss at iteration 1088: w=[ 1.81678603 -0.26236397], loss=0.16171617266469857, gradiant=[-4.24081172e-05  1.13975398e-05]\n",
      "Loss at iteration 1089: w=[ 1.81678645 -0.26236408], loss=0.16171617264550384, gradiant=[-4.20177835e-05  1.12926344e-05]\n",
      "Loss at iteration 1090: w=[ 1.81678687 -0.26236419], loss=0.16171617262666085, gradiant=[-4.16310426e-05  1.11886945e-05]\n",
      "Loss at iteration 1091: w=[ 1.81678728 -0.2623643 ], loss=0.16171617260816296, gradiant=[-4.12478613e-05  1.10857113e-05]\n",
      "Loss at iteration 1092: w=[ 1.81678769 -0.26236441], loss=0.1617161725900042, gradiant=[-4.08682068e-05  1.09836759e-05]\n",
      "Loss at iteration 1093: w=[ 1.8167881  -0.26236452], loss=0.16171617257217807, gradiant=[-4.04920468e-05  1.08825798e-05]\n",
      "Loss at iteration 1094: w=[ 1.8167885  -0.26236463], loss=0.16171617255467868, gradiant=[-4.01193491e-05  1.07824141e-05]\n",
      "Loss at iteration 1095: w=[ 1.81678889 -0.26236474], loss=0.16171617253749976, gradiant=[-3.97500817e-05  1.06831704e-05]\n",
      "Loss at iteration 1096: w=[ 1.81678929 -0.26236484], loss=0.16171617252063583, gradiant=[-3.93842132e-05  1.05848401e-05]\n",
      "Loss at iteration 1097: w=[ 1.81678968 -0.26236495], loss=0.16171617250408088, gradiant=[-3.90217122e-05  1.04874149e-05]\n",
      "Loss at iteration 1098: w=[ 1.81679006 -0.26236505], loss=0.16171617248782907, gradiant=[-3.86625477e-05  1.03908864e-05]\n",
      "Loss at iteration 1099: w=[ 1.81679045 -0.26236516], loss=0.1617161724718753, gradiant=[-3.83066891e-05  1.02952464e-05]\n",
      "Loss at iteration 1100: w=[ 1.81679083 -0.26236526], loss=0.1617161724562137, gradiant=[-3.79541058e-05  1.02004866e-05]\n",
      "Loss at iteration 1101: w=[ 1.8167912  -0.26236536], loss=0.16171617244083913, gradiant=[-3.76047679e-05  1.01065991e-05]\n",
      "Loss at iteration 1102: w=[ 1.81679158 -0.26236546], loss=0.1617161724257463, gradiant=[-3.72586453e-05  1.00135757e-05]\n",
      "Loss at iteration 1103: w=[ 1.81679194 -0.26236556], loss=0.16171617241093006, gradiant=[-3.69157085e-05  9.92140859e-06]\n",
      "Loss at iteration 1104: w=[ 1.81679231 -0.26236566], loss=0.16171617239638522, gradiant=[-3.65759281e-05  9.83008974e-06]\n",
      "Loss at iteration 1105: w=[ 1.81679267 -0.26236575], loss=0.16171617238210695, gradiant=[-3.62392752e-05  9.73961143e-06]\n",
      "Loss at iteration 1106: w=[ 1.81679303 -0.26236585], loss=0.16171617236809033, gradiant=[-3.59057209e-05  9.64996589e-06]\n",
      "Loss at iteration 1107: w=[ 1.81679339 -0.26236595], loss=0.16171617235433056, gradiant=[-3.55752367e-05  9.56114547e-06]\n",
      "Loss at iteration 1108: w=[ 1.81679374 -0.26236604], loss=0.1617161723408229, gradiant=[-3.52477944e-05  9.47314257e-06]\n",
      "Loss at iteration 1109: w=[ 1.81679409 -0.26236613], loss=0.1617161723275627, gradiant=[-3.49233659e-05  9.38594968e-06]\n",
      "Loss at iteration 1110: w=[ 1.81679444 -0.26236623], loss=0.1617161723145454, gradiant=[-3.46019235e-05  9.29955932e-06]\n",
      "Loss at iteration 1111: w=[ 1.81679478 -0.26236632], loss=0.16171617230176688, gradiant=[-3.42834397e-05  9.21396412e-06]\n",
      "Loss at iteration 1112: w=[ 1.81679512 -0.26236641], loss=0.16171617228922228, gradiant=[-3.39678874e-05  9.12915676e-06]\n",
      "Loss at iteration 1113: w=[ 1.81679545 -0.2623665 ], loss=0.16171617227690777, gradiant=[-3.36552394e-05  9.04512997e-06]\n",
      "Loss at iteration 1114: w=[ 1.81679579 -0.26236659], loss=0.16171617226481858, gradiant=[-3.33454692e-05  8.96187662e-06]\n",
      "Loss at iteration 1115: w=[ 1.81679612 -0.26236668], loss=0.16171617225295115, gradiant=[-3.30385501e-05  8.87938949e-06]\n",
      "Loss at iteration 1116: w=[ 1.81679645 -0.26236677], loss=0.16171617224130108, gradiant=[-3.27344560e-05  8.79766167e-06]\n",
      "Loss at iteration 1117: w=[ 1.81679677 -0.26236685], loss=0.16171617222986462, gradiant=[-3.24331608e-05  8.71668601e-06]\n",
      "Loss at iteration 1118: w=[ 1.81679709 -0.26236694], loss=0.16171617221863754, gradiant=[-3.21346388e-05  8.63645574e-06]\n",
      "Loss at iteration 1119: w=[ 1.81679741 -0.26236703], loss=0.16171617220761636, gradiant=[-3.18388645e-05  8.55696387e-06]\n",
      "Loss at iteration 1120: w=[ 1.81679773 -0.26236711], loss=0.16171617219679704, gradiant=[-3.15458126e-05  8.47820370e-06]\n",
      "Loss at iteration 1121: w=[ 1.81679804 -0.2623672 ], loss=0.16171617218617595, gradiant=[-3.12554579e-05  8.40016841e-06]\n",
      "Loss at iteration 1122: w=[ 1.81679835 -0.26236728], loss=0.16171617217574935, gradiant=[-3.09677758e-05  8.32285143e-06]\n",
      "Loss at iteration 1123: w=[ 1.81679865 -0.26236736], loss=0.16171617216551404, gradiant=[-3.06827415e-05  8.24624604e-06]\n",
      "Loss at iteration 1124: w=[ 1.81679896 -0.26236744], loss=0.16171617215546621, gradiant=[-3.04003308e-05  8.17034580e-06]\n",
      "Loss at iteration 1125: w=[ 1.81679926 -0.26236752], loss=0.1617161721456024, gradiant=[-3.01205194e-05  8.09514410e-06]\n",
      "Loss at iteration 1126: w=[ 1.81679956 -0.2623676 ], loss=0.1617161721359194, gradiant=[-2.98432835e-05  8.02063464e-06]\n",
      "Loss at iteration 1127: w=[ 1.81679985 -0.26236768], loss=0.16171617212641382, gradiant=[-2.95685993e-05  7.94681090e-06]\n",
      "Loss at iteration 1128: w=[ 1.81680015 -0.26236776], loss=0.1617161721170825, gradiant=[-2.92964434e-05  7.87366674e-06]\n",
      "Loss at iteration 1129: w=[ 1.81680044 -0.26236784], loss=0.1617161721079221, gradiant=[-2.90267925e-05  7.80119573e-06]\n",
      "Loss at iteration 1130: w=[ 1.81680072 -0.26236792], loss=0.16171617209892947, gradiant=[-2.87596235e-05  7.72939184e-06]\n",
      "Loss at iteration 1131: w=[ 1.81680101 -0.26236799], loss=0.1617161720901017, gradiant=[-2.84949135e-05  7.65824877e-06]\n",
      "Loss at iteration 1132: w=[ 1.81680129 -0.26236807], loss=0.1617161720814356, gradiant=[-2.82326400e-05  7.58776059e-06]\n",
      "Loss at iteration 1133: w=[ 1.81680157 -0.26236814], loss=0.16171617207292852, gradiant=[-2.79727806e-05  7.51792114e-06]\n",
      "Loss at iteration 1134: w=[ 1.81680185 -0.26236822], loss=0.1617161720645771, gradiant=[-2.77153129e-05  7.44872456e-06]\n",
      "Loss at iteration 1135: w=[ 1.81680212 -0.26236829], loss=0.16171617205637875, gradiant=[-2.74602151e-05  7.38016483e-06]\n",
      "Loss at iteration 1136: w=[ 1.8168024  -0.26236837], loss=0.16171617204833072, gradiant=[-2.72074652e-05  7.31223618e-06]\n",
      "Loss at iteration 1137: w=[ 1.81680267 -0.26236844], loss=0.16171617204043004, gradiant=[-2.69570417e-05  7.24493272e-06]\n",
      "Loss at iteration 1138: w=[ 1.81680293 -0.26236851], loss=0.1617161720326742, gradiant=[-2.67089231e-05  7.17824878e-06]\n",
      "Loss at iteration 1139: w=[ 1.8168032  -0.26236858], loss=0.16171617202506053, gradiant=[-2.64630883e-05  7.11217856e-06]\n",
      "Loss at iteration 1140: w=[ 1.81680346 -0.26236865], loss=0.1617161720175863, gradiant=[-2.62195161e-05  7.04671653e-06]\n",
      "Loss at iteration 1141: w=[ 1.81680372 -0.26236872], loss=0.16171617201024902, gradiant=[-2.59781859e-05  6.98185697e-06]\n",
      "Loss at iteration 1142: w=[ 1.81680398 -0.26236879], loss=0.16171617200304622, gradiant=[-2.57390770e-05  6.91759443e-06]\n",
      "Loss at iteration 1143: w=[ 1.81680423 -0.26236886], loss=0.16171617199597527, gradiant=[-2.55021688e-05  6.85392335e-06]\n",
      "Loss at iteration 1144: w=[ 1.81680448 -0.26236893], loss=0.16171617198903407, gradiant=[-2.52674412e-05  6.79083833e-06]\n",
      "Loss at iteration 1145: w=[ 1.81680473 -0.26236899], loss=0.16171617198222, gradiant=[-2.50348741e-05  6.72833395e-06]\n",
      "Loss at iteration 1146: w=[ 1.81680498 -0.26236906], loss=0.1617161719755308, gradiant=[-2.48044476e-05  6.66640488e-06]\n",
      "Loss at iteration 1147: w=[ 1.81680523 -0.26236913], loss=0.1617161719689641, gradiant=[-2.45761420e-05  6.60504582e-06]\n",
      "Loss at iteration 1148: w=[ 1.81680547 -0.26236919], loss=0.16171617196251767, gradiant=[-2.43499377e-05  6.54425151e-06]\n",
      "Loss at iteration 1149: w=[ 1.81680571 -0.26236926], loss=0.16171617195618965, gradiant=[-2.41258155e-05  6.48401678e-06]\n",
      "Loss at iteration 1150: w=[ 1.81680595 -0.26236932], loss=0.16171617194997728, gradiant=[-2.39037562e-05  6.42433645e-06]\n",
      "Loss at iteration 1151: w=[ 1.81680619 -0.26236939], loss=0.16171617194387883, gradiant=[-2.36837407e-05  6.36520545e-06]\n",
      "Loss at iteration 1152: w=[ 1.81680642 -0.26236945], loss=0.16171617193789217, gradiant=[-2.34657503e-05  6.30661866e-06]\n",
      "Loss at iteration 1153: w=[ 1.81680666 -0.26236951], loss=0.16171617193201518, gradiant=[-2.32497664e-05  6.24857118e-06]\n",
      "Loss at iteration 1154: w=[ 1.81680689 -0.26236957], loss=0.16171617192624588, gradiant=[-2.30357704e-05  6.19105792e-06]\n",
      "Loss at iteration 1155: w=[ 1.81680711 -0.26236963], loss=0.16171617192058232, gradiant=[-2.28237441e-05  6.13407409e-06]\n",
      "Loss at iteration 1156: w=[ 1.81680734 -0.2623697 ], loss=0.16171617191502255, gradiant=[-2.26136693e-05  6.07761469e-06]\n",
      "Loss at iteration 1157: w=[ 1.81680756 -0.26236976], loss=0.1617161719095646, gradiant=[-2.24055281e-05  6.02167500e-06]\n",
      "Loss at iteration 1158: w=[ 1.81680779 -0.26236982], loss=0.1617161719042068, gradiant=[-2.21993026e-05  5.96625015e-06]\n",
      "Loss at iteration 1159: w=[ 1.81680801 -0.26236987], loss=0.161716171898947, gradiant=[-2.19949753e-05  5.91133549e-06]\n",
      "Loss at iteration 1160: w=[ 1.81680822 -0.26236993], loss=0.16171617189378373, gradiant=[-2.17925287e-05  5.85692622e-06]\n",
      "Loss at iteration 1161: w=[ 1.81680844 -0.26236999], loss=0.16171617188871495, gradiant=[-2.15919455e-05  5.80301782e-06]\n",
      "Loss at iteration 1162: w=[ 1.81680865 -0.26237005], loss=0.16171617188373907, gradiant=[-2.13932084e-05  5.74960552e-06]\n",
      "Loss at iteration 1163: w=[ 1.81680887 -0.26237011], loss=0.1617161718788543, gradiant=[-2.11963006e-05  5.69668491e-06]\n",
      "Loss at iteration 1164: w=[ 1.81680908 -0.26237016], loss=0.16171617187405918, gradiant=[-2.10012052e-05  5.64425133e-06]\n",
      "Loss at iteration 1165: w=[ 1.81680928 -0.26237022], loss=0.16171617186935183, gradiant=[-2.08079054e-05  5.59230043e-06]\n",
      "Loss at iteration 1166: w=[ 1.81680949 -0.26237027], loss=0.16171617186473075, gradiant=[-2.06163849e-05  5.54082763e-06]\n",
      "Loss at iteration 1167: w=[ 1.81680969 -0.26237033], loss=0.16171617186019432, gradiant=[-2.04266271e-05  5.48982866e-06]\n",
      "Loss at iteration 1168: w=[ 1.8168099  -0.26237038], loss=0.16171617185574114, gradiant=[-2.02386159e-05  5.43929903e-06]\n",
      "Loss at iteration 1169: w=[ 1.8168101  -0.26237044], loss=0.1617161718513695, gradiant=[-2.00523352e-05  5.38923455e-06]\n",
      "Loss at iteration 1170: w=[ 1.8168103  -0.26237049], loss=0.1617161718470779, gradiant=[-1.98677691e-05  5.33963082e-06]\n",
      "Loss at iteration 1171: w=[ 1.81681049 -0.26237054], loss=0.16171617184286494, gradiant=[-1.96849018e-05  5.29048370e-06]\n",
      "Loss at iteration 1172: w=[ 1.81681069 -0.2623706 ], loss=0.1617161718387293, gradiant=[-1.95037176e-05  5.24178890e-06]\n",
      "Loss at iteration 1173: w=[ 1.81681088 -0.26237065], loss=0.16171617183466933, gradiant=[-1.93242011e-05  5.19354234e-06]\n",
      "Loss at iteration 1174: w=[ 1.81681107 -0.2623707 ], loss=0.16171617183068374, gradiant=[-1.91463368e-05  5.14573980e-06]\n",
      "Loss at iteration 1175: w=[ 1.81681126 -0.26237075], loss=0.1617161718267712, gradiant=[-1.89701097e-05  5.09837730e-06]\n",
      "Loss at iteration 1176: w=[ 1.81681145 -0.2623708 ], loss=0.16171617182293047, gradiant=[-1.87955046e-05  5.05145068e-06]\n",
      "Loss at iteration 1177: w=[ 1.81681164 -0.26237085], loss=0.1617161718191599, gradiant=[-1.86225067e-05  5.00495604e-06]\n",
      "Loss at iteration 1178: w=[ 1.81681182 -0.2623709 ], loss=0.16171617181545872, gradiant=[-1.84511010e-05  4.95888928e-06]\n",
      "Loss at iteration 1179: w=[ 1.816812   -0.26237095], loss=0.16171617181182513, gradiant=[-1.82812730e-05  4.91324661e-06]\n",
      "Loss at iteration 1180: w=[ 1.81681219 -0.262371  ], loss=0.16171617180825815, gradiant=[-1.81130081e-05  4.86802397e-06]\n",
      "Loss at iteration 1181: w=[ 1.81681236 -0.26237105], loss=0.16171617180475642, gradiant=[-1.79462920e-05  4.82321763e-06]\n",
      "Loss at iteration 1182: w=[ 1.81681254 -0.26237109], loss=0.1617161718013191, gradiant=[-1.77811103e-05  4.77882363e-06]\n",
      "Loss at iteration 1183: w=[ 1.81681272 -0.26237114], loss=0.1617161717979446, gradiant=[-1.76174491e-05  4.73483832e-06]\n",
      "Loss at iteration 1184: w=[ 1.81681289 -0.26237119], loss=0.161716171794632, gradiant=[-1.74552942e-05  4.69125778e-06]\n",
      "Loss at iteration 1185: w=[ 1.81681307 -0.26237123], loss=0.1617161717913802, gradiant=[-1.72946318e-05  4.64807844e-06]\n",
      "Loss at iteration 1186: w=[ 1.81681324 -0.26237128], loss=0.1617161717881877, gradiant=[-1.71354482e-05  4.60529647e-06]\n",
      "Loss at iteration 1187: w=[ 1.81681341 -0.26237133], loss=0.1617161717850539, gradiant=[-1.69777297e-05  4.56290834e-06]\n",
      "Loss at iteration 1188: w=[ 1.81681358 -0.26237137], loss=0.1617161717819776, gradiant=[-1.68214629e-05  4.52091029e-06]\n",
      "Loss at iteration 1189: w=[ 1.81681374 -0.26237142], loss=0.16171617177895758, gradiant=[-1.66666345e-05  4.47929886e-06]\n",
      "Loss at iteration 1190: w=[ 1.81681391 -0.26237146], loss=0.1617161717759928, gradiant=[-1.65132311e-05  4.43807038e-06]\n",
      "Loss at iteration 1191: w=[ 1.81681407 -0.2623715 ], loss=0.1617161717730824, gradiant=[-1.63612397e-05  4.39722142e-06]\n",
      "Loss at iteration 1192: w=[ 1.81681423 -0.26237155], loss=0.1617161717702254, gradiant=[-1.62106472e-05  4.35674840e-06]\n",
      "Loss at iteration 1193: w=[ 1.81681439 -0.26237159], loss=0.16171617176742062, gradiant=[-1.60614408e-05  4.31664795e-06]\n",
      "Loss at iteration 1194: w=[ 1.81681455 -0.26237163], loss=0.16171617176466732, gradiant=[-1.59136078e-05  4.27691654e-06]\n",
      "Loss at iteration 1195: w=[ 1.81681471 -0.26237168], loss=0.16171617176196454, gradiant=[-1.57671354e-05  4.23755088e-06]\n",
      "Loss at iteration 1196: w=[ 1.81681487 -0.26237172], loss=0.16171617175931124, gradiant=[-1.56220112e-05  4.19854750e-06]\n",
      "Loss at iteration 1197: w=[ 1.81681502 -0.26237176], loss=0.1617161717567065, gradiant=[-1.54782228e-05  4.15990316e-06]\n",
      "Loss at iteration 1198: w=[ 1.81681517 -0.2623718 ], loss=0.16171617175414957, gradiant=[-1.53357578e-05  4.12161448e-06]\n",
      "Loss at iteration 1199: w=[ 1.81681533 -0.26237184], loss=0.1617161717516394, gradiant=[-1.51946041e-05  4.08367825e-06]\n",
      "Loss at iteration 1200: w=[ 1.81681548 -0.26237188], loss=0.16171617174917516, gradiant=[-1.50547496e-05  4.04609115e-06]\n",
      "Loss at iteration 1201: w=[ 1.81681563 -0.26237192], loss=0.16171617174675618, gradiant=[-1.49161823e-05  4.00885005e-06]\n",
      "Loss at iteration 1202: w=[ 1.81681577 -0.26237196], loss=0.16171617174438152, gradiant=[-1.47788905e-05  3.97195169e-06]\n",
      "Loss at iteration 1203: w=[ 1.81681592 -0.262372  ], loss=0.1617161717420504, gradiant=[-1.46428623e-05  3.93539298e-06]\n",
      "Loss at iteration 1204: w=[ 1.81681607 -0.26237204], loss=0.16171617173976202, gradiant=[-1.45080862e-05  3.89917076e-06]\n",
      "Loss at iteration 1205: w=[ 1.81681621 -0.26237208], loss=0.16171617173751557, gradiant=[-1.43745505e-05  3.86328191e-06]\n",
      "Loss at iteration 1206: w=[ 1.81681635 -0.26237212], loss=0.16171617173531014, gradiant=[-1.42422440e-05  3.82772343e-06]\n",
      "Loss at iteration 1207: w=[ 1.81681649 -0.26237216], loss=0.16171617173314523, gradiant=[-1.41111552e-05  3.79249221e-06]\n",
      "Loss at iteration 1208: w=[ 1.81681663 -0.26237219], loss=0.16171617173102007, gradiant=[-1.39812730e-05  3.75758527e-06]\n",
      "Loss at iteration 1209: w=[ 1.81681677 -0.26237223], loss=0.1617161717289337, gradiant=[-1.38525863e-05  3.72299963e-06]\n",
      "Loss at iteration 1210: w=[ 1.81681691 -0.26237227], loss=0.1617161717268857, gradiant=[-1.37250841e-05  3.68873231e-06]\n",
      "Loss at iteration 1211: w=[ 1.81681704 -0.2623723 ], loss=0.16171617172487518, gradiant=[-1.35987553e-05  3.65478040e-06]\n",
      "Loss at iteration 1212: w=[ 1.81681718 -0.26237234], loss=0.16171617172290143, gradiant=[-1.34735894e-05  3.62114101e-06]\n",
      "Loss at iteration 1213: w=[ 1.81681731 -0.26237238], loss=0.1617161717209639, gradiant=[-1.33495755e-05  3.58781121e-06]\n",
      "Loss at iteration 1214: w=[ 1.81681745 -0.26237241], loss=0.16171617171906189, gradiant=[-1.32267031e-05  3.55478822e-06]\n",
      "Loss at iteration 1215: w=[ 1.81681758 -0.26237245], loss=0.16171617171719468, gradiant=[-1.31049616e-05  3.52206915e-06]\n",
      "Loss at iteration 1216: w=[ 1.81681771 -0.26237248], loss=0.1617161717153616, gradiant=[-1.29843406e-05  3.48965125e-06]\n",
      "Loss at iteration 1217: w=[ 1.81681783 -0.26237252], loss=0.1617161717135622, gradiant=[-1.28648299e-05  3.45753174e-06]\n",
      "Loss at iteration 1218: w=[ 1.81681796 -0.26237255], loss=0.16171617171179586, gradiant=[-1.27464191e-05  3.42570784e-06]\n",
      "Loss at iteration 1219: w=[ 1.81681809 -0.26237258], loss=0.16171617171006178, gradiant=[-1.26290983e-05  3.39417688e-06]\n",
      "Loss at iteration 1220: w=[ 1.81681821 -0.26237262], loss=0.1617161717083595, gradiant=[-1.25128572e-05  3.36293612e-06]\n",
      "Loss at iteration 1221: w=[ 1.81681834 -0.26237265], loss=0.1617161717066884, gradiant=[-1.23976861e-05  3.33198291e-06]\n",
      "Loss at iteration 1222: w=[ 1.81681846 -0.26237268], loss=0.16171617170504793, gradiant=[-1.22835751e-05  3.30131461e-06]\n",
      "Loss at iteration 1223: w=[ 1.81681858 -0.26237272], loss=0.1617161717034376, gradiant=[-1.21705144e-05  3.27092857e-06]\n",
      "Loss at iteration 1224: w=[ 1.8168187  -0.26237275], loss=0.16171617170185673, gradiant=[-1.20584943e-05  3.24082223e-06]\n",
      "Loss at iteration 1225: w=[ 1.81681882 -0.26237278], loss=0.16171617170030472, gradiant=[-1.19475052e-05  3.21099298e-06]\n",
      "Loss at iteration 1226: w=[ 1.81681894 -0.26237281], loss=0.16171617169878116, gradiant=[-1.18375377e-05  3.18143829e-06]\n",
      "Loss at iteration 1227: w=[ 1.81681906 -0.26237284], loss=0.1617161716972857, gradiant=[-1.17285824e-05  3.15215563e-06]\n",
      "Loss at iteration 1228: w=[ 1.81681917 -0.26237288], loss=0.16171617169581748, gradiant=[-1.16206300e-05  3.12314249e-06]\n",
      "Loss at iteration 1229: w=[ 1.81681929 -0.26237291], loss=0.1617161716943762, gradiant=[-1.15136711e-05  3.09439640e-06]\n",
      "Loss at iteration 1230: w=[ 1.8168194  -0.26237294], loss=0.16171617169296135, gradiant=[-1.14076967e-05  3.06591489e-06]\n",
      "Loss at iteration 1231: w=[ 1.81681952 -0.26237297], loss=0.16171617169157246, gradiant=[-1.13026978e-05  3.03769552e-06]\n",
      "Loss at iteration 1232: w=[ 1.81681963 -0.262373  ], loss=0.16171617169020894, gradiant=[-1.11986652e-05  3.00973592e-06]\n",
      "Loss at iteration 1233: w=[ 1.81681974 -0.26237303], loss=0.16171617168887048, gradiant=[-1.10955903e-05  2.98203363e-06]\n",
      "Loss at iteration 1234: w=[ 1.81681985 -0.26237306], loss=0.1617161716875566, gradiant=[-1.09934640e-05  2.95458635e-06]\n",
      "Loss at iteration 1235: w=[ 1.81681996 -0.26237309], loss=0.1617161716862666, gradiant=[-1.08922777e-05  2.92739167e-06]\n",
      "Loss at iteration 1236: w=[ 1.81682007 -0.26237312], loss=0.1617161716850003, gradiant=[-1.07920228e-05  2.90044731e-06]\n",
      "Loss at iteration 1237: w=[ 1.81682017 -0.26237314], loss=0.16171617168375732, gradiant=[-1.06926906e-05  2.87375096e-06]\n",
      "Loss at iteration 1238: w=[ 1.81682028 -0.26237317], loss=0.161716171682537, gradiant=[-1.05942727e-05  2.84730032e-06]\n",
      "Loss at iteration 1239: w=[ 1.81682038 -0.2623732 ], loss=0.16171617168133912, gradiant=[-1.04967607e-05  2.82109313e-06]\n",
      "Loss at iteration 1240: w=[ 1.81682049 -0.26237323], loss=0.16171617168016308, gradiant=[-1.04001462e-05  2.79512718e-06]\n",
      "Loss at iteration 1241: w=[ 1.81682059 -0.26237326], loss=0.16171617167900873, gradiant=[-1.03044210e-05  2.76940019e-06]\n",
      "Loss at iteration 1242: w=[ 1.81682069 -0.26237328], loss=0.1617161716778755, gradiant=[-1.02095768e-05  2.74391004e-06]\n",
      "Loss at iteration 1243: w=[ 1.81682079 -0.26237331], loss=0.16171617167676283, gradiant=[-1.01156056e-05  2.71865447e-06]\n",
      "Loss at iteration 1244: w=[ 1.81682089 -0.26237334], loss=0.16171617167567076, gradiant=[-1.00224993e-05  2.69363138e-06]\n",
      "Loss at iteration 1245: w=[ 1.81682099 -0.26237336], loss=0.1617161716745987, gradiant=[-9.93025001e-06  2.66883861e-06]\n",
      "Loss at iteration 1246: w=[ 1.81682109 -0.26237339], loss=0.1617161716735462, gradiant=[-9.83884980e-06  2.64427401e-06]\n",
      "Loss at iteration 1247: w=[ 1.81682119 -0.26237342], loss=0.1617161716725131, gradiant=[-9.74829084e-06  2.61993555e-06]\n",
      "Loss at iteration 1248: w=[ 1.81682129 -0.26237344], loss=0.16171617167149888, gradiant=[-9.65856543e-06  2.59582106e-06]\n",
      "Loss at iteration 1249: w=[ 1.81682138 -0.26237347], loss=0.16171617167050312, gradiant=[-9.56966585e-06  2.57192857e-06]\n",
      "Loss at iteration 1250: w=[ 1.81682148 -0.26237349], loss=0.1617161716695258, gradiant=[-9.48158453e-06  2.54825596e-06]\n",
      "Loss at iteration 1251: w=[ 1.81682157 -0.26237352], loss=0.1617161716685663, gradiant=[-9.39431393e-06  2.52480126e-06]\n",
      "Loss at iteration 1252: w=[ 1.81682166 -0.26237354], loss=0.16171617166762425, gradiant=[-9.30784659e-06  2.50156241e-06]\n",
      "Loss at iteration 1253: w=[ 1.81682176 -0.26237357], loss=0.16171617166669966, gradiant=[-9.22217511e-06  2.47853751e-06]\n",
      "Loss at iteration 1254: w=[ 1.81682185 -0.26237359], loss=0.16171617166579194, gradiant=[-9.13729218e-06  2.45572446e-06]\n",
      "Loss at iteration 1255: w=[ 1.81682194 -0.26237362], loss=0.16171617166490077, gradiant=[-9.05319051e-06  2.43312147e-06]\n",
      "Loss at iteration 1256: w=[ 1.81682203 -0.26237364], loss=0.16171617166402613, gradiant=[-8.96986296e-06  2.41072643e-06]\n",
      "Loss at iteration 1257: w=[ 1.81682212 -0.26237367], loss=0.16171617166316735, gradiant=[-8.88730234e-06  2.38853763e-06]\n",
      "Loss at iteration 1258: w=[ 1.8168222  -0.26237369], loss=0.16171617166232444, gradiant=[-8.80550166e-06  2.36655294e-06]\n",
      "Loss at iteration 1259: w=[ 1.81682229 -0.26237371], loss=0.16171617166149674, gradiant=[-8.72445386e-06  2.34477073e-06]\n",
      "Loss at iteration 1260: w=[ 1.81682238 -0.26237374], loss=0.16171617166068447, gradiant=[-8.64415208e-06  2.32318889e-06]\n",
      "Loss at iteration 1261: w=[ 1.81682246 -0.26237376], loss=0.16171617165988691, gradiant=[-8.56458937e-06  2.30180580e-06]\n",
      "Loss at iteration 1262: w=[ 1.81682255 -0.26237378], loss=0.16171617165910399, gradiant=[-8.48575902e-06  2.28061941e-06]\n",
      "Loss at iteration 1263: w=[ 1.81682263 -0.26237381], loss=0.16171617165833552, gradiant=[-8.40765420e-06  2.25962813e-06]\n",
      "Loss at iteration 1264: w=[ 1.81682272 -0.26237383], loss=0.16171617165758106, gradiant=[-8.33026830e-06  2.23882998e-06]\n",
      "Loss at iteration 1265: w=[ 1.8168228  -0.26237385], loss=0.16171617165684038, gradiant=[-8.25359466e-06  2.21822332e-06]\n",
      "Loss at iteration 1266: w=[ 1.81682288 -0.26237387], loss=0.16171617165611335, gradiant=[-8.17762676e-06  2.19780628e-06]\n",
      "Loss at iteration 1267: w=[ 1.81682296 -0.26237389], loss=0.1617161716553996, gradiant=[-8.10235807e-06  2.17757721e-06]\n",
      "Loss at iteration 1268: w=[ 1.81682304 -0.26237392], loss=0.16171617165469893, gradiant=[-8.02778218e-06  2.15753429e-06]\n",
      "Loss at iteration 1269: w=[ 1.81682312 -0.26237394], loss=0.1617161716540112, gradiant=[-7.95389270e-06  2.13767587e-06]\n",
      "Loss at iteration 1270: w=[ 1.8168232  -0.26237396], loss=0.1617161716533359, gradiant=[-7.88068331e-06  2.11800023e-06]\n",
      "Loss at iteration 1271: w=[ 1.81682328 -0.26237398], loss=0.1617161716526731, gradiant=[-7.80814776e-06  2.09850568e-06]\n",
      "Loss at iteration 1272: w=[ 1.81682336 -0.262374  ], loss=0.16171617165202243, gradiant=[-7.73627984e-06  2.07919059e-06]\n",
      "Loss at iteration 1273: w=[ 1.81682343 -0.26237402], loss=0.16171617165138358, gradiant=[-7.66507342e-06  2.06005323e-06]\n",
      "Loss at iteration 1274: w=[ 1.81682351 -0.26237404], loss=0.16171617165075647, gradiant=[-7.59452238e-06  2.04109208e-06]\n",
      "Loss at iteration 1275: w=[ 1.81682358 -0.26237406], loss=0.161716171650141, gradiant=[-7.52462073e-06  2.02230538e-06]\n",
      "Loss at iteration 1276: w=[ 1.81682366 -0.26237408], loss=0.1617161716495366, gradiant=[-7.45536244e-06  2.00369167e-06]\n",
      "Loss at iteration 1277: w=[ 1.81682373 -0.2623741 ], loss=0.16171617164894336, gradiant=[-7.38674165e-06  1.98524921e-06]\n",
      "Loss at iteration 1278: w=[ 1.8168238  -0.26237412], loss=0.1617161716483611, gradiant=[-7.31875244e-06  1.96697657e-06]\n",
      "Loss at iteration 1279: w=[ 1.81682388 -0.26237414], loss=0.16171617164778934, gradiant=[-7.25138903e-06  1.94887207e-06]\n",
      "Loss at iteration 1280: w=[ 1.81682395 -0.26237416], loss=0.16171617164722818, gradiant=[-7.18464564e-06  1.93093422e-06]\n",
      "Loss at iteration 1281: w=[ 1.81682402 -0.26237418], loss=0.1617161716466773, gradiant=[-7.11851657e-06  1.91316148e-06]\n",
      "Loss at iteration 1282: w=[ 1.81682409 -0.2623742 ], loss=0.16171617164613633, gradiant=[-7.05299617e-06  1.89555232e-06]\n",
      "Loss at iteration 1283: w=[ 1.81682416 -0.26237422], loss=0.16171617164560553, gradiant=[-6.98807884e-06  1.87810524e-06]\n",
      "Loss at iteration 1284: w=[ 1.81682423 -0.26237423], loss=0.16171617164508417, gradiant=[-6.92375902e-06  1.86081873e-06]\n",
      "Loss at iteration 1285: w=[ 1.8168243  -0.26237425], loss=0.16171617164457258, gradiant=[-6.86003120e-06  1.84369136e-06]\n",
      "Loss at iteration 1286: w=[ 1.81682437 -0.26237427], loss=0.16171617164407037, gradiant=[-6.79688996e-06  1.82672160e-06]\n",
      "Loss at iteration 1287: w=[ 1.81682443 -0.26237429], loss=0.1617161716435772, gradiant=[-6.73432988e-06  1.80990807e-06]\n",
      "Loss at iteration 1288: w=[ 1.8168245  -0.26237431], loss=0.16171617164309324, gradiant=[-6.67234563e-06  1.79324925e-06]\n",
      "Loss at iteration 1289: w=[ 1.81682457 -0.26237433], loss=0.16171617164261815, gradiant=[-6.61093188e-06  1.77674382e-06]\n",
      "Loss at iteration 1290: w=[ 1.81682463 -0.26237434], loss=0.1617161716421516, gradiant=[-6.55008341e-06  1.76039025e-06]\n",
      "Loss at iteration 1291: w=[ 1.8168247  -0.26237436], loss=0.16171617164169372, gradiant=[-6.48979498e-06  1.74418726e-06]\n",
      "Loss at iteration 1292: w=[ 1.81682476 -0.26237438], loss=0.16171617164124422, gradiant=[-6.43006148e-06  1.72813334e-06]\n",
      "Loss at iteration 1293: w=[ 1.81682483 -0.26237439], loss=0.16171617164080299, gradiant=[-6.37087777e-06  1.71222725e-06]\n",
      "Loss at iteration 1294: w=[ 1.81682489 -0.26237441], loss=0.16171617164036967, gradiant=[-6.31223881e-06  1.69646750e-06]\n",
      "Loss at iteration 1295: w=[ 1.81682495 -0.26237443], loss=0.16171617163994442, gradiant=[-6.25413956e-06  1.68085287e-06]\n",
      "Loss at iteration 1296: w=[ 1.81682501 -0.26237444], loss=0.16171617163952698, gradiant=[-6.19657508e-06  1.66538190e-06]\n",
      "Loss at iteration 1297: w=[ 1.81682507 -0.26237446], loss=0.1617161716391172, gradiant=[-6.13954043e-06  1.65005339e-06]\n",
      "Loss at iteration 1298: w=[ 1.81682514 -0.26237448], loss=0.16171617163871485, gradiant=[-6.08303075e-06  1.63486590e-06]\n",
      "Loss at iteration 1299: w=[ 1.8168252  -0.26237449], loss=0.16171617163832, gradiant=[-6.02704118e-06  1.61981826e-06]\n",
      "Loss at iteration 1300: w=[ 1.81682526 -0.26237451], loss=0.16171617163793228, gradiant=[-5.97156697e-06  1.60490907e-06]\n",
      "Loss at iteration 1301: w=[ 1.81682531 -0.26237453], loss=0.16171617163755173, gradiant=[-5.91660334e-06  1.59013716e-06]\n",
      "Loss at iteration 1302: w=[ 1.81682537 -0.26237454], loss=0.16171617163717808, gradiant=[-5.86214562e-06  1.57550115e-06]\n",
      "Loss at iteration 1303: w=[ 1.81682543 -0.26237456], loss=0.1617161716368113, gradiant=[-5.80818913e-06  1.56099992e-06]\n",
      "Loss at iteration 1304: w=[ 1.81682549 -0.26237457], loss=0.16171617163645124, gradiant=[-5.75472928e-06  1.54663211e-06]\n",
      "Loss at iteration 1305: w=[ 1.81682555 -0.26237459], loss=0.16171617163609786, gradiant=[-5.70176147e-06  1.53239659e-06]\n",
      "Loss at iteration 1306: w=[ 1.8168256 -0.2623746], loss=0.16171617163575072, gradiant=[-5.64928121e-06  1.51829205e-06]\n",
      "Loss at iteration 1307: w=[ 1.81682566 -0.26237462], loss=0.16171617163541008, gradiant=[-5.59728397e-06  1.50431737e-06]\n",
      "Loss at iteration 1308: w=[ 1.81682571 -0.26237463], loss=0.1617161716350758, gradiant=[-5.54576533e-06  1.49047129e-06]\n",
      "Loss at iteration 1309: w=[ 1.81682577 -0.26237465], loss=0.1617161716347475, gradiant=[-5.49472088e-06  1.47675267e-06]\n",
      "Loss at iteration 1310: w=[ 1.81682582 -0.26237466], loss=0.16171617163442534, gradiant=[-5.44414626e-06  1.46316030e-06]\n",
      "Loss at iteration 1311: w=[ 1.81682588 -0.26237468], loss=0.16171617163410892, gradiant=[-5.39403713e-06  1.44969307e-06]\n",
      "Loss at iteration 1312: w=[ 1.81682593 -0.26237469], loss=0.16171617163379842, gradiant=[-5.34438923e-06  1.43634975e-06]\n",
      "Loss at iteration 1313: w=[ 1.81682598 -0.26237471], loss=0.16171617163349358, gradiant=[-5.29519828e-06  1.42312929e-06]\n",
      "Loss at iteration 1314: w=[ 1.81682604 -0.26237472], loss=0.16171617163319427, gradiant=[-5.24646011e-06  1.41003048e-06]\n",
      "Loss at iteration 1315: w=[ 1.81682609 -0.26237473], loss=0.16171617163290047, gradiant=[-5.19817053e-06  1.39705227e-06]\n",
      "Loss at iteration 1316: w=[ 1.81682614 -0.26237475], loss=0.1617161716326121, gradiant=[-5.15032542e-06  1.38419345e-06]\n",
      "Loss at iteration 1317: w=[ 1.81682619 -0.26237476], loss=0.16171617163232904, gradiant=[-5.10292068e-06  1.37145308e-06]\n",
      "Loss at iteration 1318: w=[ 1.81682624 -0.26237477], loss=0.16171617163205115, gradiant=[-5.05595228e-06  1.35882988e-06]\n",
      "Loss at iteration 1319: w=[ 1.81682629 -0.26237479], loss=0.1617161716317782, gradiant=[-5.00941617e-06  1.34632294e-06]\n",
      "Loss at iteration 1320: w=[ 1.81682634 -0.2623748 ], loss=0.16171617163151036, gradiant=[-4.96330840e-06  1.33393106e-06]\n",
      "Loss at iteration 1321: w=[ 1.81682639 -0.26237481], loss=0.16171617163124752, gradiant=[-4.91762501e-06  1.32165329e-06]\n",
      "Loss at iteration 1322: w=[ 1.81682644 -0.26237483], loss=0.16171617163098947, gradiant=[-4.87236210e-06  1.30948848e-06]\n",
      "Loss at iteration 1323: w=[ 1.81682649 -0.26237484], loss=0.1617161716307361, gradiant=[-4.82751580e-06  1.29743568e-06]\n",
      "Loss at iteration 1324: w=[ 1.81682653 -0.26237485], loss=0.16171617163048732, gradiant=[-4.78308228e-06  1.28549378e-06]\n",
      "Loss at iteration 1325: w=[ 1.81682658 -0.26237487], loss=0.16171617163024313, gradiant=[-4.73905774e-06  1.27366181e-06]\n",
      "Loss at iteration 1326: w=[ 1.81682663 -0.26237488], loss=0.16171617163000335, gradiant=[-4.69543840e-06  1.26193875e-06]\n",
      "Loss at iteration 1327: w=[ 1.81682668 -0.26237489], loss=0.16171617162976817, gradiant=[-4.65222055e-06  1.25032358e-06]\n",
      "Loss at iteration 1328: w=[ 1.81682672 -0.2623749 ], loss=0.1617161716295371, gradiant=[-4.60940048e-06  1.23881533e-06]\n",
      "Loss at iteration 1329: w=[ 1.81682677 -0.26237492], loss=0.16171617162931035, gradiant=[-4.56697454e-06  1.22741300e-06]\n",
      "Loss at iteration 1330: w=[ 1.81682681 -0.26237493], loss=0.16171617162908786, gradiant=[-4.52493910e-06  1.21611561e-06]\n",
      "Loss at iteration 1331: w=[ 1.81682686 -0.26237494], loss=0.16171617162886925, gradiant=[-4.48329056e-06  1.20492222e-06]\n",
      "Loss at iteration 1332: w=[ 1.8168269  -0.26237495], loss=0.16171617162865468, gradiant=[-4.44202536e-06  1.19383185e-06]\n",
      "Loss at iteration 1333: w=[ 1.81682695 -0.26237496], loss=0.1617161716284441, gradiant=[-4.40113998e-06  1.18284356e-06]\n",
      "Loss at iteration 1334: w=[ 1.81682699 -0.26237498], loss=0.16171617162823732, gradiant=[-4.36063091e-06  1.17195640e-06]\n",
      "Loss at iteration 1335: w=[ 1.81682703 -0.26237499], loss=0.1617161716280344, gradiant=[-4.32049470e-06  1.16116946e-06]\n",
      "Loss at iteration 1336: w=[ 1.81682708 -0.262375  ], loss=0.1617161716278353, gradiant=[-4.28072792e-06  1.15048179e-06]\n",
      "Loss at iteration 1337: w=[ 1.81682712 -0.26237501], loss=0.16171617162763968, gradiant=[-4.24132714e-06  1.13989253e-06]\n",
      "Loss at iteration 1338: w=[ 1.81682716 -0.26237502], loss=0.16171617162744772, gradiant=[-4.20228904e-06  1.12940067e-06]\n",
      "Loss at iteration 1339: w=[ 1.8168272  -0.26237503], loss=0.1617161716272591, gradiant=[-4.16361023e-06  1.11900546e-06]\n",
      "Loss at iteration 1340: w=[ 1.81682724 -0.26237504], loss=0.16171617162707408, gradiant=[-4.12528746e-06  1.10870584e-06]\n",
      "Loss at iteration 1341: w=[ 1.81682728 -0.26237506], loss=0.1617161716268925, gradiant=[-4.08731739e-06  1.09850111e-06]\n",
      "Loss at iteration 1342: w=[ 1.81682732 -0.26237507], loss=0.16171617162671412, gradiant=[-4.04969682e-06  1.08839023e-06]\n",
      "Loss at iteration 1343: w=[ 1.81682736 -0.26237508], loss=0.16171617162653917, gradiant=[-4.01242251e-06  1.07837247e-06]\n",
      "Loss at iteration 1344: w=[ 1.8168274  -0.26237509], loss=0.16171617162636742, gradiant=[-3.97549129e-06  1.06844687e-06]\n",
      "Loss at iteration 1345: w=[ 1.81682744 -0.2623751 ], loss=0.1617161716261986, gradiant=[-3.93889999e-06  1.05861266e-06]\n",
      "Loss at iteration 1346: w=[ 1.81682748 -0.26237511], loss=0.16171617162603313, gradiant=[-3.90264548e-06  1.04886895e-06]\n",
      "Loss at iteration 1347: w=[ 1.81682752 -0.26237512], loss=0.16171617162587057, gradiant=[-3.86672467e-06  1.03921494e-06]\n",
      "Loss at iteration 1348: w=[ 1.81682756 -0.26237513], loss=0.161716171625711, gradiant=[-3.83113448e-06  1.02964976e-06]\n",
      "Loss at iteration 1349: w=[ 1.8168276  -0.26237514], loss=0.1617161716255543, gradiant=[-3.79587187e-06  1.02017264e-06]\n",
      "Loss at iteration 1350: w=[ 1.81682763 -0.26237515], loss=0.16171617162540053, gradiant=[-3.76093383e-06  1.01078274e-06]\n",
      "Loss at iteration 1351: w=[ 1.81682767 -0.26237516], loss=0.16171617162524957, gradiant=[-3.72631737e-06  1.00147928e-06]\n",
      "Loss at iteration 1352: w=[ 1.81682771 -0.26237517], loss=0.16171617162510135, gradiant=[-3.69201952e-06  9.92261443e-07]\n",
      "Loss at iteration 1353: w=[ 1.81682775 -0.26237518], loss=0.16171617162495594, gradiant=[-3.65803735e-06  9.83128442e-07]\n",
      "Loss at iteration 1354: w=[ 1.81682778 -0.26237519], loss=0.16171617162481303, gradiant=[-3.62436796e-06  9.74079531e-07]\n",
      "Loss at iteration 1355: w=[ 1.81682782 -0.2623752 ], loss=0.16171617162467278, gradiant=[-3.59100849e-06  9.65113855e-07]\n",
      "Loss at iteration 1356: w=[ 1.81682785 -0.26237521], loss=0.16171617162453525, gradiant=[-3.55795604e-06  9.56230769e-07]\n",
      "Loss at iteration 1357: w=[ 1.81682789 -0.26237522], loss=0.16171617162440005, gradiant=[-3.52520784e-06  9.47429381e-07]\n",
      "Loss at iteration 1358: w=[ 1.81682792 -0.26237523], loss=0.16171617162426746, gradiant=[-3.49276104e-06  9.38709057e-07]\n",
      "Loss at iteration 1359: w=[ 1.81682796 -0.26237524], loss=0.16171617162413726, gradiant=[-3.46061290e-06  9.30068948e-07]\n",
      "Loss at iteration 1360: w=[ 1.81682799 -0.26237525], loss=0.16171617162400947, gradiant=[-3.42876065e-06  9.21508408e-07]\n",
      "Loss at iteration 1361: w=[ 1.81682803 -0.26237525], loss=0.16171617162388394, gradiant=[-3.39720158e-06  9.13026623e-07]\n",
      "Loss at iteration 1362: w=[ 1.81682806 -0.26237526], loss=0.16171617162376073, gradiant=[-3.36593298e-06  9.04622945e-07]\n",
      "Loss at iteration 1363: w=[ 1.81682809 -0.26237527], loss=0.16171617162363983, gradiant=[-3.3349522e-06  8.9629657e-07]\n",
      "Loss at iteration 1364: w=[ 1.81682813 -0.26237528], loss=0.16171617162352114, gradiant=[-3.30425655e-06  8.88046884e-07]\n",
      "Loss at iteration 1365: w=[ 1.81682816 -0.26237529], loss=0.16171617162340457, gradiant=[-3.27384345e-06  8.79873077e-07]\n",
      "Loss at iteration 1366: w=[ 1.81682819 -0.2623753 ], loss=0.16171617162329027, gradiant=[-3.24371027e-06  8.71774566e-07]\n",
      "Loss at iteration 1367: w=[ 1.81682822 -0.26237531], loss=0.16171617162317786, gradiant=[-3.21385445e-06  8.63750519e-07]\n",
      "Loss at iteration 1368: w=[ 1.81682826 -0.26237532], loss=0.16171617162306767, gradiant=[-3.18427341e-06  8.55800402e-07]\n",
      "Loss at iteration 1369: w=[ 1.81682829 -0.26237532], loss=0.1617161716229595, gradiant=[-3.15496466e-06  8.47923401e-07]\n",
      "Loss at iteration 1370: w=[ 1.81682832 -0.26237533], loss=0.16171617162285315, gradiant=[-3.12592566e-06  8.40118955e-07]\n",
      "Loss at iteration 1371: w=[ 1.81682835 -0.26237534], loss=0.16171617162274893, gradiant=[-3.09715396e-06  8.32386279e-07]\n",
      "Loss at iteration 1372: w=[ 1.81682838 -0.26237535], loss=0.16171617162264662, gradiant=[-3.06864707e-06  8.24724838e-07]\n",
      "Loss at iteration 1373: w=[ 1.81682841 -0.26237536], loss=0.16171617162254603, gradiant=[-3.04040256e-06  8.17133873e-07]\n",
      "Loss at iteration 1374: w=[ 1.81682844 -0.26237537], loss=0.16171617162244747, gradiant=[-3.01241802e-06  8.09612811e-07]\n",
      "Loss at iteration 1375: w=[ 1.81682847 -0.26237537], loss=0.16171617162235058, gradiant=[-2.98469107e-06  8.02160936e-07]\n",
      "Loss at iteration 1376: w=[ 1.8168285  -0.26237538], loss=0.16171617162225552, gradiant=[-2.95721931e-06  7.94777685e-07]\n",
      "Loss at iteration 1377: w=[ 1.81682853 -0.26237539], loss=0.16171617162216217, gradiant=[-2.93000041e-06  7.87462369e-07]\n",
      "Loss at iteration 1378: w=[ 1.81682856 -0.2623754 ], loss=0.16171617162207041, gradiant=[-2.90303204e-06  7.80214388e-07]\n",
      "Loss at iteration 1379: w=[ 1.81682859 -0.26237541], loss=0.16171617162198054, gradiant=[-2.87631189e-06  7.73033127e-07]\n",
      "Loss at iteration 1380: w=[ 1.81682862 -0.26237541], loss=0.16171617162189228, gradiant=[-2.84983768e-06  7.65917950e-07]\n",
      "Loss at iteration 1381: w=[ 1.81682864 -0.26237542], loss=0.16171617162180557, gradiant=[-2.82360714e-06  7.58868282e-07]\n",
      "Loss at iteration 1382: w=[ 1.81682867 -0.26237543], loss=0.16171617162172047, gradiant=[-2.79761804e-06  7.51883492e-07]\n",
      "Loss at iteration 1383: w=[ 1.8168287  -0.26237544], loss=0.161716171621637, gradiant=[-2.77186815e-06  7.44962971e-07]\n",
      "Loss at iteration 1384: w=[ 1.81682873 -0.26237544], loss=0.16171617162155488, gradiant=[-2.74635525e-06  7.38106205e-07]\n",
      "Loss at iteration 1385: w=[ 1.81682875 -0.26237545], loss=0.16171617162147445, gradiant=[-2.72107720e-06  7.31312471e-07]\n",
      "Loss at iteration 1386: w=[ 1.81682878 -0.26237546], loss=0.16171617162139534, gradiant=[-2.69603179e-06  7.24581347e-07]\n",
      "Loss at iteration 1387: w=[ 1.81682881 -0.26237546], loss=0.16171617162131788, gradiant=[-2.67121693e-06  7.17912107e-07]\n",
      "Loss at iteration 1388: w=[ 1.81682883 -0.26237547], loss=0.16171617162124166, gradiant=[-2.64663045e-06  7.11304314e-07]\n",
      "Loss at iteration 1389: w=[ 1.81682886 -0.26237548], loss=0.16171617162116692, gradiant=[-2.62227029e-06  7.04757278e-07]\n",
      "Loss at iteration 1390: w=[ 1.81682889 -0.26237549], loss=0.1617161716210936, gradiant=[-2.59813432e-06  6.98270568e-07]\n",
      "Loss at iteration 1391: w=[ 1.81682891 -0.26237549], loss=0.16171617162102148, gradiant=[-2.57422053e-06  6.91843507e-07]\n",
      "Loss at iteration 1392: w=[ 1.81682894 -0.2623755 ], loss=0.1617161716209508, gradiant=[-2.55052683e-06  6.85475651e-07]\n",
      "Loss at iteration 1393: w=[ 1.81682896 -0.26237551], loss=0.1617161716208813, gradiant=[-2.52705122e-06  6.79166364e-07]\n",
      "Loss at iteration 1394: w=[ 1.81682899 -0.26237551], loss=0.16171617162081317, gradiant=[-2.50379168e-06  6.72915174e-07]\n",
      "Loss at iteration 1395: w=[ 1.81682901 -0.26237552], loss=0.16171617162074625, gradiant=[-2.48074623e-06  6.66721513e-07]\n",
      "Loss at iteration 1396: w=[ 1.81682904 -0.26237553], loss=0.1617161716206806, gradiant=[-2.45791289e-06  6.60584860e-07]\n",
      "Loss at iteration 1397: w=[ 1.81682906 -0.26237553], loss=0.1617161716206161, gradiant=[-2.43528972e-06  6.54504693e-07]\n",
      "Loss at iteration 1398: w=[ 1.81682909 -0.26237554], loss=0.16171617162055282, gradiant=[-2.41287477e-06  6.48480481e-07]\n",
      "Loss at iteration 1399: w=[ 1.81682911 -0.26237555], loss=0.16171617162049062, gradiant=[-2.39066614e-06  6.42511726e-07]\n",
      "Loss at iteration 1400: w=[ 1.81682913 -0.26237555], loss=0.16171617162042976, gradiant=[-2.36866192e-06  6.36597900e-07]\n",
      "Loss at iteration 1401: w=[ 1.81682916 -0.26237556], loss=0.1617161716203698, gradiant=[-2.34686023e-06  6.30738515e-07]\n",
      "Loss at iteration 1402: w=[ 1.81682918 -0.26237556], loss=0.16171617162031104, gradiant=[-2.32525921e-06  6.24933064e-07]\n",
      "Loss at iteration 1403: w=[ 1.8168292  -0.26237557], loss=0.16171617162025334, gradiant=[-2.30385701e-06  6.19181030e-07]\n",
      "Loss at iteration 1404: w=[ 1.81682923 -0.26237558], loss=0.16171617162019664, gradiant=[-2.28265180e-06  6.13481974e-07]\n",
      "Loss at iteration 1405: w=[ 1.81682925 -0.26237558], loss=0.161716171620141, gradiant=[-2.26164178e-06  6.07835320e-07]\n",
      "Loss at iteration 1406: w=[ 1.81682927 -0.26237559], loss=0.1617161716200864, gradiant=[-2.24082512e-06  6.02240704e-07]\n",
      "Loss at iteration 1407: w=[ 1.81682929 -0.2623756 ], loss=0.16171617162003282, gradiant=[-2.22020007e-06  5.96697521e-07]\n",
      "Loss at iteration 1408: w=[ 1.81682932 -0.2623756 ], loss=0.16171617161998014, gradiant=[-2.19976486e-06  5.91205403e-07]\n",
      "Loss at iteration 1409: w=[ 1.81682934 -0.26237561], loss=0.16171617161992863, gradiant=[-2.17951774e-06  5.85763804e-07]\n",
      "Loss at iteration 1410: w=[ 1.81682936 -0.26237561], loss=0.16171617161987795, gradiant=[-2.15945697e-06  5.80372316e-07]\n",
      "Loss at iteration 1411: w=[ 1.81682938 -0.26237562], loss=0.16171617161982815, gradiant=[-2.13958086e-06  5.75030424e-07]\n",
      "Loss at iteration 1412: w=[ 1.8168294  -0.26237562], loss=0.16171617161977925, gradiant=[-2.11988768e-06  5.69737732e-07]\n",
      "Loss at iteration 1413: w=[ 1.81682942 -0.26237563], loss=0.1617161716197314, gradiant=[-2.10037576e-06  5.64493739e-07]\n",
      "Loss at iteration 1414: w=[ 1.81682944 -0.26237564], loss=0.16171617161968418, gradiant=[-2.08104344e-06  5.59298003e-07]\n",
      "Loss at iteration 1415: w=[ 1.81682946 -0.26237564], loss=0.16171617161963803, gradiant=[-2.06188906e-06  5.54150116e-07]\n",
      "Loss at iteration 1416: w=[ 1.81682948 -0.26237565], loss=0.16171617161959267, gradiant=[-2.04291098e-06  5.49049583e-07]\n",
      "Loss at iteration 1417: w=[ 1.8168295  -0.26237565], loss=0.1617161716195481, gradiant=[-2.02410757e-06  5.43996015e-07]\n",
      "Loss at iteration 1418: w=[ 1.81682952 -0.26237566], loss=0.16171617161950436, gradiant=[-2.00547724e-06  5.38988952e-07]\n",
      "Loss at iteration 1419: w=[ 1.81682954 -0.26237566], loss=0.1617161716194614, gradiant=[-1.98701838e-06  5.34027975e-07]\n",
      "Loss at iteration 1420: w=[ 1.81682956 -0.26237567], loss=0.16171617161941926, gradiant=[-1.96872943e-06  5.29112664e-07]\n",
      "Loss at iteration 1421: w=[ 1.81682958 -0.26237567], loss=0.16171617161937796, gradiant=[-1.9506088e-06  5.2424260e-07]\n",
      "Loss at iteration 1422: w=[ 1.8168296  -0.26237568], loss=0.16171617161933727, gradiant=[-1.93265497e-06  5.19417356e-07]\n",
      "Loss at iteration 1423: w=[ 1.81682962 -0.26237568], loss=0.1617161716192974, gradiant=[-1.91486639e-06  5.14636514e-07]\n",
      "Loss at iteration 1424: w=[ 1.81682964 -0.26237569], loss=0.16171617161925828, gradiant=[-1.89724153e-06  5.09899711e-07]\n",
      "Loss at iteration 1425: w=[ 1.81682966 -0.26237569], loss=0.16171617161921992, gradiant=[-1.87977891e-06  5.05206442e-07]\n",
      "Loss at iteration 1426: w=[ 1.81682968 -0.2623757 ], loss=0.16171617161918223, gradiant=[-1.86247700e-06  5.00556452e-07]\n",
      "Loss at iteration 1427: w=[ 1.8168297 -0.2623757], loss=0.16171617161914517, gradiant=[-1.84533436e-06  4.95949180e-07]\n",
      "Loss at iteration 1428: w=[ 1.81682972 -0.26237571], loss=0.16171617161910884, gradiant=[-1.82834948e-06  4.91384388e-07]\n",
      "Loss at iteration 1429: w=[ 1.81682973 -0.26237571], loss=0.16171617161907315, gradiant=[-1.81152096e-06  4.86861543e-07]\n",
      "Loss at iteration 1430: w=[ 1.81682975 -0.26237572], loss=0.16171617161903806, gradiant=[-1.79484731e-06  4.82380396e-07]\n",
      "Loss at iteration 1431: w=[ 1.81682977 -0.26237572], loss=0.16171617161900378, gradiant=[-1.77832715e-06  4.77940433e-07]\n",
      "Loss at iteration 1432: w=[ 1.81682979 -0.26237573], loss=0.16171617161896995, gradiant=[-1.76195902e-06  4.73541395e-07]\n",
      "Loss at iteration 1433: w=[ 1.8168298  -0.26237573], loss=0.1617161716189368, gradiant=[-1.74574157e-06  4.69182781e-07]\n",
      "Loss at iteration 1434: w=[ 1.81682982 -0.26237574], loss=0.16171617161890436, gradiant=[-1.72967337e-06  4.64864351e-07]\n",
      "Loss at iteration 1435: w=[ 1.81682984 -0.26237574], loss=0.1617161716188723, gradiant=[-1.71375309e-06  4.60585598e-07]\n",
      "Loss at iteration 1436: w=[ 1.81682986 -0.26237575], loss=0.16171617161884105, gradiant=[-1.69797931e-06  4.56346317e-07]\n",
      "Loss at iteration 1437: w=[ 1.81682987 -0.26237575], loss=0.16171617161881027, gradiant=[-1.68235075e-06  4.52145953e-07]\n",
      "Loss at iteration 1438: w=[ 1.81682989 -0.26237576], loss=0.1617161716187801, gradiant=[-1.66686601e-06  4.47984347e-07]\n",
      "Loss at iteration 1439: w=[ 1.81682991 -0.26237576], loss=0.1617161716187504, gradiant=[-1.65152382e-06  4.43860951e-07]\n",
      "Loss at iteration 1440: w=[ 1.81682992 -0.26237576], loss=0.16171617161872126, gradiant=[-1.63632281e-06  4.39775602e-07]\n",
      "Loss at iteration 1441: w=[ 1.81682994 -0.26237577], loss=0.16171617161869276, gradiant=[-1.62126175e-06  4.35727768e-07]\n",
      "Loss at iteration 1442: w=[ 1.81682995 -0.26237577], loss=0.16171617161866464, gradiant=[-1.60633929e-06  4.31717277e-07]\n",
      "Loss at iteration 1443: w=[ 1.81682997 -0.26237578], loss=0.16171617161863716, gradiant=[-1.59155419e-06  4.27743614e-07]\n",
      "Loss at iteration 1444: w=[ 1.81682999 -0.26237578], loss=0.16171617161861013, gradiant=[-1.57690516e-06  4.23806616e-07]\n",
      "Loss at iteration 1445: w=[ 1.81683    -0.26237579], loss=0.1617161716185836, gradiant=[-1.56239100e-06  4.19905751e-07]\n",
      "Loss at iteration 1446: w=[ 1.81683002 -0.26237579], loss=0.16171617161855745, gradiant=[-1.54801039e-06  4.16040905e-07]\n",
      "Loss at iteration 1447: w=[ 1.81683003 -0.26237579], loss=0.161716171618532, gradiant=[-1.53376218e-06  4.12211508e-07]\n",
      "Loss at iteration 1448: w=[ 1.81683005 -0.2623758 ], loss=0.16171617161850688, gradiant=[-1.51964507e-06  4.08417491e-07]\n",
      "Loss at iteration 1449: w=[ 1.81683006 -0.2623758 ], loss=0.1617161716184821, gradiant=[-1.50565794e-06  4.04658266e-07]\n",
      "Loss at iteration 1450: w=[ 1.81683008 -0.26237581], loss=0.16171617161845797, gradiant=[-1.49179952e-06  4.00933745e-07]\n",
      "Loss at iteration 1451: w=[ 1.81683009 -0.26237581], loss=0.16171617161843424, gradiant=[-1.47806867e-06  3.97243428e-07]\n",
      "Loss at iteration 1452: w=[ 1.81683011 -0.26237581], loss=0.16171617161841098, gradiant=[-1.46446420e-06  3.93587142e-07]\n",
      "Loss at iteration 1453: w=[ 1.81683012 -0.26237582], loss=0.16171617161838808, gradiant=[-1.45098495e-06  3.89964454e-07]\n",
      "Loss at iteration 1454: w=[ 1.81683014 -0.26237582], loss=0.16171617161836555, gradiant=[-1.43762976e-06  3.86375157e-07]\n",
      "Loss at iteration 1455: w=[ 1.81683015 -0.26237583], loss=0.16171617161834345, gradiant=[-1.42439750e-06  3.82818845e-07]\n",
      "Loss at iteration 1456: w=[ 1.81683016 -0.26237583], loss=0.16171617161832189, gradiant=[-1.41128702e-06  3.79295338e-07]\n",
      "Loss at iteration 1457: w=[ 1.81683018 -0.26237583], loss=0.1617161716183006, gradiant=[-1.39829724e-06  3.75804170e-07]\n",
      "Loss at iteration 1458: w=[ 1.81683019 -0.26237584], loss=0.1617161716182797, gradiant=[-1.38542699e-06  3.72345232e-07]\n",
      "Loss at iteration 1459: w=[ 1.81683021 -0.26237584], loss=0.16171617161825924, gradiant=[-1.37267522e-06  3.68918047e-07]\n",
      "Loss at iteration 1460: w=[ 1.81683022 -0.26237584], loss=0.16171617161823904, gradiant=[-1.36004081e-06  3.65522477e-07]\n",
      "Loss at iteration 1461: w=[ 1.81683023 -0.26237585], loss=0.16171617161821944, gradiant=[-1.34752270e-06  3.62158093e-07]\n",
      "Loss at iteration 1462: w=[ 1.81683025 -0.26237585], loss=0.1617161716182, gradiant=[-1.33511980e-06  3.58824742e-07]\n",
      "Loss at iteration 1463: w=[ 1.81683026 -0.26237585], loss=0.16171617161818092, gradiant=[-1.32283107e-06  3.55522011e-07]\n",
      "Loss at iteration 1464: w=[ 1.81683027 -0.26237586], loss=0.16171617161816232, gradiant=[-1.31065543e-06  3.52249735e-07]\n",
      "Loss at iteration 1465: w=[ 1.81683029 -0.26237586], loss=0.1617161716181439, gradiant=[-1.29859187e-06  3.49007527e-07]\n",
      "Loss at iteration 1466: w=[ 1.8168303  -0.26237587], loss=0.16171617161812596, gradiant=[-1.28663934e-06  3.45795206e-07]\n",
      "Loss at iteration 1467: w=[ 1.81683031 -0.26237587], loss=0.1617161716181083, gradiant=[-1.27479683e-06  3.42612419e-07]\n",
      "Loss at iteration 1468: w=[ 1.81683032 -0.26237587], loss=0.16171617161809104, gradiant=[-1.26306332e-06  3.39458932e-07]\n",
      "Loss at iteration 1469: w=[ 1.81683034 -0.26237588], loss=0.16171617161807386, gradiant=[-1.2514378e-06  3.3633449e-07]\n",
      "Loss at iteration 1470: w=[ 1.81683035 -0.26237588], loss=0.16171617161805715, gradiant=[-1.23991930e-06  3.33238784e-07]\n",
      "Loss at iteration 1471: w=[ 1.81683036 -0.26237588], loss=0.16171617161804078, gradiant=[-1.22850680e-06  3.30171585e-07]\n",
      "Loss at iteration 1472: w=[ 1.81683037 -0.26237589], loss=0.16171617161802476, gradiant=[-1.21719935e-06  3.27132613e-07]\n",
      "Loss at iteration 1473: w=[ 1.81683038 -0.26237589], loss=0.1617161716180088, gradiant=[-1.20599598e-06  3.24121610e-07]\n",
      "Loss at iteration 1474: w=[ 1.8168304  -0.26237589], loss=0.1617161716179933, gradiant=[-1.19489573e-06  3.21138325e-07]\n",
      "Loss at iteration 1475: w=[ 1.81683041 -0.2623759 ], loss=0.16171617161797813, gradiant=[-1.18389765e-06  3.18182494e-07]\n",
      "Loss at iteration 1476: w=[ 1.81683042 -0.2623759 ], loss=0.1617161716179632, gradiant=[-1.17300079e-06  3.15253872e-07]\n",
      "Loss at iteration 1477: w=[ 1.81683043 -0.2623759 ], loss=0.16171617161794838, gradiant=[-1.16220423e-06  3.12352206e-07]\n",
      "Loss at iteration 1478: w=[ 1.81683044 -0.2623759 ], loss=0.161716171617934, gradiant=[-1.15150705e-06  3.09477246e-07]\n",
      "Loss at iteration 1479: w=[ 1.81683046 -0.26237591], loss=0.16171617161791993, gradiant=[-1.14090832e-06  3.06628758e-07]\n",
      "Loss at iteration 1480: w=[ 1.81683047 -0.26237591], loss=0.161716171617906, gradiant=[-1.13040715e-06  3.03806473e-07]\n",
      "Loss at iteration 1481: w=[ 1.81683048 -0.26237591], loss=0.1617161716178923, gradiant=[-1.12000263e-06  3.01010168e-07]\n",
      "Loss at iteration 1482: w=[ 1.81683049 -0.26237592], loss=0.16171617161787896, gradiant=[-1.10969388e-06  2.98239607e-07]\n",
      "Loss at iteration 1483: w=[ 1.8168305  -0.26237592], loss=0.16171617161786575, gradiant=[-1.09948001e-06  2.95494543e-07]\n",
      "Loss at iteration 1484: w=[ 1.81683051 -0.26237592], loss=0.1617161716178529, gradiant=[-1.08936015e-06  2.92774745e-07]\n",
      "Loss at iteration 1485: w=[ 1.81683052 -0.26237593], loss=0.16171617161784035, gradiant=[-1.07933344e-06  2.90079989e-07]\n",
      "Loss at iteration 1486: w=[ 1.81683053 -0.26237593], loss=0.16171617161782778, gradiant=[-1.06939902e-06  2.87410021e-07]\n",
      "Loss at iteration 1487: w=[ 1.81683054 -0.26237593], loss=0.16171617161781557, gradiant=[-1.05955604e-06  2.84764632e-07]\n",
      "Loss at iteration 1488: w=[ 1.81683055 -0.26237593], loss=0.1617161716178036, gradiant=[-1.04980364e-06  2.82143606e-07]\n",
      "Loss at iteration 1489: w=[ 1.81683056 -0.26237594], loss=0.16171617161779178, gradiant=[-1.04014102e-06  2.79546678e-07]\n",
      "Loss at iteration 1490: w=[ 1.81683057 -0.26237594], loss=0.16171617161778032, gradiant=[-1.03056733e-06  2.76973682e-07]\n",
      "Loss at iteration 1491: w=[ 1.81683058 -0.26237594], loss=0.1617161716177689, gradiant=[-1.02108176e-06  2.74424353e-07]\n",
      "Loss at iteration 1492: w=[ 1.81683059 -0.26237594], loss=0.16171617161775786, gradiant=[-1.01168350e-06  2.71898489e-07]\n",
      "Loss at iteration 1493: w=[ 1.8168306  -0.26237595], loss=0.1617161716177469, gradiant=[-1.00237174e-06  2.69395876e-07]\n",
      "Loss at iteration 1494: w=[ 1.81683061 -0.26237595], loss=0.16171617161773622, gradiant=[-9.93145693e-07  2.66916292e-07]\n",
      "Loss at iteration 1495: w=[ 1.81683062 -0.26237595], loss=0.16171617161772564, gradiant=[-9.84004559e-07  2.64459543e-07]\n",
      "Loss at iteration 1496: w=[ 1.81683063 -0.26237596], loss=0.16171617161771532, gradiant=[-9.74947564e-07  2.62025397e-07]\n",
      "Loss at iteration 1497: w=[ 1.81683064 -0.26237596], loss=0.1617161716177052, gradiant=[-9.65973930e-07  2.59613659e-07]\n",
      "Loss at iteration 1498: w=[ 1.81683065 -0.26237596], loss=0.1617161716176952, gradiant=[-9.57082895e-07  2.57224110e-07]\n",
      "Loss at iteration 1499: w=[ 1.81683066 -0.26237596], loss=0.1617161716176854, gradiant=[-9.48273688e-07  2.54856578e-07]\n",
      "Loss at iteration 1500: w=[ 1.81683067 -0.26237597], loss=0.16171617161767582, gradiant=[-9.39545575e-07  2.52510794e-07]\n",
      "Loss at iteration 1501: w=[ 1.81683068 -0.26237597], loss=0.16171617161766647, gradiant=[-9.30897782e-07  2.50186658e-07]\n",
      "Loss at iteration 1502: w=[ 1.81683069 -0.26237597], loss=0.16171617161765725, gradiant=[-9.22329601e-07  2.47883857e-07]\n",
      "Loss at iteration 1503: w=[ 1.8168307  -0.26237597], loss=0.16171617161764815, gradiant=[-9.13840269e-07  2.45602303e-07]\n",
      "Loss at iteration 1504: w=[ 1.81683071 -0.26237598], loss=0.16171617161763926, gradiant=[-9.05429086e-07  2.43341706e-07]\n",
      "Loss at iteration 1505: w=[ 1.81683072 -0.26237598], loss=0.16171617161763038, gradiant=[-8.97095312e-07  2.41101954e-07]\n",
      "Loss at iteration 1506: w=[ 1.81683073 -0.26237598], loss=0.16171617161762183, gradiant=[-8.88838253e-07  2.38882778e-07]\n",
      "Loss at iteration 1507: w=[ 1.81683074 -0.26237598], loss=0.1617161716176134, gradiant=[-8.80657185e-07  2.36684066e-07]\n",
      "Loss at iteration 1508: w=[ 1.81683074 -0.26237599], loss=0.16171617161760513, gradiant=[-8.72551424e-07  2.34505565e-07]\n",
      "Loss at iteration 1509: w=[ 1.81683075 -0.26237599], loss=0.16171617161759705, gradiant=[-8.64520265e-07  2.32347132e-07]\n",
      "Loss at iteration 1510: w=[ 1.81683076 -0.26237599], loss=0.1617161716175891, gradiant=[-8.56563033e-07  2.30208546e-07]\n",
      "Loss at iteration 1511: w=[ 1.81683077 -0.26237599], loss=0.16171617161758128, gradiant=[-8.48679033e-07  2.28089671e-07]\n",
      "Loss at iteration 1512: w=[ 1.81683078 -0.26237599], loss=0.1617161716175735, gradiant=[-8.40867607e-07  2.25990273e-07]\n",
      "Loss at iteration 1513: w=[ 1.81683079 -0.262376  ], loss=0.16171617161756596, gradiant=[-8.33128075e-07  2.23910210e-07]\n",
      "Loss at iteration 1514: w=[ 1.81683079 -0.262376  ], loss=0.1617161716175585, gradiant=[-8.25459781e-07  2.21849288e-07]\n",
      "Loss at iteration 1515: w=[ 1.8168308 -0.262376 ], loss=0.16171617161755136, gradiant=[-8.17862064e-07  2.19807346e-07]\n",
      "Loss at iteration 1516: w=[ 1.81683081 -0.262376  ], loss=0.16171617161754415, gradiant=[-8.10334282e-07  2.17784185e-07]\n",
      "Loss at iteration 1517: w=[ 1.81683082 -0.26237601], loss=0.16171617161753715, gradiant=[-8.02875786e-07  2.15779655e-07]\n",
      "Loss at iteration 1518: w=[ 1.81683083 -0.26237601], loss=0.1617161716175302, gradiant=[-7.95485941e-07  2.13793566e-07]\n",
      "Loss at iteration 1519: w=[ 1.81683083 -0.26237601], loss=0.16171617161752355, gradiant=[-7.88164111e-07  2.11825768e-07]\n",
      "Loss at iteration 1520: w=[ 1.81683084 -0.26237601], loss=0.16171617161751686, gradiant=[-7.80909676e-07  2.09876071e-07]\n",
      "Loss at iteration 1521: w=[ 1.81683085 -0.26237601], loss=0.16171617161751042, gradiant=[-7.73722010e-07  2.07944326e-07]\n",
      "Loss at iteration 1522: w=[ 1.81683086 -0.26237602], loss=0.16171617161750404, gradiant=[-7.66600501e-07  2.06030365e-07]\n",
      "Loss at iteration 1523: w=[ 1.81683087 -0.26237602], loss=0.1617161716174978, gradiant=[-7.59544543e-07  2.04134007e-07]\n",
      "Loss at iteration 1524: w=[ 1.81683087 -0.26237602], loss=0.16171617161749155, gradiant=[-7.52553523e-07  2.02255128e-07]\n",
      "Loss at iteration 1525: w=[ 1.81683088 -0.26237602], loss=0.16171617161748558, gradiant=[-7.45626858e-07  2.00393511e-07]\n",
      "Loss at iteration 1526: w=[ 1.81683089 -0.26237602], loss=0.16171617161747956, gradiant=[-7.38763942e-07  1.98549052e-07]\n",
      "Loss at iteration 1527: w=[ 1.8168309  -0.26237603], loss=0.16171617161747381, gradiant=[-7.31964194e-07  1.96721565e-07]\n",
      "Loss at iteration 1528: w=[ 1.8168309  -0.26237603], loss=0.16171617161746815, gradiant=[-7.25227036e-07  1.94910890e-07]\n",
      "Loss at iteration 1529: w=[ 1.81683091 -0.26237603], loss=0.1617161716174624, gradiant=[-7.18551886e-07  1.93116889e-07]\n",
      "Loss at iteration 1530: w=[ 1.81683092 -0.26237603], loss=0.16171617161745694, gradiant=[-7.11938174e-07  1.91339401e-07]\n",
      "Loss at iteration 1531: w=[ 1.81683092 -0.26237603], loss=0.16171617161745155, gradiant=[-7.05385338e-07  1.89578272e-07]\n",
      "Loss at iteration 1532: w=[ 1.81683093 -0.26237604], loss=0.16171617161744611, gradiant=[-6.98892816e-07  1.87833348e-07]\n",
      "Loss at iteration 1533: w=[ 1.81683094 -0.26237604], loss=0.16171617161744092, gradiant=[-6.92460053e-07  1.86104487e-07]\n",
      "Loss at iteration 1534: w=[ 1.81683094 -0.26237604], loss=0.16171617161743582, gradiant=[-6.86086497e-07  1.84391542e-07]\n",
      "Loss at iteration 1535: w=[ 1.81683095 -0.26237604], loss=0.1617161716174309, gradiant=[-6.79771604e-07  1.82694366e-07]\n",
      "Loss at iteration 1536: w=[ 1.81683096 -0.26237604], loss=0.161716171617426, gradiant=[-6.73514839e-07  1.81012797e-07]\n",
      "Loss at iteration 1537: w=[ 1.81683096 -0.26237604], loss=0.16171617161742108, gradiant=[-6.67315656e-07  1.79346727e-07]\n",
      "Loss at iteration 1538: w=[ 1.81683097 -0.26237605], loss=0.1617161716174163, gradiant=[-6.61173539e-07  1.77695967e-07]\n",
      "Loss at iteration 1539: w=[ 1.81683098 -0.26237605], loss=0.16171617161741164, gradiant=[-6.55087948e-07  1.76060425e-07]\n",
      "Loss at iteration 1540: w=[ 1.81683098 -0.26237605], loss=0.16171617161740717, gradiant=[-6.49058378e-07  1.74439911e-07]\n",
      "Loss at iteration 1541: w=[ 1.81683099 -0.26237605], loss=0.16171617161740265, gradiant=[-6.43084295e-07  1.72834351e-07]\n",
      "Loss at iteration 1542: w=[ 1.816831   -0.26237605], loss=0.1617161716173982, gradiant=[-6.37165212e-07  1.71243520e-07]\n",
      "Loss at iteration 1543: w=[ 1.816831   -0.26237605], loss=0.1617161716173938, gradiant=[-6.31300595e-07  1.69667385e-07]\n",
      "Loss at iteration 1544: w=[ 1.81683101 -0.26237606], loss=0.1617161716173896, gradiant=[-6.25489973e-07  1.68105696e-07]\n",
      "Loss at iteration 1545: w=[ 1.81683102 -0.26237606], loss=0.16171617161738544, gradiant=[-6.19732816e-07  1.66558448e-07]\n",
      "Loss at iteration 1546: w=[ 1.81683102 -0.26237606], loss=0.16171617161738128, gradiant=[-6.14028667e-07  1.65025375e-07]\n",
      "Loss at iteration 1547: w=[ 1.81683103 -0.26237606], loss=0.16171617161737734, gradiant=[-6.08377003e-07  1.63506476e-07]\n",
      "Loss at iteration 1548: w=[ 1.81683103 -0.26237606], loss=0.16171617161737328, gradiant=[-6.02777375e-07  1.62001495e-07]\n",
      "Loss at iteration 1549: w=[ 1.81683104 -0.26237606], loss=0.16171617161736956, gradiant=[-5.97229270e-07  1.60510433e-07]\n",
      "Loss at iteration 1550: w=[ 1.81683105 -0.26237607], loss=0.16171617161736568, gradiant=[-5.91732249e-07  1.59033022e-07]\n",
      "Loss at iteration 1551: w=[ 1.81683105 -0.26237607], loss=0.16171617161736193, gradiant=[-5.86285805e-07  1.57569282e-07]\n",
      "Loss at iteration 1552: w=[ 1.81683106 -0.26237607], loss=0.1617161716173583, gradiant=[-5.80889510e-07  1.56118948e-07]\n",
      "Loss at iteration 1553: w=[ 1.81683106 -0.26237607], loss=0.1617161716173547, gradiant=[-5.75542865e-07  1.54682030e-07]\n",
      "Loss at iteration 1554: w=[ 1.81683107 -0.26237607], loss=0.16171617161735113, gradiant=[-5.70245452e-07  1.53258263e-07]\n",
      "Loss at iteration 1555: w=[ 1.81683107 -0.26237607], loss=0.1617161716173477, gradiant=[-5.64996776e-07  1.51847680e-07]\n",
      "Loss at iteration 1556: w=[ 1.81683108 -0.26237608], loss=0.16171617161734425, gradiant=[-5.59796430e-07  1.50450005e-07]\n",
      "Loss at iteration 1557: w=[ 1.81683109 -0.26237608], loss=0.1617161716173408, gradiant=[-5.54643931e-07  1.49065265e-07]\n",
      "Loss at iteration 1558: w=[ 1.81683109 -0.26237608], loss=0.16171617161733756, gradiant=[-5.49538877e-07  1.47693193e-07]\n",
      "Loss at iteration 1559: w=[ 1.8168311  -0.26237608], loss=0.16171617161733443, gradiant=[-5.44480786e-07  1.46333841e-07]\n",
      "Loss at iteration 1560: w=[ 1.8168311  -0.26237608], loss=0.16171617161733115, gradiant=[-5.39469277e-07  1.44986907e-07]\n",
      "Loss at iteration 1561: w=[ 1.81683111 -0.26237608], loss=0.1617161716173281, gradiant=[-5.34503873e-07  1.43652449e-07]\n",
      "Loss at iteration 1562: w=[ 1.81683111 -0.26237608], loss=0.161716171617325, gradiant=[-5.29584192e-07  1.42330202e-07]\n",
      "Loss at iteration 1563: w=[ 1.81683112 -0.26237609], loss=0.16171617161732216, gradiant=[-5.24709770e-07  1.41020208e-07]\n",
      "Loss at iteration 1564: w=[ 1.81683112 -0.26237609], loss=0.16171617161731916, gradiant=[-5.19880237e-07  1.39722184e-07]\n",
      "Loss at iteration 1565: w=[ 1.81683113 -0.26237609], loss=0.16171617161731622, gradiant=[-5.15095134e-07  1.38436187e-07]\n",
      "Loss at iteration 1566: w=[ 1.81683113 -0.26237609], loss=0.16171617161731341, gradiant=[-5.10354091e-07  1.37161968e-07]\n",
      "Loss at iteration 1567: w=[ 1.81683114 -0.26237609], loss=0.16171617161731064, gradiant=[-5.05656677e-07  1.35899506e-07]\n",
      "Loss at iteration 1568: w=[ 1.81683114 -0.26237609], loss=0.16171617161730795, gradiant=[-5.01002504e-07  1.34648648e-07]\n",
      "Loss at iteration 1569: w=[ 1.81683115 -0.26237609], loss=0.16171617161730514, gradiant=[-4.96391161e-07  1.33409330e-07]\n",
      "Loss at iteration 1570: w=[ 1.81683115 -0.2623761 ], loss=0.1617161716173025, gradiant=[-4.91822273e-07  1.32181379e-07]\n",
      "Loss at iteration 1571: w=[ 1.81683116 -0.2623761 ], loss=0.16171617161729993, gradiant=[-4.87295425e-07  1.30964779e-07]\n",
      "Loss at iteration 1572: w=[ 1.81683116 -0.2623761 ], loss=0.1617161716172975, gradiant=[-4.82810257e-07  1.29759323e-07]\n",
      "Loss at iteration 1573: w=[ 1.81683117 -0.2623761 ], loss=0.16171617161729496, gradiant=[-4.7836636e-07  1.2856501e-07]\n",
      "Loss at iteration 1574: w=[ 1.81683117 -0.2623761 ], loss=0.16171617161729246, gradiant=[-4.73963372e-07  1.27381661e-07]\n",
      "Loss at iteration 1575: w=[ 1.81683118 -0.2623761 ], loss=0.16171617161729018, gradiant=[-4.69600907e-07  1.26209215e-07]\n",
      "Loss at iteration 1576: w=[ 1.81683118 -0.2623761 ], loss=0.16171617161728785, gradiant=[-4.65278597e-07  1.25047555e-07]\n",
      "Loss at iteration 1577: w=[ 1.81683119 -0.2623761 ], loss=0.16171617161728555, gradiant=[-4.60996072e-07  1.23896585e-07]\n",
      "Loss at iteration 1578: w=[ 1.81683119 -0.26237611], loss=0.16171617161728324, gradiant=[-4.56752962e-07  1.22756214e-07]\n",
      "Loss at iteration 1579: w=[ 1.8168312  -0.26237611], loss=0.16171617161728097, gradiant=[-4.52548905e-07  1.21626345e-07]\n",
      "Loss at iteration 1580: w=[ 1.8168312  -0.26237611], loss=0.1617161716172789, gradiant=[-4.48383547e-07  1.20506861e-07]\n",
      "Loss at iteration 1581: w=[ 1.8168312  -0.26237611], loss=0.16171617161727658, gradiant=[-4.44256521e-07  1.19397706e-07]\n",
      "Loss at iteration 1582: w=[ 1.81683121 -0.26237611], loss=0.16171617161727458, gradiant=[-4.40167493e-07  1.18298716e-07]\n",
      "Loss at iteration 1583: w=[ 1.81683121 -0.26237611], loss=0.16171617161727248, gradiant=[-4.36116084e-07  1.17209905e-07]\n",
      "Loss at iteration 1584: w=[ 1.81683122 -0.26237611], loss=0.1617161716172705, gradiant=[-4.32101986e-07  1.16131039e-07]\n",
      "Loss at iteration 1585: w=[ 1.81683122 -0.26237611], loss=0.16171617161726837, gradiant=[-4.28124814e-07  1.15062179e-07]\n",
      "Loss at iteration 1586: w=[ 1.81683123 -0.26237611], loss=0.16171617161726648, gradiant=[-4.24184265e-07  1.14003099e-07]\n",
      "Loss at iteration 1587: w=[ 1.81683123 -0.26237612], loss=0.16171617161726454, gradiant=[-4.20279975e-07  1.12953807e-07]\n",
      "Loss at iteration 1588: w=[ 1.81683123 -0.26237612], loss=0.16171617161726262, gradiant=[-4.16411632e-07  1.11914128e-07]\n",
      "Loss at iteration 1589: w=[ 1.81683124 -0.26237612], loss=0.16171617161726085, gradiant=[-4.12578878e-07  1.10884079e-07]\n",
      "Loss at iteration 1590: w=[ 1.81683124 -0.26237612], loss=0.16171617161725904, gradiant=[-4.08781420e-07  1.09863443e-07]\n",
      "Loss at iteration 1591: w=[ 1.81683125 -0.26237612], loss=0.1617161716172572, gradiant=[-4.05018895e-07  1.08852277e-07]\n",
      "Loss at iteration 1592: w=[ 1.81683125 -0.26237612], loss=0.1617161716172556, gradiant=[-4.01291026e-07  1.07850323e-07]\n",
      "Loss at iteration 1593: w=[ 1.81683126 -0.26237612], loss=0.1617161716172538, gradiant=[-3.9759744e-07  1.0685770e-07]\n",
      "Loss at iteration 1594: w=[ 1.81683126 -0.26237612], loss=0.1617161716172521, gradiant=[-3.93937878e-07  1.05874108e-07]\n",
      "Loss at iteration 1595: w=[ 1.81683126 -0.26237612], loss=0.16171617161725044, gradiant=[-3.90311975e-07  1.04899665e-07]\n",
      "Loss at iteration 1596: w=[ 1.81683127 -0.26237613], loss=0.16171617161724883, gradiant=[-3.86719468e-07  1.03934103e-07]\n",
      "Loss at iteration 1597: w=[ 1.81683127 -0.26237613], loss=0.16171617161724725, gradiant=[-3.83160004e-07  1.02977519e-07]\n",
      "Loss at iteration 1598: w=[ 1.81683127 -0.26237613], loss=0.16171617161724566, gradiant=[-3.79633331e-07  1.02029628e-07]\n",
      "Loss at iteration 1599: w=[ 1.81683128 -0.26237613], loss=0.1617161716172441, gradiant=[-3.76139085e-07  1.01090588e-07]\n",
      "Loss at iteration 1600: w=[ 1.81683128 -0.26237613], loss=0.1617161716172427, gradiant=[-3.72677034e-07  1.00160071e-07]\n",
      "Loss at iteration 1601: w=[ 1.81683129 -0.26237613], loss=0.16171617161724114, gradiant=[-3.69246817e-07  9.92382295e-08]\n",
      "Loss at iteration 1602: w=[ 1.81683129 -0.26237613], loss=0.16171617161723964, gradiant=[-3.65848201e-07  9.83247691e-08]\n",
      "Loss at iteration 1603: w=[ 1.81683129 -0.26237613], loss=0.1617161716172382, gradiant=[-3.62480842e-07  9.74198088e-08]\n",
      "Loss at iteration 1604: w=[ 1.8168313  -0.26237613], loss=0.1617161716172369, gradiant=[-3.59144499e-07  9.65230968e-08]\n",
      "Loss at iteration 1605: w=[ 1.8168313  -0.26237613], loss=0.16171617161723542, gradiant=[-3.55838843e-07  9.56347170e-08]\n",
      "Loss at iteration 1606: w=[ 1.8168313  -0.26237614], loss=0.1617161716172341, gradiant=[-3.52563633e-07  9.47544375e-08]\n",
      "Loss at iteration 1607: w=[ 1.81683131 -0.26237614], loss=0.16171617161723284, gradiant=[-3.49318552e-07  9.38823241e-08]\n",
      "Loss at iteration 1608: w=[ 1.81683131 -0.26237614], loss=0.16171617161723145, gradiant=[-3.46103353e-07  9.30181893e-08]\n",
      "Loss at iteration 1609: w=[ 1.81683131 -0.26237614], loss=0.16171617161723026, gradiant=[-3.42917736e-07  9.21620491e-08]\n",
      "Loss at iteration 1610: w=[ 1.81683132 -0.26237614], loss=0.16171617161722895, gradiant=[-3.39761450e-07  9.13137511e-08]\n",
      "Loss at iteration 1611: w=[ 1.81683132 -0.26237614], loss=0.16171617161722776, gradiant=[-3.36634207e-07  9.04732917e-08]\n",
      "Loss at iteration 1612: w=[ 1.81683132 -0.26237614], loss=0.16171617161722648, gradiant=[-3.33535752e-07  8.96405531e-08]\n",
      "Loss at iteration 1613: w=[ 1.81683133 -0.26237614], loss=0.16171617161722532, gradiant=[-3.30465814e-07  8.88154850e-08]\n",
      "Loss at iteration 1614: w=[ 1.81683133 -0.26237614], loss=0.16171617161722407, gradiant=[-3.27424136e-07  8.79980013e-08]\n",
      "Loss at iteration 1615: w=[ 1.81683133 -0.26237614], loss=0.16171617161722304, gradiant=[-3.24410452e-07  8.71880458e-08]\n",
      "Loss at iteration 1616: w=[ 1.81683134 -0.26237614], loss=0.1617161716172219, gradiant=[-3.21424504e-07  8.63855569e-08]\n",
      "Loss at iteration 1617: w=[ 1.81683134 -0.26237615], loss=0.16171617161722074, gradiant=[-3.18466043e-07  8.55904426e-08]\n",
      "Loss at iteration 1618: w=[ 1.81683134 -0.26237615], loss=0.16171617161721963, gradiant=[-3.15534809e-07  8.48026547e-08]\n",
      "Loss at iteration 1619: w=[ 1.81683135 -0.26237615], loss=0.1617161716172186, gradiant=[-3.12630562e-07  8.40220933e-08]\n",
      "Loss at iteration 1620: w=[ 1.81683135 -0.26237615], loss=0.1617161716172176, gradiant=[-3.09753036e-07  8.32487554e-08]\n",
      "Loss at iteration 1621: w=[ 1.81683135 -0.26237615], loss=0.16171617161721663, gradiant=[-3.06902006e-07  8.24824976e-08]\n",
      "Loss at iteration 1622: w=[ 1.81683136 -0.26237615], loss=0.16171617161721552, gradiant=[-3.04077206e-07  8.17233320e-08]\n",
      "Loss at iteration 1623: w=[ 1.81683136 -0.26237615], loss=0.16171617161721458, gradiant=[-3.01278419e-07  8.09711052e-08]\n",
      "Loss at iteration 1624: w=[ 1.81683136 -0.26237615], loss=0.16171617161721358, gradiant=[-2.98505379e-07  8.02258559e-08]\n",
      "Loss at iteration 1625: w=[ 1.81683136 -0.26237615], loss=0.16171617161721258, gradiant=[-2.95757875e-07  7.94874190e-08]\n",
      "Loss at iteration 1626: w=[ 1.81683137 -0.26237615], loss=0.16171617161721177, gradiant=[-2.93035648e-07  7.87558235e-08]\n",
      "Loss at iteration 1627: w=[ 1.81683137 -0.26237615], loss=0.16171617161721086, gradiant=[-2.90338492e-07  7.80309019e-08]\n",
      "Loss at iteration 1628: w=[ 1.81683137 -0.26237615], loss=0.16171617161720997, gradiant=[-2.87666144e-07  7.73127202e-08]\n",
      "Loss at iteration 1629: w=[ 1.81683138 -0.26237616], loss=0.16171617161720908, gradiant=[-2.85018410e-07  7.66010837e-08]\n",
      "Loss at iteration 1630: w=[ 1.81683138 -0.26237616], loss=0.1617161716172082, gradiant=[-2.82395026e-07  7.58960752e-08]\n",
      "Loss at iteration 1631: w=[ 1.81683138 -0.26237616], loss=0.1617161716172073, gradiant=[-2.79795812e-07  7.51974657e-08]\n",
      "Loss at iteration 1632: w=[ 1.81683138 -0.26237616], loss=0.16171617161720653, gradiant=[-2.77220498e-07  7.45053725e-08]\n",
      "Loss at iteration 1633: w=[ 1.81683139 -0.26237616], loss=0.1617161716172057, gradiant=[-2.74668911e-07  7.38195647e-08]\n",
      "Loss at iteration 1634: w=[ 1.81683139 -0.26237616], loss=0.16171617161720495, gradiant=[-2.72140787e-07  7.31401547e-08]\n",
      "Loss at iteration 1635: w=[ 1.81683139 -0.26237616], loss=0.16171617161720406, gradiant=[-2.69635953e-07  7.24669177e-08]\n",
      "Loss at iteration 1636: w=[ 1.8168314  -0.26237616], loss=0.1617161716172033, gradiant=[-2.67154152e-07  7.17999632e-08]\n",
      "Loss at iteration 1637: w=[ 1.8168314  -0.26237616], loss=0.16171617161720256, gradiant=[-2.64695219e-07  7.11390523e-08]\n",
      "Loss at iteration 1638: w=[ 1.8168314  -0.26237616], loss=0.16171617161720173, gradiant=[-2.62258893e-07  7.04843218e-08]\n",
      "Loss at iteration 1639: w=[ 1.8168314  -0.26237616], loss=0.16171617161720112, gradiant=[-2.59845017e-07  6.98355181e-08]\n",
      "Loss at iteration 1640: w=[ 1.81683141 -0.26237616], loss=0.16171617161720026, gradiant=[-2.57453335e-07  6.91927768e-08]\n",
      "Loss at iteration 1641: w=[ 1.81683141 -0.26237616], loss=0.1617161716171997, gradiant=[-2.55083689e-07  6.85558697e-08]\n",
      "Loss at iteration 1642: w=[ 1.81683141 -0.26237616], loss=0.16171617161719898, gradiant=[-2.52735831e-07  6.79249104e-08]\n",
      "Loss at iteration 1643: w=[ 1.81683141 -0.26237617], loss=0.16171617161719834, gradiant=[-2.50409604e-07  6.72996790e-08]\n",
      "Loss at iteration 1644: w=[ 1.81683142 -0.26237617], loss=0.16171617161719765, gradiant=[-2.48104769e-07  6.66802743e-08]\n",
      "Loss at iteration 1645: w=[ 1.81683142 -0.26237617], loss=0.16171617161719695, gradiant=[-2.45821168e-07  6.60664945e-08]\n",
      "Loss at iteration 1646: w=[ 1.81683142 -0.26237617], loss=0.16171617161719634, gradiant=[-2.43558567e-07  6.54584367e-08]\n",
      "Loss at iteration 1647: w=[ 1.81683142 -0.26237617], loss=0.1617161716171956, gradiant=[-2.41316807e-07  6.48559186e-08]\n",
      "Loss at iteration 1648: w=[ 1.81683143 -0.26237617], loss=0.1617161716171951, gradiant=[-2.39095668e-07  6.42589898e-08]\n",
      "Loss at iteration 1649: w=[ 1.81683143 -0.26237617], loss=0.1617161716171944, gradiant=[-2.36894983e-07  6.36675198e-08]\n",
      "Loss at iteration 1650: w=[ 1.81683143 -0.26237617], loss=0.16171617161719387, gradiant=[-2.34714545e-07  6.30815270e-08]\n",
      "Loss at iteration 1651: w=[ 1.81683143 -0.26237617], loss=0.16171617161719326, gradiant=[-2.32554187e-07  6.25008855e-08]\n",
      "Loss at iteration 1652: w=[ 1.81683144 -0.26237617], loss=0.16171617161719268, gradiant=[-2.30413698e-07  6.19256468e-08]\n",
      "Loss at iteration 1653: w=[ 1.81683144 -0.26237617], loss=0.1617161716171921, gradiant=[-2.28292930e-07  6.13556284e-08]\n",
      "Loss at iteration 1654: w=[ 1.81683144 -0.26237617], loss=0.16171617161719148, gradiant=[-2.26191657e-07  6.07909536e-08]\n",
      "Loss at iteration 1655: w=[ 1.81683144 -0.26237617], loss=0.16171617161719087, gradiant=[-2.24109755e-07  6.02313589e-08]\n",
      "Loss at iteration 1656: w=[ 1.81683144 -0.26237617], loss=0.16171617161719043, gradiant=[-2.22046984e-07  5.96770318e-08]\n",
      "Loss at iteration 1657: w=[ 1.81683145 -0.26237617], loss=0.16171617161718993, gradiant=[-2.20003228e-07  5.91277013e-08]\n",
      "Loss at iteration 1658: w=[ 1.81683145 -0.26237617], loss=0.16171617161718943, gradiant=[-2.17978257e-07  5.85835247e-08]\n",
      "Loss at iteration 1659: w=[ 1.81683145 -0.26237618], loss=0.16171617161718888, gradiant=[-2.15971951e-07  5.80442577e-08]\n",
      "Loss at iteration 1660: w=[ 1.81683145 -0.26237618], loss=0.16171617161718838, gradiant=[-2.13984083e-07  5.75100573e-08]\n",
      "Loss at iteration 1661: w=[ 1.81683145 -0.26237618], loss=0.1617161716171878, gradiant=[-2.12014540e-07  5.69806691e-08]\n",
      "Loss at iteration 1662: w=[ 1.81683146 -0.26237618], loss=0.16171617161718743, gradiant=[-2.10063098e-07  5.64562579e-08]\n",
      "Loss at iteration 1663: w=[ 1.81683146 -0.26237618], loss=0.161716171617187, gradiant=[-2.08129642e-07  5.59365795e-08]\n",
      "Loss at iteration 1664: w=[ 1.81683146 -0.26237618], loss=0.16171617161718643, gradiant=[-2.06213959e-07  5.54217710e-08]\n",
      "Loss at iteration 1665: w=[ 1.81683146 -0.26237618], loss=0.16171617161718602, gradiant=[-2.04315935e-07  5.49116031e-08]\n",
      "Loss at iteration 1666: w=[ 1.81683147 -0.26237618], loss=0.16171617161718552, gradiant=[-2.02435349e-07  5.44062445e-08]\n",
      "Loss at iteration 1667: w=[ 1.81683147 -0.26237618], loss=0.16171617161718524, gradiant=[-2.00572106e-07  5.39054160e-08]\n",
      "Loss at iteration 1668: w=[ 1.81683147 -0.26237618], loss=0.16171617161718466, gradiant=[-1.98725978e-07  5.34093270e-08]\n",
      "Loss at iteration 1669: w=[ 1.81683147 -0.26237618], loss=0.16171617161718432, gradiant=[-1.96896881e-07  5.29176563e-08]\n",
      "Loss at iteration 1670: w=[ 1.81683147 -0.26237618], loss=0.1617161716171838, gradiant=[-1.95084576e-07  5.24306739e-08]\n",
      "Loss at iteration 1671: w=[ 1.81683148 -0.26237618], loss=0.1617161716171835, gradiant=[-1.93288999e-07  5.19480000e-08]\n",
      "Loss at iteration 1672: w=[ 1.81683148 -0.26237618], loss=0.16171617161718305, gradiant=[-1.91509899e-07  5.14699539e-08]\n",
      "Loss at iteration 1673: w=[ 1.81683148 -0.26237618], loss=0.16171617161718277, gradiant=[-1.89747226e-07  5.09961165e-08]\n",
      "Loss at iteration 1674: w=[ 1.81683148 -0.26237618], loss=0.16171617161718227, gradiant=[-1.88000726e-07  5.05268282e-08]\n",
      "Loss at iteration 1675: w=[ 1.81683148 -0.26237618], loss=0.16171617161718188, gradiant=[-1.86270347e-07  5.00616895e-08]\n",
      "Loss at iteration 1676: w=[ 1.81683148 -0.26237618], loss=0.16171617161718163, gradiant=[-1.84555852e-07  4.96009885e-08]\n",
      "Loss at iteration 1677: w=[ 1.81683149 -0.26237618], loss=0.16171617161718116, gradiant=[-1.82857180e-07  4.91443729e-08]\n",
      "Loss at iteration 1678: w=[ 1.81683149 -0.26237619], loss=0.16171617161718083, gradiant=[-1.81174104e-07  4.86921066e-08]\n",
      "Loss at iteration 1679: w=[ 1.81683149 -0.26237619], loss=0.16171617161718044, gradiant=[-1.79506555e-07  4.82438682e-08]\n",
      "Loss at iteration 1680: w=[ 1.81683149 -0.26237619], loss=0.1617161716171801, gradiant=[-1.77854319e-07  4.77998872e-08]\n",
      "Loss at iteration 1681: w=[ 1.81683149 -0.26237619], loss=0.16171617161717983, gradiant=[-1.76217327e-07  4.73598566e-08]\n",
      "Loss at iteration 1682: w=[ 1.8168315  -0.26237619], loss=0.16171617161717944, gradiant=[-1.74595364e-07  4.69240217e-08]\n",
      "Loss at iteration 1683: w=[ 1.8168315  -0.26237619], loss=0.16171617161717913, gradiant=[-1.7298837e-07  4.6492047e-08]\n",
      "Loss at iteration 1684: w=[ 1.8168315  -0.26237619], loss=0.16171617161717872, gradiant=[-1.71396127e-07  4.60641960e-08]\n",
      "Loss at iteration 1685: w=[ 1.8168315  -0.26237619], loss=0.16171617161717855, gradiant=[-1.69818580e-07  4.56401346e-08]\n",
      "Loss at iteration 1686: w=[ 1.8168315  -0.26237619], loss=0.1617161716171781, gradiant=[-1.68255511e-07  4.52201310e-08]\n",
      "Loss at iteration 1687: w=[ 1.8168315  -0.26237619], loss=0.16171617161717794, gradiant=[-1.66706870e-07  4.48038415e-08]\n",
      "Loss at iteration 1688: w=[ 1.81683151 -0.26237619], loss=0.16171617161717755, gradiant=[-1.65172445e-07  4.43915231e-08]\n",
      "Loss at iteration 1689: w=[ 1.81683151 -0.26237619], loss=0.16171617161717733, gradiant=[-1.63652177e-07  4.39828759e-08]\n",
      "Loss at iteration 1690: w=[ 1.81683151 -0.26237619], loss=0.16171617161717705, gradiant=[-1.62145872e-07  4.35780991e-08]\n",
      "Loss at iteration 1691: w=[ 1.81683151 -0.26237619], loss=0.1617161716171768, gradiant=[-1.60653457e-07  4.31769553e-08]\n",
      "Loss at iteration 1692: w=[ 1.81683151 -0.26237619], loss=0.1617161716171765, gradiant=[-1.5917476e-07  4.2779571e-08]\n",
      "Loss at iteration 1693: w=[ 1.81683151 -0.26237619], loss=0.16171617161717616, gradiant=[-1.57709683e-07  4.23858080e-08]\n",
      "Loss at iteration 1694: w=[ 1.81683151 -0.26237619], loss=0.161716171617176, gradiant=[-1.56258086e-07  4.19956890e-08]\n",
      "Loss at iteration 1695: w=[ 1.81683152 -0.26237619], loss=0.16171617161717566, gradiant=[-1.54819855e-07  4.16091397e-08]\n",
      "Loss at iteration 1696: w=[ 1.81683152 -0.26237619], loss=0.1617161716171754, gradiant=[-1.53394857e-07  4.12261662e-08]\n",
      "Loss at iteration 1697: w=[ 1.81683152 -0.26237619], loss=0.1617161716171751, gradiant=[-1.51982979e-07  4.08467038e-08]\n",
      "Loss at iteration 1698: w=[ 1.81683152 -0.26237619], loss=0.16171617161717494, gradiant=[-1.50584091e-07  4.04707542e-08]\n",
      "Loss at iteration 1699: w=[ 1.81683152 -0.26237619], loss=0.16171617161717464, gradiant=[-1.49198084e-07  4.00982438e-08]\n",
      "Loss at iteration 1700: w=[ 1.81683152 -0.26237619], loss=0.16171617161717447, gradiant=[-1.47824831e-07  3.97291743e-08]\n",
      "Loss at iteration 1701: w=[ 1.81683153 -0.2623762 ], loss=0.16171617161717422, gradiant=[-1.46464218e-07  3.93634976e-08]\n",
      "Loss at iteration 1702: w=[ 1.81683153 -0.2623762 ], loss=0.1617161716171739, gradiant=[-1.45116129e-07  3.90011888e-08]\n",
      "Loss at iteration 1703: w=[ 1.81683153 -0.2623762 ], loss=0.16171617161717372, gradiant=[-1.43780452e-07  3.86421989e-08]\n",
      "Loss at iteration 1704: w=[ 1.81683153 -0.2623762 ], loss=0.16171617161717347, gradiant=[-1.42457059e-07  3.82865483e-08]\n",
      "Loss at iteration 1705: w=[ 1.81683153 -0.2623762 ], loss=0.16171617161717328, gradiant=[-1.41145856e-07  3.79341403e-08]\n",
      "Loss at iteration 1706: w=[ 1.81683153 -0.2623762 ], loss=0.16171617161717317, gradiant=[-1.39846718e-07  3.75849870e-08]\n",
      "Loss at iteration 1707: w=[ 1.81683153 -0.2623762 ], loss=0.1617161716171729, gradiant=[-1.38559537e-07  3.72390478e-08]\n",
      "Loss at iteration 1708: w=[ 1.81683154 -0.2623762 ], loss=0.16171617161717264, gradiant=[-1.37284204e-07  3.68962958e-08]\n",
      "Loss at iteration 1709: w=[ 1.81683154 -0.2623762 ], loss=0.16171617161717247, gradiant=[-1.36020613e-07  3.65566821e-08]\n",
      "Loss at iteration 1710: w=[ 1.81683154 -0.2623762 ], loss=0.1617161716171723, gradiant=[-1.34768644e-07  3.62202250e-08]\n",
      "Loss at iteration 1711: w=[ 1.81683154 -0.2623762 ], loss=0.1617161716171721, gradiant=[-1.33528211e-07  3.58868174e-08]\n",
      "Loss at iteration 1712: w=[ 1.81683154 -0.2623762 ], loss=0.1617161716171719, gradiant=[-1.32299179e-07  3.55565419e-08]\n",
      "Loss at iteration 1713: w=[ 1.81683154 -0.2623762 ], loss=0.1617161716171718, gradiant=[-1.31081478e-07  3.52292335e-08]\n",
      "Loss at iteration 1714: w=[ 1.81683154 -0.2623762 ], loss=0.16171617161717156, gradiant=[-1.29874966e-07  3.49050106e-08]\n",
      "Loss at iteration 1715: w=[ 1.81683154 -0.2623762 ], loss=0.16171617161717136, gradiant=[-1.28679578e-07  3.45836999e-08]\n",
      "Loss at iteration 1716: w=[ 1.81683155 -0.2623762 ], loss=0.1617161716171711, gradiant=[-1.27495172e-07  3.42654225e-08]\n",
      "Loss at iteration 1717: w=[ 1.81683155 -0.2623762 ], loss=0.161716171617171, gradiant=[-1.26321688e-07  3.39500006e-08]\n",
      "Loss at iteration 1718: w=[ 1.81683155 -0.2623762 ], loss=0.16171617161717083, gradiant=[-1.25158985e-07  3.36375576e-08]\n",
      "Loss at iteration 1719: w=[ 1.81683155 -0.2623762 ], loss=0.16171617161717067, gradiant=[-1.24007004e-07  3.33279101e-08]\n",
      "Loss at iteration 1720: w=[ 1.81683155 -0.2623762 ], loss=0.16171617161717042, gradiant=[-1.22865607e-07  3.30211852e-08]\n",
      "Loss at iteration 1721: w=[ 1.81683155 -0.2623762 ], loss=0.16171617161717036, gradiant=[-1.21734734e-07  3.27172211e-08]\n",
      "Loss at iteration 1722: w=[ 1.81683155 -0.2623762 ], loss=0.16171617161717022, gradiant=[-1.20614253e-07  3.24161122e-08]\n",
      "Loss at iteration 1723: w=[ 1.81683155 -0.2623762 ], loss=0.16171617161717, gradiant=[-1.195041e-07  3.211772e-08]\n",
      "Loss at iteration 1724: w=[ 1.81683156 -0.2623762 ], loss=0.16171617161716986, gradiant=[-1.18404150e-07  3.18221303e-08]\n",
      "Loss at iteration 1725: w=[ 1.81683156 -0.2623762 ], loss=0.16171617161716983, gradiant=[-1.17314338e-07  3.15292108e-08]\n",
      "Loss at iteration 1726: w=[ 1.81683156 -0.2623762 ], loss=0.1617161716171695, gradiant=[-1.16234547e-07  3.12390219e-08]\n",
      "Loss at iteration 1727: w=[ 1.81683156 -0.2623762 ], loss=0.1617161716171694, gradiant=[-1.15164700e-07  3.09514846e-08]\n",
      "Loss at iteration 1728: w=[ 1.81683156 -0.2623762 ], loss=0.16171617161716936, gradiant=[-1.14104699e-07  3.06665999e-08]\n",
      "Loss at iteration 1729: w=[ 1.81683156 -0.2623762 ], loss=0.1617161716171691, gradiant=[-1.13054453e-07  3.03843401e-08]\n",
      "Loss at iteration 1730: w=[ 1.81683156 -0.26237621], loss=0.16171617161716897, gradiant=[-1.12013875e-07  3.01046768e-08]\n",
      "Loss at iteration 1731: w=[ 1.81683156 -0.26237621], loss=0.16171617161716884, gradiant=[-1.10982875e-07  2.98275838e-08]\n",
      "Loss at iteration 1732: w=[ 1.81683156 -0.26237621], loss=0.1617161716171687, gradiant=[-1.09961365e-07  2.95530415e-08]\n",
      "Loss at iteration 1733: w=[ 1.81683157 -0.26237621], loss=0.1617161716171686, gradiant=[-1.08949254e-07  2.92810370e-08]\n",
      "Loss at iteration 1734: w=[ 1.81683157 -0.26237621], loss=0.16171617161716853, gradiant=[-1.07946465e-07  2.90115141e-08]\n",
      "Loss at iteration 1735: w=[ 1.81683157 -0.26237621], loss=0.16171617161716845, gradiant=[-1.06952896e-07  2.87445068e-08]\n",
      "Loss at iteration 1736: w=[ 1.81683157 -0.26237621], loss=0.16171617161716817, gradiant=[-1.05968484e-07  2.84799125e-08]\n",
      "Loss at iteration 1737: w=[ 1.81683157 -0.26237621], loss=0.16171617161716811, gradiant=[-1.04993122e-07  2.82177967e-08]\n",
      "Loss at iteration 1738: w=[ 1.81683157 -0.26237621], loss=0.16171617161716806, gradiant=[-1.04026744e-07  2.79580646e-08]\n",
      "Loss at iteration 1739: w=[ 1.81683157 -0.26237621], loss=0.1617161716171678, gradiant=[-1.03069258e-07  2.77007374e-08]\n",
      "Loss at iteration 1740: w=[ 1.81683157 -0.26237621], loss=0.16171617161716778, gradiant=[-1.02120586e-07  2.74457723e-08]\n",
      "Loss at iteration 1741: w=[ 1.81683157 -0.26237621], loss=0.16171617161716773, gradiant=[-1.01180648e-07  2.71931457e-08]\n",
      "Loss at iteration 1742: w=[ 1.81683158 -0.26237621], loss=0.16171617161716764, gradiant=[-1.00249357e-07  2.69428625e-08]\n",
      "Loss at iteration 1743: w=[ 1.81683158 -0.26237621], loss=0.16171617161716745, gradiant=[-9.93266398e-08  2.66948734e-08]\n",
      "Loss at iteration 1744: w=[ 1.81683158 -0.26237621], loss=0.16171617161716742, gradiant=[-9.84124164e-08  2.64491648e-08]\n",
      "Loss at iteration 1745: w=[ 1.81683158 -0.26237621], loss=0.16171617161716728, gradiant=[-9.75066059e-08  2.62057246e-08]\n",
      "Loss at iteration 1746: w=[ 1.81683158 -0.26237621], loss=0.16171617161716706, gradiant=[-9.66091345e-08  2.59645169e-08]\n",
      "Loss at iteration 1747: w=[ 1.81683158 -0.26237621], loss=0.1617161716171671, gradiant=[-9.57199212e-08  2.57255399e-08]\n",
      "Loss at iteration 1748: w=[ 1.81683158 -0.26237621], loss=0.16171617161716695, gradiant=[-9.48388939e-08  2.54887556e-08]\n",
      "Loss at iteration 1749: w=[ 1.81683158 -0.26237621], loss=0.1617161716171669, gradiant=[-9.39659766e-08  2.52541481e-08]\n",
      "Loss at iteration 1750: w=[ 1.81683158 -0.26237621], loss=0.16171617161716684, gradiant=[-9.31010938e-08  2.50217013e-08]\n",
      "Loss at iteration 1751: w=[ 1.81683158 -0.26237621], loss=0.16171617161716662, gradiant=[-9.22441670e-08  2.47914095e-08]\n",
      "Loss at iteration 1752: w=[ 1.81683158 -0.26237621], loss=0.16171617161716653, gradiant=[-9.13951349e-08  2.45632097e-08]\n",
      "Loss at iteration 1753: w=[ 1.81683159 -0.26237621], loss=0.1617161716171664, gradiant=[-9.0553910e-08  2.4337139e-08]\n",
      "Loss at iteration 1754: w=[ 1.81683159 -0.26237621], loss=0.1617161716171664, gradiant=[-8.97204352e-08  2.41131219e-08]\n",
      "Loss at iteration 1755: w=[ 1.81683159 -0.26237621], loss=0.16171617161716628, gradiant=[-8.88946280e-08  2.38911818e-08]\n",
      "Loss at iteration 1756: w=[ 1.81683159 -0.26237621], loss=0.16171617161716625, gradiant=[-8.80764233e-08  2.36712781e-08]\n",
      "Loss at iteration 1757: w=[ 1.81683159 -0.26237621], loss=0.16171617161716623, gradiant=[-8.72657465e-08  2.34534100e-08]\n",
      "Loss at iteration 1758: w=[ 1.81683159 -0.26237621], loss=0.16171617161716606, gradiant=[-8.64625348e-08  2.32375335e-08]\n",
      "Loss at iteration 1759: w=[ 1.81683159 -0.26237621], loss=0.161716171617166, gradiant=[-8.56667143e-08  2.30236508e-08]\n",
      "Loss at iteration 1760: w=[ 1.81683159 -0.26237621], loss=0.16171617161716587, gradiant=[-8.48782168e-08  2.28117445e-08]\n",
      "Loss at iteration 1761: w=[ 1.81683159 -0.26237621], loss=0.16171617161716578, gradiant=[-8.4096983e-08  2.2601764e-08]\n",
      "Loss at iteration 1762: w=[ 1.81683159 -0.26237621], loss=0.16171617161716584, gradiant=[-8.33229328e-08  2.23937446e-08]\n",
      "Loss at iteration 1763: w=[ 1.81683159 -0.26237621], loss=0.16171617161716573, gradiant=[-8.25560112e-08  2.21876242e-08]\n",
      "Loss at iteration 1764: w=[ 1.8168316  -0.26237621], loss=0.16171617161716556, gradiant=[-8.17961470e-08  2.19834056e-08]\n",
      "Loss at iteration 1765: w=[ 1.8168316  -0.26237621], loss=0.1617161716171655, gradiant=[-8.10432762e-08  2.17810689e-08]\n",
      "Loss at iteration 1766: w=[ 1.8168316  -0.26237621], loss=0.16171617161716542, gradiant=[-8.02973377e-08  2.15805841e-08]\n",
      "Loss at iteration 1767: w=[ 1.8168316  -0.26237621], loss=0.1617161716171655, gradiant=[-7.95582595e-08  2.13819661e-08]\n",
      "Loss at iteration 1768: w=[ 1.8168316  -0.26237621], loss=0.16171617161716528, gradiant=[-7.88259958e-08  2.11851319e-08]\n",
      "Loss at iteration 1769: w=[ 1.8168316  -0.26237622], loss=0.16171617161716528, gradiant=[-7.81004544e-08  2.09901746e-08]\n",
      "Loss at iteration 1770: w=[ 1.8168316  -0.26237622], loss=0.16171617161716514, gradiant=[-7.73816093e-08  2.07969440e-08]\n",
      "Loss at iteration 1771: w=[ 1.8168316  -0.26237622], loss=0.16171617161716517, gradiant=[-7.66693618e-08  2.06055611e-08]\n",
      "Loss at iteration 1772: w=[ 1.8168316  -0.26237622], loss=0.16171617161716503, gradiant=[-7.59636916e-08  2.04158604e-08]\n",
      "Loss at iteration 1773: w=[ 1.8168316  -0.26237622], loss=0.161716171617165, gradiant=[-7.52644908e-08  2.02280008e-08]\n",
      "Loss at iteration 1774: w=[ 1.8168316  -0.26237622], loss=0.1617161716171649, gradiant=[-7.45717574e-08  2.00417520e-08]\n",
      "Loss at iteration 1775: w=[ 1.8168316  -0.26237622], loss=0.16171617161716484, gradiant=[-7.38853654e-08  1.98573478e-08]\n",
      "Loss at iteration 1776: w=[ 1.8168316  -0.26237622], loss=0.1617161716171648, gradiant=[-7.32053242e-08  1.96745165e-08]\n",
      "Loss at iteration 1777: w=[ 1.81683161 -0.26237622], loss=0.1617161716171648, gradiant=[-7.25315097e-08  1.94934889e-08]\n",
      "Loss at iteration 1778: w=[ 1.81683161 -0.26237622], loss=0.1617161716171647, gradiant=[-7.18639293e-08  1.93140076e-08]\n",
      "Loss at iteration 1779: w=[ 1.81683161 -0.26237622], loss=0.1617161716171647, gradiant=[-7.12024617e-08  1.91362982e-08]\n",
      "Loss at iteration 1780: w=[ 1.81683161 -0.26237622], loss=0.16171617161716462, gradiant=[-7.05471145e-08  1.89601031e-08]\n",
      "Loss at iteration 1781: w=[ 1.81683161 -0.26237622], loss=0.1617161716171645, gradiant=[-6.98977683e-08  1.87856449e-08]\n",
      "Loss at iteration 1782: w=[ 1.81683161 -0.26237622], loss=0.16171617161716445, gradiant=[-6.92544274e-08  1.86126877e-08]\n",
      "Loss at iteration 1783: w=[ 1.81683161 -0.26237622], loss=0.16171617161716448, gradiant=[-6.86169832e-08  1.84414134e-08]\n",
      "Loss at iteration 1784: w=[ 1.81683161 -0.26237622], loss=0.1617161716171644, gradiant=[-6.79854283e-08  1.82716343e-08]\n",
      "Loss at iteration 1785: w=[ 1.81683161 -0.26237622], loss=0.16171617161716442, gradiant=[-6.73596654e-08  1.81034959e-08]\n",
      "Loss at iteration 1786: w=[ 1.81683161 -0.26237622], loss=0.1617161716171644, gradiant=[-6.67396794e-08  1.79368400e-08]\n",
      "Loss at iteration 1787: w=[ 1.81683161 -0.26237622], loss=0.16171617161716417, gradiant=[-6.61253862e-08  1.77717685e-08]\n",
      "Loss at iteration 1788: w=[ 1.81683161 -0.26237622], loss=0.1617161716171642, gradiant=[-6.55167581e-08  1.76081764e-08]\n",
      "Loss at iteration 1789: w=[ 1.81683161 -0.26237622], loss=0.16171617161716417, gradiant=[-6.49137261e-08  1.74461126e-08]\n",
      "Loss at iteration 1790: w=[ 1.81683161 -0.26237622], loss=0.16171617161716417, gradiant=[-6.43162463e-08  1.72855333e-08]\n",
      "Loss at iteration 1791: w=[ 1.81683161 -0.26237622], loss=0.16171617161716406, gradiant=[-6.37242654e-08  1.71264326e-08]\n",
      "Loss at iteration 1792: w=[ 1.81683162 -0.26237622], loss=0.16171617161716403, gradiant=[-6.31377319e-08  1.69688016e-08]\n",
      "Loss at iteration 1793: w=[ 1.81683162 -0.26237622], loss=0.16171617161716398, gradiant=[-6.25565998e-08  1.68126113e-08]\n",
      "Loss at iteration 1794: w=[ 1.81683162 -0.26237622], loss=0.161716171617164, gradiant=[-6.19808135e-08  1.66578702e-08]\n",
      "Loss at iteration 1795: w=[ 1.81683162 -0.26237622], loss=0.16171617161716392, gradiant=[-6.14103281e-08  1.65045477e-08]\n",
      "Loss at iteration 1796: w=[ 1.81683162 -0.26237622], loss=0.16171617161716395, gradiant=[-6.08450967e-08  1.63526265e-08]\n",
      "Loss at iteration 1797: w=[ 1.81683162 -0.26237622], loss=0.1617161716171638, gradiant=[-6.02850589e-08  1.62021352e-08]\n",
      "Loss at iteration 1798: w=[ 1.81683162 -0.26237622], loss=0.16171617161716387, gradiant=[-5.97301885e-08  1.60529826e-08]\n",
      "Loss at iteration 1799: w=[ 1.81683162 -0.26237622], loss=0.1617161716171637, gradiant=[-5.91804109e-08  1.59052547e-08]\n",
      "Loss at iteration 1800: w=[ 1.81683162 -0.26237622], loss=0.16171617161716378, gradiant=[-5.86357107e-08  1.57588255e-08]\n",
      "Loss at iteration 1801: w=[ 1.81683162 -0.26237622], loss=0.16171617161716356, gradiant=[-5.80960056e-08  1.56138101e-08]\n",
      "Loss at iteration 1802: w=[ 1.81683162 -0.26237622], loss=0.1617161716171636, gradiant=[-5.75612873e-08  1.54700596e-08]\n",
      "Loss at iteration 1803: w=[ 1.81683162 -0.26237622], loss=0.16171617161716367, gradiant=[-5.70314693e-08  1.53277124e-08]\n",
      "Loss at iteration 1804: w=[ 1.81683162 -0.26237622], loss=0.16171617161716353, gradiant=[-5.65065497e-08  1.51865930e-08]\n",
      "Loss at iteration 1805: w=[ 1.81683162 -0.26237622], loss=0.16171617161716356, gradiant=[-5.59864398e-08  1.50468538e-08]\n",
      "Loss at iteration 1806: w=[ 1.81683162 -0.26237622], loss=0.16171617161716342, gradiant=[-5.54711395e-08  1.49083162e-08]\n",
      "Loss at iteration 1807: w=[ 1.81683162 -0.26237622], loss=0.16171617161716342, gradiant=[-5.49605597e-08  1.47711393e-08]\n",
      "Loss at iteration 1808: w=[ 1.81683162 -0.26237622], loss=0.16171617161716345, gradiant=[-5.44547012e-08  1.46351420e-08]\n",
      "Loss at iteration 1809: w=[ 1.81683163 -0.26237622], loss=0.16171617161716334, gradiant=[-5.39534783e-08  1.45004730e-08]\n",
      "Loss at iteration 1810: w=[ 1.81683163 -0.26237622], loss=0.16171617161716334, gradiant=[-5.34568874e-08  1.43669747e-08]\n",
      "Loss at iteration 1811: w=[ 1.81683163 -0.26237622], loss=0.16171617161716334, gradiant=[-5.29648512e-08  1.42347648e-08]\n",
      "Loss at iteration 1812: w=[ 1.81683163 -0.26237622], loss=0.16171617161716323, gradiant=[-5.24773561e-08  1.41037253e-08]\n",
      "Loss at iteration 1813: w=[ 1.81683163 -0.26237622], loss=0.16171617161716328, gradiant=[-5.19943388e-08  1.39739273e-08]\n",
      "Loss at iteration 1814: w=[ 1.81683163 -0.26237622], loss=0.16171617161716328, gradiant=[-5.15157759e-08  1.38452917e-08]\n",
      "Loss at iteration 1815: w=[ 1.81683163 -0.26237622], loss=0.16171617161716317, gradiant=[-5.10416095e-08  1.37178701e-08]\n",
      "Loss at iteration 1816: w=[ 1.81683163 -0.26237622], loss=0.16171617161716315, gradiant=[-5.05718140e-08  1.35915972e-08]\n",
      "Loss at iteration 1817: w=[ 1.81683163 -0.26237622], loss=0.16171617161716312, gradiant=[-5.01063361e-08  1.34665110e-08]\n",
      "Loss at iteration 1818: w=[ 1.81683163 -0.26237622], loss=0.16171617161716315, gradiant=[-4.96451508e-08  1.33425460e-08]\n",
      "Loss at iteration 1819: w=[ 1.81683163 -0.26237622], loss=0.16171617161716317, gradiant=[-4.91882008e-08  1.32197577e-08]\n",
      "Loss at iteration 1820: w=[ 1.81683163 -0.26237622], loss=0.16171617161716312, gradiant=[-4.87354674e-08  1.30980588e-08]\n",
      "Loss at iteration 1821: w=[ 1.81683163 -0.26237622], loss=0.16171617161716306, gradiant=[-4.82868926e-08  1.29775114e-08]\n",
      "Loss at iteration 1822: w=[ 1.81683163 -0.26237622], loss=0.161716171617163, gradiant=[-4.78424515e-08  1.28580551e-08]\n",
      "Loss at iteration 1823: w=[ 1.81683163 -0.26237622], loss=0.16171617161716298, gradiant=[-4.74020962e-08  1.27397183e-08]\n",
      "Loss at iteration 1824: w=[ 1.81683163 -0.26237622], loss=0.16171617161716306, gradiant=[-4.69657987e-08  1.26224515e-08]\n",
      "Loss at iteration 1825: w=[ 1.81683163 -0.26237622], loss=0.16171617161716306, gradiant=[-4.65335129e-08  1.25062800e-08]\n",
      "Loss at iteration 1826: w=[ 1.81683163 -0.26237622], loss=0.16171617161716295, gradiant=[-4.61052105e-08  1.23911605e-08]\n",
      "Loss at iteration 1827: w=[ 1.81683163 -0.26237622], loss=0.16171617161716287, gradiant=[-4.56808456e-08  1.22771175e-08]\n",
      "Loss at iteration 1828: w=[ 1.81683163 -0.26237622], loss=0.16171617161716284, gradiant=[-4.52603931e-08  1.21641023e-08]\n",
      "Loss at iteration 1829: w=[ 1.81683163 -0.26237622], loss=0.16171617161716287, gradiant=[-4.48438011e-08  1.20521606e-08]\n",
      "Loss at iteration 1830: w=[ 1.81683164 -0.26237622], loss=0.16171617161716284, gradiant=[-4.44310539e-08  1.19412111e-08]\n",
      "Loss at iteration 1831: w=[ 1.81683164 -0.26237622], loss=0.16171617161716292, gradiant=[-4.40220944e-08  1.18313253e-08]\n",
      "Loss at iteration 1832: w=[ 1.81683164 -0.26237623], loss=0.16171617161716284, gradiant=[-4.36169123e-08  1.17224013e-08]\n",
      "Loss at iteration 1833: w=[ 1.81683164 -0.26237623], loss=0.1617161716171627, gradiant=[-4.32154460e-08  1.16145292e-08]\n",
      "Loss at iteration 1834: w=[ 1.81683164 -0.26237623], loss=0.16171617161716284, gradiant=[-4.28176885e-08  1.15076012e-08]\n",
      "Loss at iteration 1835: w=[ 1.81683164 -0.26237623], loss=0.16171617161716276, gradiant=[-4.24235781e-08  1.14017080e-08]\n",
      "Loss at iteration 1836: w=[ 1.81683164 -0.26237623], loss=0.16171617161716284, gradiant=[-4.20331087e-08  1.12967403e-08]\n",
      "Loss at iteration 1837: w=[ 1.81683164 -0.26237623], loss=0.16171617161716267, gradiant=[-4.16462201e-08  1.11927860e-08]\n",
      "Loss at iteration 1838: w=[ 1.81683164 -0.26237623], loss=0.16171617161716267, gradiant=[-4.12629054e-08  1.10897425e-08]\n",
      "Loss at iteration 1839: w=[ 1.81683164 -0.26237623], loss=0.16171617161716267, gradiant=[-4.08831056e-08  1.09876953e-08]\n",
      "Loss at iteration 1840: w=[ 1.81683164 -0.26237623], loss=0.16171617161716273, gradiant=[-4.05068169e-08  1.08865320e-08]\n",
      "Loss at iteration 1841: w=[ 1.81683164 -0.26237623], loss=0.16171617161716267, gradiant=[-4.01339732e-08  1.07863662e-08]\n",
      "Loss at iteration 1842: w=[ 1.81683164 -0.26237623], loss=0.16171617161716256, gradiant=[-3.97645830e-08  1.06870421e-08]\n",
      "Loss at iteration 1843: w=[ 1.81683164 -0.26237623], loss=0.16171617161716265, gradiant=[-3.93985671e-08  1.05887290e-08]\n",
      "Loss at iteration 1844: w=[ 1.81683164 -0.26237623], loss=0.16171617161716262, gradiant=[-3.90359487e-08  1.04912129e-08]\n",
      "Loss at iteration 1845: w=[ 1.81683164 -0.26237623], loss=0.16171617161716256, gradiant=[-3.86766389e-08  1.03947018e-08]\n",
      "Loss at iteration 1846: w=[ 1.81683164 -0.26237623], loss=0.16171617161716262, gradiant=[-3.83206646e-08  1.02989753e-08]\n",
      "Loss at iteration 1847: w=[ 1.81683164 -0.26237623], loss=0.16171617161716262, gradiant=[-3.79679392e-08  1.02042317e-08]\n",
      "Loss at iteration 1848: w=[ 1.81683164 -0.26237623], loss=0.16171617161716254, gradiant=[-3.76184868e-08  1.01102609e-08]\n",
      "Loss at iteration 1849: w=[ 1.81683164 -0.26237623], loss=0.16171617161716242, gradiant=[-3.72722263e-08  1.00172467e-08]\n",
      "Loss at iteration 1850: w=[ 1.81683164 -0.26237623], loss=0.16171617161716245, gradiant=[-3.69291753e-08  9.92500630e-09]\n",
      "Loss at iteration 1851: w=[ 1.81683164 -0.26237623], loss=0.16171617161716248, gradiant=[-3.65892605e-08  9.83369327e-09]\n",
      "Loss at iteration 1852: w=[ 1.81683164 -0.26237623], loss=0.16171617161716256, gradiant=[-3.62524952e-08  9.74314348e-09]\n",
      "Loss at iteration 1853: w=[ 1.81683164 -0.26237623], loss=0.16171617161716242, gradiant=[-3.59188104e-08  9.65349815e-09]\n",
      "Loss at iteration 1854: w=[ 1.81683164 -0.26237623], loss=0.1617161716171624, gradiant=[-3.55882150e-08  9.56461103e-09]\n",
      "Loss at iteration 1855: w=[ 1.81683165 -0.26237623], loss=0.1617161716171624, gradiant=[-3.52606406e-08  9.47662215e-09]\n",
      "Loss at iteration 1856: w=[ 1.81683165 -0.26237623], loss=0.1617161716171624, gradiant=[-3.49361060e-08  9.38935226e-09]\n",
      "Loss at iteration 1857: w=[ 1.81683165 -0.26237623], loss=0.1617161716171624, gradiant=[-3.46145364e-08  9.30296847e-09]\n",
      "Loss at iteration 1858: w=[ 1.81683165 -0.26237623], loss=0.16171617161716245, gradiant=[-3.42959468e-08  9.21730366e-09]\n",
      "Loss at iteration 1859: w=[ 1.81683165 -0.26237623], loss=0.16171617161716237, gradiant=[-3.39802697e-08  9.13250172e-09]\n",
      "Loss at iteration 1860: w=[ 1.81683165 -0.26237623], loss=0.16171617161716245, gradiant=[-3.36675166e-08  9.04841195e-09]\n",
      "Loss at iteration 1861: w=[ 1.81683165 -0.26237623], loss=0.1617161716171623, gradiant=[-3.33576251e-08  8.96515899e-09]\n",
      "Loss at iteration 1862: w=[ 1.81683165 -0.26237623], loss=0.16171617161716242, gradiant=[-3.30506002e-08  8.88261938e-09]\n",
      "Loss at iteration 1863: w=[ 1.81683165 -0.26237623], loss=0.16171617161716237, gradiant=[-3.27463889e-08  8.80088521e-09]\n",
      "Loss at iteration 1864: w=[ 1.81683165 -0.26237623], loss=0.16171617161716229, gradiant=[-3.24449937e-08  8.71984322e-09]\n",
      "Loss at iteration 1865: w=[ 1.81683165 -0.26237623], loss=0.1617161716171624, gradiant=[-3.21463497e-08  8.63963227e-09]\n",
      "Loss at iteration 1866: w=[ 1.81683165 -0.26237623], loss=0.16171617161716234, gradiant=[-3.18504821e-08  8.56005740e-09]\n",
      "Loss at iteration 1867: w=[ 1.81683165 -0.26237623], loss=0.16171617161716226, gradiant=[-3.15573100e-08  8.48131787e-09]\n",
      "Loss at iteration 1868: w=[ 1.81683165 -0.26237623], loss=0.16171617161716229, gradiant=[-3.12668598e-08  8.40321546e-09]\n",
      "Loss at iteration 1869: w=[ 1.81683165 -0.26237623], loss=0.16171617161716234, gradiant=[-3.09790658e-08  8.32589642e-09]\n",
      "Loss at iteration 1870: w=[ 1.81683165 -0.26237623], loss=0.16171617161716223, gradiant=[-3.06939321e-08  8.24924543e-09]\n",
      "Loss at iteration 1871: w=[ 1.81683165 -0.26237623], loss=0.1617161716171622, gradiant=[-3.04114140e-08  8.17333386e-09]\n",
      "Loss at iteration 1872: w=[ 1.81683165 -0.26237623], loss=0.16171617161716217, gradiant=[-3.01315068e-08  8.09808280e-09]\n",
      "Loss at iteration 1873: w=[ 1.81683165 -0.26237623], loss=0.16171617161716229, gradiant=[-2.98541646e-08  8.02356596e-09]\n",
      "Loss at iteration 1874: w=[ 1.81683165 -0.26237623], loss=0.16171617161716223, gradiant=[-2.95793831e-08  7.94970430e-09]\n",
      "Loss at iteration 1875: w=[ 1.81683165 -0.26237623], loss=0.16171617161716212, gradiant=[-2.93071241e-08  7.87654712e-09]\n",
      "Loss at iteration 1876: w=[ 1.81683165 -0.26237623], loss=0.1617161716171622, gradiant=[-2.90373796e-08  7.80403209e-09]\n",
      "Loss at iteration 1877: w=[ 1.81683165 -0.26237623], loss=0.16171617161716212, gradiant=[-2.87701082e-08  7.73222005e-09]\n",
      "Loss at iteration 1878: w=[ 1.81683165 -0.26237623], loss=0.16171617161716215, gradiant=[-2.85053089e-08  7.66102559e-09]\n",
      "Loss at iteration 1879: w=[ 1.81683165 -0.26237623], loss=0.16171617161716223, gradiant=[-2.82429321e-08  7.59053975e-09]\n",
      "Loss at iteration 1880: w=[ 1.81683165 -0.26237623], loss=0.16171617161716215, gradiant=[-2.79829829e-08  7.52065625e-09]\n",
      "Loss at iteration 1881: w=[ 1.81683165 -0.26237623], loss=0.16171617161716212, gradiant=[-2.77254162e-08  7.45145264e-09]\n",
      "Loss at iteration 1882: w=[ 1.81683165 -0.26237623], loss=0.16171617161716217, gradiant=[-2.74702304e-08  7.38285004e-09]\n",
      "Loss at iteration 1883: w=[ 1.81683165 -0.26237623], loss=0.16171617161716206, gradiant=[-2.72173851e-08  7.31490779e-09]\n",
      "Loss at iteration 1884: w=[ 1.81683165 -0.26237623], loss=0.16171617161716212, gradiant=[-2.69668744e-08  7.24756492e-09]\n",
      "Loss at iteration 1885: w=[ 1.81683165 -0.26237623], loss=0.1617161716171622, gradiant=[-2.67186600e-08  7.18087678e-09]\n",
      "Loss at iteration 1886: w=[ 1.81683165 -0.26237623], loss=0.16171617161716206, gradiant=[-2.64727425e-08  7.11475590e-09]\n",
      "Loss at iteration 1887: w=[ 1.81683165 -0.26237623], loss=0.16171617161716206, gradiant=[-2.62290728e-08  7.04930277e-09]\n",
      "Loss at iteration 1888: w=[ 1.81683166 -0.26237623], loss=0.16171617161716215, gradiant=[-2.59876636e-08  6.98438699e-09]\n",
      "Loss at iteration 1889: w=[ 1.81683166 -0.26237623], loss=0.16171617161716206, gradiant=[-2.57484589e-08  6.92013247e-09]\n",
      "Loss at iteration 1890: w=[ 1.81683166 -0.26237623], loss=0.16171617161716204, gradiant=[-2.55114703e-08  6.85641588e-09]\n",
      "Loss at iteration 1891: w=[ 1.81683166 -0.26237623], loss=0.16171617161716212, gradiant=[-2.52766554e-08  6.79331554e-09]\n",
      "Loss at iteration 1892: w=[ 1.81683166 -0.26237623], loss=0.16171617161716212, gradiant=[-2.50440048e-08  6.73078304e-09]\n",
      "Loss at iteration 1893: w=[ 1.81683166 -0.26237623], loss=0.16171617161716206, gradiant=[-2.48134908e-08  6.66884414e-09]\n",
      "Loss at iteration 1894: w=[ 1.81683166 -0.26237623], loss=0.16171617161716206, gradiant=[-2.45851067e-08  6.60744526e-09]\n",
      "Loss at iteration 1895: w=[ 1.81683166 -0.26237623], loss=0.16171617161716204, gradiant=[-2.43588157e-08  6.54664441e-09]\n",
      "Loss at iteration 1896: w=[ 1.81683166 -0.26237623], loss=0.16171617161716206, gradiant=[-2.41346150e-08  6.48637573e-09]\n",
      "Loss at iteration 1897: w=[ 1.81683166 -0.26237623], loss=0.161716171617162, gradiant=[-2.39124736e-08  6.42667682e-09]\n",
      "Loss at iteration 1898: w=[ 1.81683166 -0.26237623], loss=0.16171617161716204, gradiant=[-2.36923768e-08  6.36752917e-09]\n",
      "Loss at iteration 1899: w=[ 1.81683166 -0.26237623], loss=0.16171617161716204, gradiant=[-2.34743074e-08  6.30891916e-09]\n",
      "Loss at iteration 1900: w=[ 1.81683166 -0.26237623], loss=0.16171617161716204, gradiant=[-2.32582448e-08  6.25084962e-09]\n",
      "Loss at iteration 1901: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-2.30441713e-08  6.19331342e-09]\n",
      "Loss at iteration 1902: w=[ 1.81683166 -0.26237623], loss=0.16171617161716206, gradiant=[-2.28320673e-08  6.13631116e-09]\n",
      "Loss at iteration 1903: w=[ 1.81683166 -0.26237623], loss=0.16171617161716192, gradiant=[-2.26219163e-08  6.07982864e-09]\n",
      "Loss at iteration 1904: w=[ 1.81683166 -0.26237623], loss=0.161716171617162, gradiant=[-2.2413698e-08  6.0238731e-09]\n",
      "Loss at iteration 1905: w=[ 1.81683166 -0.26237623], loss=0.1617161716171621, gradiant=[-2.22074008e-08  5.96841584e-09]\n",
      "Loss at iteration 1906: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-2.20029939e-08  5.91349947e-09]\n",
      "Loss at iteration 1907: w=[ 1.81683166 -0.26237623], loss=0.161716171617162, gradiant=[-2.18004764e-08  5.85905990e-09]\n",
      "Loss at iteration 1908: w=[ 1.81683166 -0.26237623], loss=0.161716171617162, gradiant=[-2.15998182e-08  5.80513830e-09]\n",
      "Loss at iteration 1909: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-2.14010125e-08  5.75169246e-09]\n",
      "Loss at iteration 1910: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-2.12040283e-08  5.69876990e-09]\n",
      "Loss at iteration 1911: w=[ 1.81683166 -0.26237623], loss=0.161716171617162, gradiant=[-2.10088643e-08  5.64630801e-09]\n",
      "Loss at iteration 1912: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-2.08154918e-08  5.59434543e-09]\n",
      "Loss at iteration 1913: w=[ 1.81683166 -0.26237623], loss=0.16171617161716187, gradiant=[-2.06239066e-08  5.54283508e-09]\n",
      "Loss at iteration 1914: w=[ 1.81683166 -0.26237623], loss=0.16171617161716184, gradiant=[-2.04340722e-08  5.49184520e-09]\n",
      "Loss at iteration 1915: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-2.02460001e-08  5.44126906e-09]\n",
      "Loss at iteration 1916: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-2.00596440e-08  5.39121399e-09]\n",
      "Loss at iteration 1917: w=[ 1.81683166 -0.26237623], loss=0.16171617161716204, gradiant=[-1.98750190e-08  5.34156127e-09]\n",
      "Loss at iteration 1918: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-1.96920767e-08  5.29242724e-09]\n",
      "Loss at iteration 1919: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-1.95108347e-08  5.24368371e-09]\n",
      "Loss at iteration 1920: w=[ 1.81683166 -0.26237623], loss=0.16171617161716192, gradiant=[-1.93312433e-08  5.19545444e-09]\n",
      "Loss at iteration 1921: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-1.91533234e-08  5.14759998e-09]\n",
      "Loss at iteration 1922: w=[ 1.81683166 -0.26237623], loss=0.16171617161716184, gradiant=[-1.89770228e-08  5.10025429e-09]\n",
      "Loss at iteration 1923: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-1.88023640e-08  5.05327469e-09]\n",
      "Loss at iteration 1924: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-1.86292935e-08  5.00679750e-09]\n",
      "Loss at iteration 1925: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-1.84578320e-08  4.96068904e-09]\n",
      "Loss at iteration 1926: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-1.82879362e-08  4.91505118e-09]\n",
      "Loss at iteration 1927: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-1.81196187e-08  4.86978073e-09]\n",
      "Loss at iteration 1928: w=[ 1.81683166 -0.26237623], loss=0.16171617161716198, gradiant=[-1.79528330e-08  4.82499048e-09]\n",
      "Loss at iteration 1929: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-1.77875972e-08  4.78055699e-09]\n",
      "Loss at iteration 1930: w=[ 1.81683166 -0.26237623], loss=0.16171617161716179, gradiant=[-1.76238727e-08  4.73656847e-09]\n",
      "Loss at iteration 1931: w=[ 1.81683166 -0.26237623], loss=0.16171617161716192, gradiant=[-1.74616592e-08  4.69297090e-09]\n",
      "Loss at iteration 1932: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-1.73009388e-08  4.64977390e-09]\n",
      "Loss at iteration 1933: w=[ 1.81683166 -0.26237623], loss=0.1617161716171619, gradiant=[-1.71416975e-08  4.60697406e-09]\n",
      "Loss at iteration 1934: w=[ 1.81683166 -0.26237623], loss=0.16171617161716195, gradiant=[-1.69839232e-08  4.56456547e-09]\n",
      "Loss at iteration 1935: w=[ 1.81683167 -0.26237623], loss=0.16171617161716187, gradiant=[-1.68275952e-08  4.52256736e-09]\n",
      "Loss at iteration 1936: w=[ 1.81683167 -0.26237623], loss=0.16171617161716195, gradiant=[-1.66727125e-08  4.48093222e-09]\n",
      "Loss at iteration 1937: w=[ 1.81683167 -0.26237623], loss=0.1617161716171619, gradiant=[-1.65192535e-08  4.43968714e-09]\n",
      "Loss at iteration 1938: w=[ 1.81683167 -0.26237623], loss=0.16171617161716184, gradiant=[-1.63672058e-08  4.39882604e-09]\n",
      "Loss at iteration 1939: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.62165602e-08  4.35833207e-09]\n",
      "Loss at iteration 1940: w=[ 1.81683167 -0.26237623], loss=0.1617161716171619, gradiant=[-1.60672959e-08  4.31823022e-09]\n",
      "Loss at iteration 1941: w=[ 1.81683167 -0.26237623], loss=0.16171617161716192, gradiant=[-1.59194120e-08  4.27847343e-09]\n",
      "Loss at iteration 1942: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-1.57728826e-08  4.23910655e-09]\n",
      "Loss at iteration 1943: w=[ 1.81683167 -0.26237623], loss=0.16171617161716184, gradiant=[-1.56277135e-08  4.20005912e-09]\n",
      "Loss at iteration 1944: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.54838600e-08  4.16144704e-09]\n",
      "Loss at iteration 1945: w=[ 1.81683167 -0.26237623], loss=0.1617161716171619, gradiant=[-1.53413588e-08  4.12308661e-09]\n",
      "Loss at iteration 1946: w=[ 1.81683167 -0.26237623], loss=0.16171617161716184, gradiant=[-1.52001376e-08  4.08519603e-09]\n",
      "Loss at iteration 1947: w=[ 1.81683167 -0.26237623], loss=0.16171617161716187, gradiant=[-1.50602471e-08  4.04754023e-09]\n",
      "Loss at iteration 1948: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.49216155e-08  4.01033621e-09]\n",
      "Loss at iteration 1949: w=[ 1.81683167 -0.26237623], loss=0.1617161716171619, gradiant=[-1.47842864e-08  3.97337748e-09]\n",
      "Loss at iteration 1950: w=[ 1.81683167 -0.26237623], loss=0.16171617161716179, gradiant=[-1.46481961e-08  3.93685085e-09]\n",
      "Loss at iteration 1951: w=[ 1.81683167 -0.26237623], loss=0.16171617161716179, gradiant=[-1.45133831e-08  3.90056994e-09]\n",
      "Loss at iteration 1952: w=[ 1.81683167 -0.26237623], loss=0.1617161716171619, gradiant=[-1.43797858e-08  3.86471699e-09]\n",
      "Loss at iteration 1953: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.42474454e-08  3.82909097e-09]\n",
      "Loss at iteration 1954: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.41162945e-08  3.79390031e-09]\n",
      "Loss at iteration 1955: w=[ 1.81683167 -0.26237623], loss=0.16171617161716187, gradiant=[-1.39863790e-08  3.75892961e-09]\n",
      "Loss at iteration 1956: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-1.38576310e-08  3.72438362e-09]\n",
      "Loss at iteration 1957: w=[ 1.81683167 -0.26237623], loss=0.1617161716171619, gradiant=[-1.37300962e-08  3.69005271e-09]\n",
      "Loss at iteration 1958: w=[ 1.81683167 -0.26237623], loss=0.16171617161716187, gradiant=[-1.36037072e-08  3.65614028e-09]\n",
      "Loss at iteration 1959: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.34785093e-08  3.62243776e-09]\n",
      "Loss at iteration 1960: w=[ 1.81683167 -0.26237623], loss=0.16171617161716179, gradiant=[-1.33544370e-08  3.58914498e-09]\n",
      "Loss at iteration 1961: w=[ 1.81683167 -0.26237623], loss=0.1617161716171619, gradiant=[-1.32315347e-08  3.55605471e-09]\n",
      "Loss at iteration 1962: w=[ 1.81683167 -0.26237623], loss=0.16171617161716184, gradiant=[-1.31097346e-08  3.52337685e-09]\n",
      "Loss at iteration 1963: w=[ 1.81683167 -0.26237623], loss=0.16171617161716187, gradiant=[-1.29890809e-08  3.49090445e-09]\n",
      "Loss at iteration 1964: w=[ 1.81683167 -0.26237623], loss=0.16171617161716184, gradiant=[-1.28695163e-08  3.45881264e-09]\n",
      "Loss at iteration 1965: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.27510729e-08  3.42693710e-09]\n",
      "Loss at iteration 1966: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.26336993e-08  3.39543194e-09]\n",
      "Loss at iteration 1967: w=[ 1.81683167 -0.26237623], loss=0.16171617161716179, gradiant=[-1.25174269e-08  3.36413934e-09]\n",
      "Loss at iteration 1968: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.24021994e-08  3.33322777e-09]\n",
      "Loss at iteration 1969: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.22880642e-08  3.30248451e-09]\n",
      "Loss at iteration 1970: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.21749453e-08  3.27214951e-09]\n",
      "Loss at iteration 1971: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.20628984e-08  3.24198017e-09]\n",
      "Loss at iteration 1972: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.19518549e-08  3.21219229e-09]\n",
      "Loss at iteration 1973: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-1.18418611e-08  3.18257568e-09]\n",
      "Loss at iteration 1974: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.17328538e-08  3.15332678e-09]\n",
      "Loss at iteration 1975: w=[ 1.81683167 -0.26237623], loss=0.16171617161716184, gradiant=[-1.16248732e-08  3.12426144e-09]\n",
      "Loss at iteration 1976: w=[ 1.81683167 -0.26237623], loss=0.1617161716171617, gradiant=[-1.15178641e-08  3.09554619e-09]\n",
      "Loss at iteration 1977: w=[ 1.81683167 -0.26237623], loss=0.16171617161716167, gradiant=[-1.14118638e-08  3.06700768e-09]\n",
      "Loss at iteration 1978: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-1.13068124e-08  3.03883052e-09]\n",
      "Loss at iteration 1979: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.12027544e-08  3.01081338e-09]\n",
      "Loss at iteration 1980: w=[ 1.81683167 -0.26237623], loss=0.16171617161716184, gradiant=[-1.10996328e-08  2.98313596e-09]\n",
      "Loss at iteration 1981: w=[ 1.81683167 -0.26237623], loss=0.16171617161716184, gradiant=[-1.09974772e-08  2.95564950e-09]\n",
      "Loss at iteration 1982: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.08962455e-08  2.92847539e-09]\n",
      "Loss at iteration 1983: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-1.07959631e-08  2.90148838e-09]\n",
      "Loss at iteration 1984: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.06965867e-08  2.87481209e-09]\n",
      "Loss at iteration 1985: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-1.05981407e-08  2.84832276e-09]\n",
      "Loss at iteration 1986: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.05005838e-08  2.82214104e-09]\n",
      "Loss at iteration 1987: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.04039456e-08  2.79612304e-09]\n",
      "Loss at iteration 1988: w=[ 1.81683167 -0.26237623], loss=0.1617161716171617, gradiant=[-1.03081736e-08  2.77043011e-09]\n",
      "Loss at iteration 1989: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-1.02133046e-08  2.74489527e-09]\n",
      "Loss at iteration 1990: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-1.01192910e-08  2.71966079e-09]\n",
      "Loss at iteration 1991: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-1.00261606e-08  2.69459181e-09]\n",
      "Loss at iteration 1992: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-9.93386751e-09  2.66982717e-09]\n",
      "Loss at iteration 1993: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-9.84244034e-09  2.64523041e-09]\n",
      "Loss at iteration 1994: w=[ 1.81683167 -0.26237623], loss=0.16171617161716179, gradiant=[-9.75184348e-09  2.62090113e-09]\n",
      "Loss at iteration 1995: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-9.66209172e-09  2.59675407e-09]\n",
      "Loss at iteration 1996: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-9.57315323e-09  2.57287673e-09]\n",
      "Loss at iteration 1997: w=[ 1.81683167 -0.26237623], loss=0.16171617161716179, gradiant=[-9.48504593e-09  2.54917302e-09]\n",
      "Loss at iteration 1998: w=[ 1.81683167 -0.26237623], loss=0.16171617161716176, gradiant=[-9.39773814e-09  2.52573029e-09]\n",
      "Loss at iteration 1999: w=[ 1.81683167 -0.26237623], loss=0.1617161716171618, gradiant=[-9.31124511e-09  2.50246224e-09]\n",
      "Loss at iteration 2000: w=[ 1.81683167 -0.26237623], loss=0.16171617161716167, gradiant=[-9.22553708e-09  2.47944776e-09]\n",
      "Loss at iteration 2001: w=[ 1.81683167 -0.26237623], loss=0.16171617161716173, gradiant=[-9.14062825e-09  2.45660751e-09]\n",
      "Loss at iteration 2002: w=[ 1.81683167 -0.26237624], loss=0.16171617161716179, gradiant=[-9.05649037e-09  2.43401773e-09]\n",
      "Loss at iteration 2003: w=[ 1.81683167 -0.26237624], loss=0.16171617161716179, gradiant=[-8.97313808e-09  2.41159330e-09]\n",
      "Loss at iteration 2004: w=[ 1.81683167 -0.26237624], loss=0.16171617161716176, gradiant=[-8.89054149e-09  2.38941859e-09]\n",
      "Loss at iteration 2005: w=[ 1.81683167 -0.26237624], loss=0.16171617161716167, gradiant=[-8.80871598e-09  2.36740716e-09]\n",
      "Loss at iteration 2006: w=[ 1.81683167 -0.26237624], loss=0.16171617161716173, gradiant=[-8.72763432e-09  2.34563302e-09]\n",
      "Loss at iteration 2007: w=[ 1.81683167 -0.26237624], loss=0.1617161716171617, gradiant=[-8.64730687e-09  2.32402956e-09]\n",
      "Loss at iteration 2008: w=[ 1.81683167 -0.26237624], loss=0.1617161716171617, gradiant=[-8.56771202e-09  2.30265037e-09]\n",
      "Loss at iteration 2009: w=[ 1.81683167 -0.26237624], loss=0.16171617161716179, gradiant=[-8.48885821e-09  2.28143726e-09]\n",
      "Loss at iteration 2010: w=[ 1.81683167 -0.26237624], loss=0.16171617161716184, gradiant=[-8.41071790e-09  2.26046515e-09]\n",
      "Loss at iteration 2011: w=[ 1.81683167 -0.26237624], loss=0.16171617161716179, gradiant=[-8.33330975e-09  2.23963662e-09]\n",
      "Loss at iteration 2012: w=[ 1.81683167 -0.26237624], loss=0.16171617161716167, gradiant=[-8.25660222e-09  2.21904421e-09]\n",
      "Loss at iteration 2013: w=[ 1.81683167 -0.26237624], loss=0.16171617161716179, gradiant=[-8.18061263e-09  2.19859775e-09]\n",
      "Loss at iteration 2014: w=[ 1.81683167 -0.26237624], loss=0.16171617161716179, gradiant=[-8.10531079e-09  2.17838251e-09]\n",
      "Loss at iteration 2015: w=[ 1.81683167 -0.26237624], loss=0.1617161716171617, gradiant=[-8.03071372e-09  2.15830924e-09]\n",
      "Loss at iteration 2016: w=[ 1.81683167 -0.26237624], loss=0.16171617161716167, gradiant=[-7.95679049e-09  2.13846925e-09]\n",
      "Loss at iteration 2017: w=[ 1.81683167 -0.26237624], loss=0.16171617161716167, gradiant=[-7.88356047e-09  2.11876383e-09]\n",
      "Loss at iteration 2018: w=[ 1.81683167 -0.26237624], loss=0.16171617161716179, gradiant=[-7.81099511e-09  2.09927483e-09]\n",
      "Loss at iteration 2019: w=[ 1.81683167 -0.26237624], loss=0.16171617161716173, gradiant=[-7.73910358e-09  2.07994333e-09]\n",
      "Loss at iteration 2020: w=[ 1.81683167 -0.26237624], loss=0.16171617161716167, gradiant=[-7.66786782e-09  2.06081108e-09]\n",
      "Loss at iteration 2021: w=[ 1.81683167 -0.26237624], loss=0.16171617161716179, gradiant=[-7.59729553e-09  2.04182656e-09]\n",
      "Loss at iteration 2022: w=[ 1.81683168 -0.26237624], loss=0.1617161716171618, gradiant=[-7.52736333e-09  2.02305224e-09]\n",
      "Loss at iteration 2023: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-7.45808600e-09  2.00440819e-09]\n",
      "Loss at iteration 2024: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-7.38943351e-09  1.98598427e-09]\n",
      "Loss at iteration 2025: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-7.32142362e-09  1.96768986e-09]\n",
      "Loss at iteration 2026: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-7.25403056e-09  1.94959693e-09]\n",
      "Loss at iteration 2027: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-7.18727122e-09  1.93162168e-09]\n",
      "Loss at iteration 2028: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-7.12110874e-09  1.91387676e-09]\n",
      "Loss at iteration 2029: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-7.05557287e-09  1.89622940e-09]\n",
      "Loss at iteration 2030: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-6.99062275e-09  1.87880985e-09]\n",
      "Loss at iteration 2031: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-6.92628784e-09  1.86148563e-09]\n",
      "Loss at iteration 2032: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-6.86253040e-09  1.84437621e-09]\n",
      "Loss at iteration 2033: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-6.79937188e-09  1.82737958e-09]\n",
      "Loss at iteration 2034: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-6.73678446e-09  1.81057628e-09]\n",
      "Loss at iteration 2035: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-6.67478014e-09  1.79390221e-09]\n",
      "Loss at iteration 2036: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-6.61334498e-09  1.77738668e-09]\n",
      "Loss at iteration 2037: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-6.55247145e-09  1.76103724e-09]\n",
      "Loss at iteration 2038: w=[ 1.81683168 -0.26237624], loss=0.1617161716171618, gradiant=[-6.49216370e-09  1.74481955e-09]\n",
      "Loss at iteration 2039: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-6.43240690e-09  1.72876498e-09]\n",
      "Loss at iteration 2040: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-6.37320315e-09  1.71284779e-09]\n",
      "Loss at iteration 2041: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-6.31453823e-09  1.69709817e-09]\n",
      "Loss at iteration 2042: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-6.25642249e-09  1.68146016e-09]\n",
      "Loss at iteration 2043: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-6.19883448e-09  1.66599253e-09]\n",
      "Loss at iteration 2044: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-6.14178131e-09  1.65065028e-09]\n",
      "Loss at iteration 2045: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-6.08525023e-09  1.63546095e-09]\n",
      "Loss at iteration 2046: w=[ 1.81683168 -0.26237624], loss=0.1617161716171618, gradiant=[-6.02923918e-09  1.62041180e-09]\n",
      "Loss at iteration 2047: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-5.97374402e-09  1.60549929e-09]\n",
      "Loss at iteration 2048: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.91876089e-09  1.59071896e-09]\n",
      "Loss at iteration 2049: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.86428402e-09  1.57607542e-09]\n",
      "Loss at iteration 2050: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-5.81030883e-09  1.56156554e-09]\n",
      "Loss at iteration 2051: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-5.75682894e-09  1.54719511e-09]\n",
      "Loss at iteration 2052: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-5.70383992e-09  1.53296087e-09]\n",
      "Loss at iteration 2053: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-5.65134399e-09  1.51883750e-09]\n",
      "Loss at iteration 2054: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-5.59932219e-09  1.50487889e-09]\n",
      "Loss at iteration 2055: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-5.54779230e-09  1.49100036e-09]\n",
      "Loss at iteration 2056: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.49672308e-09  1.47729947e-09]\n",
      "Loss at iteration 2057: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.44613613e-09  1.46367881e-09]\n",
      "Loss at iteration 2058: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.39600187e-09  1.45023312e-09]\n",
      "Loss at iteration 2059: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.34634292e-09  1.43685804e-09]\n",
      "Loss at iteration 2060: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-5.29712481e-09  1.42366741e-09]\n",
      "Loss at iteration 2061: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-5.24837921e-09  1.41052533e-09]\n",
      "Loss at iteration 2062: w=[ 1.81683168 -0.26237624], loss=0.1617161716171618, gradiant=[-5.20006319e-09  1.39757598e-09]\n",
      "Loss at iteration 2063: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.15220814e-09  1.38468437e-09]\n",
      "Loss at iteration 2064: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.10477897e-09  1.37196491e-09]\n",
      "Loss at iteration 2065: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-5.05779862e-09  1.35931829e-09]\n",
      "Loss at iteration 2066: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-5.01124135e-09  1.34682177e-09]\n",
      "Loss at iteration 2067: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-4.96512268e-09  1.33440473e-09]\n",
      "Loss at iteration 2068: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-4.91941628e-09  1.32214565e-09]\n",
      "Loss at iteration 2069: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.87414242e-09  1.30995496e-09]\n",
      "Loss at iteration 2070: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-4.82927505e-09  1.29791555e-09]\n",
      "Loss at iteration 2071: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-4.78483031e-09  1.28595090e-09]\n",
      "Loss at iteration 2072: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-4.74078140e-09  1.27414523e-09]\n",
      "Loss at iteration 2073: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-4.69715659e-09  1.26237909e-09]\n",
      "Loss at iteration 2074: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-4.65391281e-09  1.25079739e-09]\n",
      "Loss at iteration 2075: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.61108633e-09  1.23925078e-09]\n",
      "Loss at iteration 2076: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.56863702e-09  1.22787351e-09]\n",
      "Loss at iteration 2077: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-4.52659317e-09  1.21654612e-09]\n",
      "Loss at iteration 2078: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.48491984e-09  1.20538350e-09]\n",
      "Loss at iteration 2079: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-4.44365196e-09  1.19424352e-09]\n",
      "Loss at iteration 2080: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-4.40274069e-09  1.18329228e-09]\n",
      "Loss at iteration 2081: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-4.3622251e-09  1.1723699e-09]\n",
      "Loss at iteration 2082: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-4.32206656e-09  1.16160799e-09]\n",
      "Loss at iteration 2083: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-4.28229245e-09  1.15088916e-09]\n",
      "Loss at iteration 2084: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.24287242e-09  1.14031421e-09]\n",
      "Loss at iteration 2085: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.20382425e-09  1.12980381e-09]\n",
      "Loss at iteration 2086: w=[ 1.81683168 -0.26237624], loss=0.1617161716171618, gradiant=[-4.16512721e-09  1.11942070e-09]\n",
      "Loss at iteration 2087: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.12679313e-09  1.10910688e-09]\n",
      "Loss at iteration 2088: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.08880796e-09  1.09890245e-09]\n",
      "Loss at iteration 2089: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-4.05117391e-09  1.08878743e-09]\n",
      "Loss at iteration 2090: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-4.01388656e-09  1.07876389e-09]\n",
      "Loss at iteration 2091: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.97694233e-09  1.06883302e-09]\n",
      "Loss at iteration 2092: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.94033487e-09  1.05900533e-09]\n",
      "Loss at iteration 2093: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-3.90406877e-09  1.04925135e-09]\n",
      "Loss at iteration 2094: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-3.86813278e-09  1.03960129e-09]\n",
      "Loss at iteration 2095: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.83253429e-09  1.03001518e-09]\n",
      "Loss at iteration 2096: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-3.79725377e-09  1.02055401e-09]\n",
      "Loss at iteration 2097: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-3.76230883e-09  1.01113858e-09]\n",
      "Loss at iteration 2098: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.72767565e-09  1.00184779e-09]\n",
      "Loss at iteration 2099: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-3.69336635e-09  9.92622725e-10]\n",
      "Loss at iteration 2100: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-3.65937236e-09  9.83483813e-10]\n",
      "Loss at iteration 2101: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-3.62569115e-09  9.74430018e-10]\n",
      "Loss at iteration 2102: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.59231652e-09  9.65471703e-10]\n",
      "Loss at iteration 2103: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.55925393e-09  9.56577928e-10]\n",
      "Loss at iteration 2104: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-3.52649228e-09  9.47778744e-10]\n",
      "Loss at iteration 2105: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-3.49403721e-09  9.39042473e-10]\n",
      "Loss at iteration 2106: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-3.46187419e-09  9.30410859e-10]\n",
      "Loss at iteration 2107: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-3.43001242e-09  9.21840382e-10]\n",
      "Loss at iteration 2108: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-3.39843975e-09  9.13363311e-10]\n",
      "Loss at iteration 2109: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-3.36716196e-09  9.04947820e-10]\n",
      "Loss at iteration 2110: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-3.33616927e-09  8.96621444e-10]\n",
      "Loss at iteration 2111: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-3.30546020e-09  8.88376187e-10]\n",
      "Loss at iteration 2112: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-3.27503891e-09  8.80188367e-10]\n",
      "Loss at iteration 2113: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-3.24489147e-09  8.72098542e-10]\n",
      "Loss at iteration 2114: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-3.21502706e-09  8.64063191e-10]\n",
      "Loss at iteration 2115: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-3.18543443e-09  8.56113106e-10]\n",
      "Loss at iteration 2116: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.15611715e-09  8.48225786e-10]\n",
      "Loss at iteration 2117: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.12706379e-09  8.40431576e-10]\n",
      "Loss at iteration 2118: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-3.0982874e-09  8.3267541e-10]\n",
      "Loss at iteration 2119: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-3.06976237e-09  8.25039740e-10]\n",
      "Loss at iteration 2120: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-3.04151519e-09  8.17418725e-10]\n",
      "Loss at iteration 2121: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-3.01351196e-09  8.09925608e-10]\n",
      "Loss at iteration 2122: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-2.98578170e-09  8.02444925e-10]\n",
      "Loss at iteration 2123: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-2.95829539e-09  7.95075561e-10]\n",
      "Loss at iteration 2124: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-2.93106991e-09  7.87744980e-10]\n",
      "Loss at iteration 2125: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-2.90408956e-09  7.80502033e-10]\n",
      "Loss at iteration 2126: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-2.87736087e-09  7.73313561e-10]\n",
      "Loss at iteration 2127: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-2.85087628e-09  7.66198364e-10]\n",
      "Loss at iteration 2128: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-2.82463697e-09  7.59143415e-10]\n",
      "Loss at iteration 2129: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-2.79863806e-09  7.52157151e-10]\n",
      "Loss at iteration 2130: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-2.77288133e-09  7.45225363e-10]\n",
      "Loss at iteration 2131: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.74735612e-09  7.38377655e-10]\n",
      "Loss at iteration 2132: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-2.72206909e-09  7.31580722e-10]\n",
      "Loss at iteration 2133: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.69701594e-09  7.24841816e-10]\n",
      "Loss at iteration 2134: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.67218973e-09  7.18178554e-10]\n",
      "Loss at iteration 2135: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-2.64759710e-09  7.11557628e-10]\n",
      "Loss at iteration 2136: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-2.62322564e-09  7.05016934e-10]\n",
      "Loss at iteration 2137: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.59908243e-09  6.98522129e-10]\n",
      "Loss at iteration 2138: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-2.57516009e-09  6.92091865e-10]\n",
      "Loss at iteration 2139: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-2.55145549e-09  6.85731028e-10]\n",
      "Loss at iteration 2140: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-2.52797427e-09  6.79408529e-10]\n",
      "Loss at iteration 2141: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.50470163e-09  6.73171741e-10]\n",
      "Loss at iteration 2142: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.48165325e-09  6.66955232e-10]\n",
      "Loss at iteration 2143: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.45880590e-09  6.60837311e-10]\n",
      "Loss at iteration 2144: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-2.43618015e-09  6.54733453e-10]\n",
      "Loss at iteration 2145: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.41375053e-09  6.48731291e-10]\n",
      "Loss at iteration 2146: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-2.39153986e-09  6.42736975e-10]\n",
      "Loss at iteration 2147: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-2.36952384e-09  6.36835769e-10]\n",
      "Loss at iteration 2148: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.34771654e-09  6.30965650e-10]\n",
      "Loss at iteration 2149: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-2.32610657e-09  6.25162292e-10]\n",
      "Loss at iteration 2150: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-2.30469495e-09  6.19413409e-10]\n",
      "Loss at iteration 2151: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-2.28348733e-09  6.13692652e-10]\n",
      "Loss at iteration 2152: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-2.2624634e-09  6.0806767e-10]\n",
      "Loss at iteration 2153: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-2.24164642e-09  6.02444169e-10]\n",
      "Loss at iteration 2154: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-2.22100531e-09  5.96930209e-10]\n",
      "Loss at iteration 2155: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-2.20057039e-09  5.91407812e-10]\n",
      "Loss at iteration 2156: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.18030956e-09  5.85987259e-10]\n",
      "Loss at iteration 2157: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-2.16024768e-09  5.80570851e-10]\n",
      "Loss at iteration 2158: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.14035634e-09  5.75256879e-10]\n",
      "Loss at iteration 2159: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-2.12066453e-09  5.69929881e-10]\n",
      "Loss at iteration 2160: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-2.10113734e-09  5.64715386e-10]\n",
      "Loss at iteration 2161: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-2.08180539e-09  5.59489788e-10]\n",
      "Loss at iteration 2162: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-2.06263924e-09  5.54358337e-10]\n",
      "Loss at iteration 2163: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-2.04365606e-09  5.49248202e-10]\n",
      "Loss at iteration 2164: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-2.02484592e-09  5.44192395e-10]\n",
      "Loss at iteration 2165: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-2.00620794e-09  5.39187065e-10]\n",
      "Loss at iteration 2166: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.98774138e-09  5.34228217e-10]\n",
      "Loss at iteration 2167: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-1.96944786e-09  5.29302380e-10]\n",
      "Loss at iteration 2168: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.95132029e-09  5.24432053e-10]\n",
      "Loss at iteration 2169: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-1.93335703e-09  5.19615314e-10]\n",
      "Loss at iteration 2170: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.91556652e-09  5.14815301e-10]\n",
      "Loss at iteration 2171: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.89793277e-09  5.10087084e-10]\n",
      "Loss at iteration 2172: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.88046541e-09  5.05386695e-10]\n",
      "Loss at iteration 2173: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.86315289e-09  5.00750552e-10]\n",
      "Loss at iteration 2174: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.84600912e-09  4.96122106e-10]\n",
      "Loss at iteration 2175: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.82901265e-09  4.91576113e-10]\n",
      "Loss at iteration 2176: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.81218462e-09  4.87027455e-10]\n",
      "Loss at iteration 2177: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.79549679e-09  4.82573685e-10]\n",
      "Loss at iteration 2178: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.77898081e-09  4.78094897e-10]\n",
      "Loss at iteration 2179: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.76259644e-09  4.73731424e-10]\n",
      "Loss at iteration 2180: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.74638348e-09  4.69332721e-10]\n",
      "Loss at iteration 2181: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-1.73029783e-09  4.65056586e-10]\n",
      "Loss at iteration 2182: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.71438197e-09  4.60737374e-10]\n",
      "Loss at iteration 2183: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.69859386e-09  4.56529037e-10]\n",
      "Loss at iteration 2184: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.68296814e-09  4.52295682e-10]\n",
      "Loss at iteration 2185: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-1.66746883e-09  4.48165061e-10]\n",
      "Loss at iteration 2186: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.65213043e-09  4.44006165e-10]\n",
      "Loss at iteration 2187: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.63691579e-09  4.39949114e-10]\n",
      "Loss at iteration 2188: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-1.62185613e-09  4.35874448e-10]\n",
      "Loss at iteration 2189: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-1.60692111e-09  4.31888895e-10]\n",
      "Loss at iteration 2190: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.59213857e-09  4.27884691e-10]\n",
      "Loss at iteration 2191: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-1.57747504e-09  4.23979370e-10]\n",
      "Loss at iteration 2192: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-1.56296502e-09  4.20042519e-10]\n",
      "Loss at iteration 2193: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-1.54856927e-09  4.16212842e-10]\n",
      "Loss at iteration 2194: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-1.53432704e-09  4.12340976e-10]\n",
      "Loss at iteration 2195: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-1.52019404e-09  4.08584870e-10]\n",
      "Loss at iteration 2196: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.50621323e-09  4.04783170e-10]\n",
      "Loss at iteration 2197: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-1.49233633e-09  4.01106703e-10]\n",
      "Loss at iteration 2198: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-1.47861382e-09  3.97364660e-10]\n",
      "Loss at iteration 2199: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.46499228e-09  3.93752586e-10]\n",
      "Loss at iteration 2200: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.45152039e-09  3.90083225e-10]\n",
      "Loss at iteration 2201: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.43814619e-09  3.86544130e-10]\n",
      "Loss at iteration 2202: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.42492670e-09  3.82922583e-10]\n",
      "Loss at iteration 2203: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.41179172e-09  3.79470529e-10]\n",
      "Loss at iteration 2204: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.39881736e-09  3.75904049e-10]\n",
      "Loss at iteration 2205: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.38592338e-09  3.72514168e-10]\n",
      "Loss at iteration 2206: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.37318512e-09  3.69018297e-10]\n",
      "Loss at iteration 2207: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.36052621e-09  3.65695326e-10]\n",
      "Loss at iteration 2208: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-1.34802377e-09  3.62254375e-10]\n",
      "Loss at iteration 2209: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-1.33559652e-09  3.58993428e-10]\n",
      "Loss at iteration 2210: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.32332441e-09  3.55611096e-10]\n",
      "Loss at iteration 2211: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.31112306e-09  3.52416762e-10]\n",
      "Loss at iteration 2212: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-1.29907507e-09  3.49098676e-10]\n",
      "Loss at iteration 2213: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.28709961e-09  3.45954376e-10]\n",
      "Loss at iteration 2214: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.27527056e-09  3.42703939e-10]\n",
      "Loss at iteration 2215: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.26351551e-09  3.39613967e-10]\n",
      "Loss at iteration 2216: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-1.25189962e-09  3.36435768e-10]\n",
      "Loss at iteration 2217: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.24036500e-09  3.33383543e-10]\n",
      "Loss at iteration 2218: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.22896034e-09  3.30270034e-10]\n",
      "Loss at iteration 2219: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.21763888e-09  3.27268139e-10]\n",
      "Loss at iteration 2220: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-1.20643732e-09  3.24233381e-10]\n",
      "Loss at iteration 2221: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.19533050e-09  3.21257835e-10]\n",
      "Loss at iteration 2222: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.18433056e-09  3.18293244e-10]\n",
      "Loss at iteration 2223: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.17342684e-09  3.15374541e-10]\n",
      "Loss at iteration 2224: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.16263088e-09  3.12454655e-10]\n",
      "Loss at iteration 2225: w=[ 1.81683168 -0.26237624], loss=0.1617161716171618, gradiant=[-1.15192537e-09  3.09595904e-10]\n",
      "Loss at iteration 2226: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.14132659e-09  3.06732269e-10]\n",
      "Loss at iteration 2227: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.13081603e-09  3.03928438e-10]\n",
      "Loss at iteration 2228: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.12041561e-09  3.01102698e-10]\n",
      "Loss at iteration 2229: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.11009453e-09  2.98362816e-10]\n",
      "Loss at iteration 2230: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-1.09988463e-09  2.95588739e-10]\n",
      "Loss at iteration 2231: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.08975673e-09  2.92884828e-10]\n",
      "Loss at iteration 2232: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.07972742e-09  2.90184765e-10]\n",
      "Loss at iteration 2233: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-1.06978722e-09  2.87521118e-10]\n",
      "Loss at iteration 2234: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.05994354e-09  2.84864576e-10]\n",
      "Loss at iteration 2235: w=[ 1.81683168 -0.26237624], loss=0.16171617161716173, gradiant=[-1.05018557e-09  2.82250223e-10]\n",
      "Loss at iteration 2236: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-1.04052204e-09  2.79641495e-10]\n",
      "Loss at iteration 2237: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.03094229e-09  2.77078064e-10]\n",
      "Loss at iteration 2238: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-1.02145314e-09  2.74527068e-10]\n",
      "Loss at iteration 2239: w=[ 1.81683168 -0.26237624], loss=0.16171617161716179, gradiant=[-1.01205281e-09  2.71996203e-10]\n",
      "Loss at iteration 2240: w=[ 1.81683168 -0.26237624], loss=0.16171617161716176, gradiant=[-1.00273508e-09  2.69502346e-10]\n",
      "Loss at iteration 2241: w=[ 1.81683168 -0.26237624], loss=0.1617161716171617, gradiant=[-9.93509571e-10  2.67006861e-10]\n",
      "Loss at iteration 2242: w=[ 1.81683168 -0.26237624], loss=0.16171617161716167, gradiant=[-9.84361481e-10  2.64562446e-10]\n",
      "Loss at iteration 2243: w=[ 1.81683168 -0.26237624], loss=0.16171617161716165, gradiant=[-9.75302505e-10  2.62121880e-10]\n",
      "Loss at iteration 2244: w=[ 1.81683168 -0.26237624], loss=0.16171617161716162, gradiant=[-9.66326130e-10  2.59707071e-10]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "trainingData = [\n",
    "    [1, 1],\n",
    "    [2, 3],\n",
    "    [4, 3],\n",
    "]\n",
    "\n",
    "def phi(x):\n",
    "    return np.array([1, x])\n",
    "\n",
    "def trainLoss(w):\n",
    "    return 1 / len(trainingData) * sum((w.dot(phi(x)) - y) ** 2 for x, y in trainingData)\n",
    "\n",
    "def gradiantTrainLoss(w):\n",
    "    return 1 / len(trainingData) * sum(2 * (w.dot(phi(x)) - y) * phi(x) for x, y in trainingData)\n",
    "\n",
    "def gradiantDescent(F, gradiantF, step):\n",
    "    w = np.array([1, 1])\n",
    "    w_values = []\n",
    "    iteration = 0  # Manual iteration counter\n",
    "\n",
    "    while True:\n",
    "        loss = F(w)\n",
    "        gradiant = gradiantF(w)\n",
    "        w = w - step * gradiant\n",
    "        if np.linalg.norm(gradiant) < 1e-9:  # Convergence condition\n",
    "            break\n",
    "        if iteration > 5000:  # Iteration limit to prevent infinite loops\n",
    "            print(\"Iteration limit reached. Stopping.\")\n",
    "            break\n",
    "        w_values.append(w.copy())\n",
    "        print(f\"Loss at iteration {iteration}: w={w}, loss={loss}, gradiant={gradiant}\")\n",
    "        iteration += 1  # Increment iteration counter\n",
    "\n",
    "    # Plot the line using coordinates from w_values\n",
    "    w_df = pd.DataFrame(w_values, columns=[\"x\", \"y\"])\n",
    "    plt.plot(w_df[\"x\"], w_df[\"y\"], marker=\"o\", label=\"Line\")\n",
    "    plt.title(\"Line Plot from Vector Array (w_values)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "gradiantDescent(trainLoss, gradiantTrainLoss, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac6890e82e4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
